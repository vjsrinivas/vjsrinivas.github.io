<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=logo.png rel=icon type=image/png> <script> MathJax = {
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				processEscapes: true
			},
			svg: {
				fontCache: 'global'
			}
		}; </script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async id=MathJax-script></script> <script src=https://kit.fontawesome.com/d1e7542e7f.js crossorigin=anonymous></script> <link href=client/main.677622768.css rel=stylesheet><link href=client/[slug].d2f2c26c.css rel=stylesheet><link href=client/client.4da8ec3a.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Pseudo-LIDAR Projection via Depth Maps</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class=svelte-wyfi55><ul class=svelte-wyfi55><li class=svelte-wyfi55><a href=blog rel=prefetch aria-current=page class="svelte-wyfi55 btn-gen">Blog</a></li> <li class=svelte-wyfi55><a href=. class="svelte-wyfi55 btn-gen">Home</a></li> </ul></nav> <main class=svelte-1xjjggr> <div class="content svelte-1d9fknd"><h1>Pseudo-LIDAR Projection via Depth Maps</h1> <hr> <p>In many navigation and layout applications, there is a need for spatial data. Historically, this meant that most hardware choices were radio frequency based - mainly LiDAR or RADAR.</p> <script src=https://cdn.jsdelivr.net/npm/d3@7></script> <script src=https://cdn.plot.ly/plotly-2.31.0.min.js charset=utf-8></script> <script> function unpack(rows, key) {
        return rows.map(function(row)
    { return row[key]; });}

    function generate_plot(url, html_element) {
        //const data = d3.csv("./post-res/pseudo_lidar/20240414_115355.csv");
        const data = d3.csv(url)
        data.then( (rows) => {
            var z_data = unpack(rows, 'z1');

            var trace1 = {
                x:unpack(rows, 'x1'),
                y: unpack(rows, 'y1'),
                z: z_data,
                mode: 'markers',
                colorscale: 'YlGnBu',
                marker: {
                    size: 2,
                    opacity: 1.0,
                    color: z_data,
                    colorscale: 'Viridis'
                },
                type: 'scatter3D',
            };

            var data = [trace1];
            var layout = {
            margin: {
                l: 0,
                r: 0,
                b: 0,
                t: 0
            },
            scene: {
                    camera: {
                        eye: {x:0, y:0, z:2.5},
                    },
                }
            };
            Plotly.newPlot(html_element, data, layout);
            console.log(`Generated plotly graph for ${html_element}`)

            /*
            var previewID = html_element+"_preview"
            const preview_img = document.getElementById(previewID)
            preview_img.remove()
            */
        } )
    }

    /*
    generate_plot("./post-res/pseudo_lidar/20240414_115355.csv", "gd")
    generate_plot("./post-res/pseudo_lidar/20240414_120609.csv", "gd1")
    generate_plot("./post-res/pseudo_lidar/20240414_120944.csv", "gd2")
    generate_plot("./post-res/pseudo_lidar/20240413_174803.csv", "gd3")
    generate_plot("./post-res/pseudo_lidar/20240414_123255.csv", "gd4")
    */ </script> <style>.viz{width:400px;height:400px;display:inline}.preview{max-width:400px;max-height:400px}</style> <h2 id=motivation>Motivation</h2> <p>Both LiDAR and RADAR can produce a 3D point cloud but can also be composed into a depth map. Although both are still used extensively for spatial imaging, the biggest drawbacks have been the price, size, and processing power needed to get these methods working efficiently.</p> <p>An alternative method has been to use stereosopic cameras - where a camera system has two lenses set a certain distance apart. Similar to how our eyes work, stereosopic cameras utilize the difference in the two images along with the distance between the two lenses to calculate spatial data. Typically this data is in the form of a depth map, but it can also be converted into a 3D point cloud if the intrinsics of the two lenses are known. The downside of stereosopic cameras is also hardware cost and the relatively large housing requirements for the two lenses. The quality of the depth map output can also decrease based on various environmental factors such as clutter.</p> <p>So what could you do if you are limited by a tight budget or dimension requirement? In many computer vision applications, you’re usually stuck with a less-than-ideal, single lens camera, but a client could ask you to develop something that will require you to have spatial data. I was ruminating about this kind of situation and what my options could be.</p> <p>One potential method of deriving good spatial data from a traditional single lens camera would be applying a neural network trained for monocular depth estimation. Then, if you have the cameras intrinsics (mainly focal length and sensor center coordinates), you can generate a rough 3D point cloud. Additionally, if you had a set of reliable reference points, you could also derive a metric 3D point cloud. </p> <p>In recent years, advancements in image encoding and segmentation methods have indirectly improved many monocular depth estimation networks. One of the best papers has been <a href=https://arxiv.org/pdf/2401.10891.pdf rel=nofollow target=_blank>Depth Anything</a>. The basic idea of the paper is to use a "data engine" with a massive amount of unlabeled data to generate a model with great generalization abilities (very similar to the approach taken by Segment Anything Method). Along with the data engine, the authors employ different training techniques related to augmentation and auxiliary supervision to develop a foundational model for monocular depth estimation. </p> <h2 id=methodology>Methodology</h2> <p>The general idea is to <a href=https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html rel=nofollow target=_blank>generate or retrieve the intrinsics of the camera</a> you are using - mainly the focal length (single value) and optical centers. After that, for each image taken, process it through Depth Anything. Finally, <a href=https://arxiv.org/pdf/2302.10007.pdf rel=nofollow target=_blank>based on this survey paper</a>, use the following equation to generate the 3D point cloud:</p> <p>$$ x \leftarrow (z/F)(u-C_{u}) $$ $$ y \leftarrow (z/F)(v-C_{v}) $$ $$ z \leftarrow d_{u,v} $$</p> <p>$$ d_{u,v} \rightarrow \text{the depth value at a given image coordinate (u, v)} $$ $$ C_{u,v} \rightarrow \text{the optical center tuple on the x and y coordinate } $$ $$ F \rightarrow \text{the focal length of the camera} $$</p> <p>The results were okay... The model performed pretty well on all the sample images, but the point cloud was not as accurate as I hoped it to be. This is probably because of the camera intrinsics. I was using my phone camera and couldn't get the intrinsics from online specifications or the operating system so had to manually calibrate it, which could lead to incorrect values.</p> <h2 id=examples>Examples:</h2> <figure> <div> <img src=./post-res/pseudo_lidar/20240414_115355_processed.jpg> <div class=viz id=gd> <img src=./post-res/pseudo_lidar/gd.png class=preview id=gd_preview> </div> </div> <figcaption><b>Fig. 1</b> - Projection of a water bottle sitting on top of a laptop.</figcaption> </figure> <figure> <img src=./post-res/pseudo_lidar/20240414_120609_processed.jpg> <div class=viz id=gd1> <img src=./post-res/pseudo_lidar/gd1.png class=preview id=gd1_preview> </div> <figcaption><b>Fig. 2</b> - Projection of a printer on the ground next to a cardboard box.</figcaption> </figure> <figure> <img src=./post-res/pseudo_lidar/20240414_120944_processed.jpg> <div class=viz id=gd2> <img src=./post-res/pseudo_lidar/gd2.png class=preview id=gd2_preview> </div> <figcaption><b>Fig. 3</b> - Projection of a plush toy on top of a stack of pillows. I have pillowcases; I was just washing them. </figcaption> </figure> <figure> <img src=./post-res/pseudo_lidar/20240413_174803_processed.jpg> <div class=viz id=gd3> <img src=./post-res/pseudo_lidar/gd3.png class=preview id=gd3_preview> </div> <figcaption><b>Fig. 4</b> - Two lamps with a TV stand in between them. The projection is pretty bad with this one.</figcaption> </figure> <figure> <img src=./post-res/pseudo_lidar/20240414_123255_processed.jpg> <div class=viz id=gd4> <img src=./post-res/pseudo_lidar/gd4.png class=preview id=gd4_preview> </div> <figcaption><b>Fig. 5</b> - Crowded desk with a piece of foam sticking towards the camera.</figcaption> </figure></div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{title:"Pseudo-LIDAR Projection via Depth Maps",description:"Description goes here",slug:"pseudo_lidar",html:"\u003Cp\u003EIn many navigation and layout applications, there is a need for spatial data. Historically, this meant that most hardware choices were radio frequency based - mainly LiDAR or RADAR.\u003C\u002Fp\u003E\n\u003Cscript src=\"https:\u002F\u002Fcdn.jsdelivr.net\u002Fnpm\u002Fd3@7\"\u003E\u003C\u002Fscript\u003E\n\u003Cscript src=\"https:\u002F\u002Fcdn.plot.ly\u002Fplotly-2.31.0.min.js\" charset=\"utf-8\"\u003E\u003C\u002Fscript\u003E\n\n\u003Cscript\u003E\n    function unpack(rows, key) {\n        return rows.map(function(row)\n    { return row[key]; });}\n\n    function generate_plot(url, html_element) {\n        \u002F\u002Fconst data = d3.csv(\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240414_115355.csv\");\n        const data = d3.csv(url)\n        data.then( (rows) =\u003E {\n            var z_data = unpack(rows, 'z1');\n\n            var trace1 = {\n                x:unpack(rows, 'x1'),\n                y: unpack(rows, 'y1'),\n                z: z_data,\n                mode: 'markers',\n                colorscale: 'YlGnBu',\n                marker: {\n                    size: 2,\n                    opacity: 1.0,\n                    color: z_data,\n                    colorscale: 'Viridis'\n                },\n                type: 'scatter3D',\n            };\n\n            var data = [trace1];\n            var layout = {\n            margin: {\n                l: 0,\n                r: 0,\n                b: 0,\n                t: 0\n            },\n            scene: {\n                    camera: {\n                        eye: {x:0, y:0, z:2.5},\n                    },\n                }\n            };\n            Plotly.newPlot(html_element, data, layout);\n            console.log(`Generated plotly graph for ${html_element}`)\n\n            \u002F*\n            var previewID = html_element+\"_preview\"\n            const preview_img = document.getElementById(previewID)\n            preview_img.remove()\n            *\u002F\n        } )\n    }\n\n    \u002F*\n    generate_plot(\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240414_115355.csv\", \"gd\")\n    generate_plot(\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240414_120609.csv\", \"gd1\")\n    generate_plot(\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240414_120944.csv\", \"gd2\")\n    generate_plot(\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240413_174803.csv\", \"gd3\")\n    generate_plot(\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240414_123255.csv\", \"gd4\")\n    *\u002F\n\u003C\u002Fscript\u003E\n\u003Cstyle\u003E\n    .viz {\n        width: 400px;\n        height: 400px;\n        display: inline\n    }\n\n    .preview {\n        max-width: 400px;\n        max-height: 400px;\n    }\n\u003C\u002Fstyle\u003E\n\n\u003Ch2 id=\"motivation\"\u003EMotivation\u003C\u002Fh2\u003E\n\u003Cp\u003EBoth LiDAR and RADAR can produce a 3D point cloud but can also be composed into a depth map. Although both are still used extensively for spatial imaging, the biggest drawbacks have been the price, size, and processing power needed to get these methods working efficiently.\u003C\u002Fp\u003E\n\u003Cp\u003EAn alternative method has been to use stereosopic cameras - where a camera system has two lenses set a certain distance apart. Similar to how our eyes work, stereosopic cameras utilize the difference in the two images along with the distance between the two lenses to calculate spatial data. Typically this data is in the form of a depth map, but it can also be converted into a 3D point cloud if the intrinsics of the two lenses are known. The downside of stereosopic cameras is also hardware cost and the relatively large housing requirements for the two lenses. The quality of the depth map output can also decrease based on various environmental factors such as clutter.\u003C\u002Fp\u003E\n\u003Cp\u003ESo what could you do if you are limited by a tight budget or dimension requirement? In many computer vision applications, you’re usually stuck with a less-than-ideal, single lens camera, but a client could ask you to develop something that will require you to have spatial data. I was ruminating about this kind of situation and what my options could be.\u003C\u002Fp\u003E\n\u003Cp\u003EOne potential method of deriving good spatial data from a traditional single lens camera would be applying a neural network trained for monocular depth estimation. Then, if you have the cameras intrinsics (mainly focal length and sensor center coordinates), you can generate a rough 3D point \ncloud. Additionally, if you had a set of reliable reference points, you could also derive a metric 3D point cloud. \u003C\u002Fp\u003E\n\u003Cp\u003EIn recent years, advancements in image encoding and segmentation methods have indirectly improved many monocular depth estimation networks. One of the best papers has been \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2401.10891.pdf\"\u003EDepth Anything\u003C\u002Fa\u003E. The basic idea of the paper is to use a &quot;data engine&quot; with a massive amount of unlabeled data to generate a model with great generalization abilities (very similar to the approach taken by Segment Anything Method). Along with the data engine, the authors employ different training techniques related to augmentation and auxiliary supervision to develop a foundational model for monocular depth estimation. \u003C\u002Fp\u003E\n\u003Ch2 id=\"methodology\"\u003EMethodology\u003C\u002Fh2\u003E\n\u003Cp\u003EThe general idea is to \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fdocs.opencv.org\u002F4.x\u002Fdc\u002Fdbb\u002Ftutorial_py_calibration.html\"\u003Egenerate or retrieve the intrinsics of the camera\u003C\u002Fa\u003E you are using - mainly the focal length (single value) and optical centers. After that, for each image taken, process it through Depth Anything. Finally, \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2302.10007.pdf\"\u003Ebased on this survey paper\u003C\u002Fa\u003E, use the following equation to generate the 3D point cloud:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\nx \\leftarrow (z\u002FF)(u-C_{u})\n$$\n$$\ny \\leftarrow (z\u002FF)(v-C_{v})\n$$\n$$\nz \\leftarrow d_{u,v}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$\nd_{u,v} \\rightarrow \\text{the depth value at a given image coordinate (u, v)}\n$$\n$$\nC_{u,v} \\rightarrow \\text{the optical center tuple on the x and y coordinate }\n$$\n$$\nF \\rightarrow \\text{the focal length of the camera}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003EThe results were okay... The model performed pretty well on all the sample images, but the point cloud was not as accurate as I hoped it to be. This is probably because of the camera intrinsics. I was using my phone camera and couldn&#39;t get the intrinsics from online specifications or the operating system so had to manually calibrate it, which could lead to incorrect values.\u003C\u002Fp\u003E\n\u003Ch2 id=\"examples\"\u003EExamples:\u003C\u002Fh2\u003E\n\u003Cfigure\u003E\n\u003Cdiv\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240414_115355_processed.jpg\"\u002F\u003E\n\u003Cdiv class=\"viz\" id=\"gd\"\u003E\n    \u003Cimg id=\"gd_preview\" class=\"preview\" src=\".\u002Fpost-res\u002Fpseudo_lidar\u002Fgd.png\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\u003C\u002Fdiv\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 1\u003C\u002Fb\u003E - Projection of a water bottle sitting on top of a laptop.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240414_120609_processed.jpg\"\u002F\u003E\n\u003Cdiv class=\"viz\" id=\"gd1\"\u003E\n    \u003Cimg id=\"gd1_preview\" class=\"preview\" src=\".\u002Fpost-res\u002Fpseudo_lidar\u002Fgd1.png\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 2\u003C\u002Fb\u003E - Projection of a printer on the ground next to a cardboard box.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240414_120944_processed.jpg\"\u002F\u003E\n\u003Cdiv class=\"viz\" id=\"gd2\"\u003E\n    \u003Cimg id=\"gd2_preview\" class=\"preview\" src=\".\u002Fpost-res\u002Fpseudo_lidar\u002Fgd2.png\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 3\u003C\u002Fb\u003E - Projection of a plush toy on top of a stack of pillows. I have pillowcases; I was just washing them. \u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240413_174803_processed.jpg\"\u002F\u003E\n\u003Cdiv class=\"viz\" id=\"gd3\"\u003E\n    \u003Cimg id=\"gd3_preview\" class=\"preview\" src=\".\u002Fpost-res\u002Fpseudo_lidar\u002Fgd3.png\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 4\u003C\u002Fb\u003E - Two lamps with a TV stand in between them. The projection is pretty bad with this one.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fpseudo_lidar\u002F20240414_123255_processed.jpg\"\u002F\u003E\n\u003Cdiv class=\"viz\" id=\"gd4\"\u003E\n    \u003Cimg id=\"gd4_preview\" class=\"preview\" src=\".\u002Fpost-res\u002Fpseudo_lidar\u002Fgd4.png\"\u002F\u003E\n\u003C\u002Fdiv\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 5\u003C\u002Fb\u003E - Crowded desk with a piece of foam sticking towards the camera.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E",created:"Thu, 18 Apr 2024 19:45:28 GMT",excerpt:"In many navigation and layout applications, there is a need for spatial data. Historically, this meant that most hardware choices were radio frequency based - mainly LiDAR or RADAR.\n",author:a,readingTime:"5 min read",mediaFilePath:"post-res\u002Fpseudo_lidar\u002Fpseudo_lidar_thumb.png",tags:["image processing"],art_credit:a}}}("Vijay Rajagopal"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');var s=document.createElement("script");try{new Function("if(0)import('')")();s.src="/client/client.4da8ec3a.js";s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main","/client/client.4da8ec3a.js")}document.head.appendChild(s)</script> 