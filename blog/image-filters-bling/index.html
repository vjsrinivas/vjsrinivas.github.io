<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=logo.png rel=icon type=image/png> <script> MathJax = {
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				processEscapes: true
			},
			svg: {
				fontCache: 'global'
			}
		}; </script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async id=MathJax-script></script> <script src=https://kit.fontawesome.com/d1e7542e7f.js crossorigin=anonymous></script> <link href=client/main.677622768.css rel=stylesheet><link href=client/[slug].d2f2c26c.css rel=stylesheet><link href=client/client.4da8ec3a.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Bling Filter</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class=svelte-wyfi55><ul class=svelte-wyfi55><li class=svelte-wyfi55><a href=blog rel=prefetch aria-current=page class="svelte-wyfi55 btn-gen">Blog</a></li> <li class=svelte-wyfi55><a href=. class="svelte-wyfi55 btn-gen">Home</a></li> </ul></nav> <main class=svelte-1xjjggr> <div class="content svelte-1d9fknd"><h1>Bling Filter</h1> <hr> <link href=post-res/image-filters-bling/style.css rel=stylesheet> <h2 id=table-of-contents>Table of Contents</h2> <ul> <li><a href=javascript:; onclick="document.location.hash='style-introduction';">Style Introduction</a></li> <li><a href=javascript:; onclick="document.location.hash='implementing-specular-detection';">Implementing Specular Detection</a><ul> <li><a href=javascript:; onclick="document.location.hash='pre-processing';">Pre-processing</a></li> <li><a href=javascript:; onclick="document.location.hash='thresholding';">Thresholding</a></li> <li><a href=javascript:; onclick="document.location.hash='post-processing';">Post-processing</a></li> <li><a href=javascript:; onclick="document.location.hash='caveats';">Caveats</a></li> </ul> </li> <li><a href=javascript:; onclick="document.location.hash='adding-lens-flares';">Adding "Lens Flares"</a></li> <li><a href=javascript:; onclick="document.location.hash='bloom-effect';">Bloom Effect</a></li> <li><a href=javascript:; onclick="document.location.hash='results';">Results</a></li> <li><a href=javascript:; onclick="document.location.hash='notes';">Notes</a></li> </ul> <h1 id=style-introduction>Style Introduction</h1> <p>This filter, which I will be referring to as the "bling filter", is a filter I happened to see a lot on TikTok as of recently. Mainly used to stylize a person's face or a scenic view as something out of a late-2000s movie, the bling filter applies cartoonized lens flares and adds a bloom shader to an image.</p> <p>Although, it is important to note that some TikToks use a custom bling effect that is created with software suites such as Adobe After Effects. These types of TikToks have lens flares that are far more realistic, with the prism color effect on the fringes of each lens flare. For the sake of simplicity, I did not spend my time trying to achieve these kinds of results.</p> <figure> <img src=./post-res/image-filters-bling/example1.png alt="Example of simple bling filter" width=300> <figcaption><b>Fig. 1</b> - This is an example of the bling filter that can be used in the TikTok app</figcaption> </figure> <p>In order to even start an attempt at recreating this filter, I had to first determine the location of where hypothetical lens flares would go. How would I go about doing that? From briefly analyzing some of video clips with this filter, I came to the conclusion that computing regions where there were specularities could be a fair way of determining lens flare positions. </p> <p>To go about detecting specularity algorithmically, accurately, and consistently was a daunting task for me at first. But by searching on Google and arxiv, I eventually found a paper by the name of <a href=https://ieeexplore.ieee.org/document/7294821 rel=nofollow target=_blank>"Generic and real-time detection of specular reflections in images"</a>. This paper was written by Alexandre Morgand and Mohamed Tamaazousti from the Vision & Content Engineering Laboratory and CEA LIST, respectively.</p> <h1 id=implementing-specular-detection>Implementing Specular Detection</h1> <p>Morgan and Tamaazousti's paper discusses a method of dynamically creating a threshold for specular detection as well as some pre and post-processing functions that help remove false positives.</p> <h2 id=pre-processing>Pre-processing</h2> <p>The pre-processing of an image is intended to reduce noise and equalize any highly-saturated images, which could cause false specular detections or malformed specular masks. With according to the paper, I implemented the following contrast equalization algorithm but inverted the inequalities in both conditionals (<code>></code> instead of <code>≤</code>):</p> <pre class=language-><code class=language->contrast = 1
if Brightness ≤ Tb then
  while Brightness ≤ Tb do
    contrast &lt;- contrast - 0.01
    Image_pixels = contrast * Image_pixels
    Compute(Brightness)
  end while
end if</code></pre><p>For reference, the brightness of an image can be computed with the following calculation:</p> <p>$$ Brightness = \sqrt(0.241*(C_R^2)+0.691*(C_G^2) + 0.068*(C_B^2))/(Width * Height) $$</p> <pre class=language-><code class=language->def calculateBrightness(img,w,h):
    # cr, cg, cb = red channel, green channel, blue channel
    # normalizes luminance to 0-1 (Y/width*height)
    rcoef, gcoef, bcoef = 0.241,0.691,0.068
    r,g,b = img[:,:,2], img[:,:,1], img[:,:,0]

    # need to square:
    r = rcoef*(r**2)
    g = gcoef*(g**2)
    b = bcoef*(b**2)
    _out = np.sqrt(r+g+b)/(w*h)
    return np.sum(_out)</code></pre><p>After this pre-processing is complete, we need to switch from RGB to HSV color space in order to utilize the value and saturation details of the image.</p> <h2 id=thresholding>Thresholding</h2> <p>Thresholding can be basically be described as a process of assigning 0 or 1 (or (0,0,0) and (255,255,255) in RGB space) to each pixel of an image based on a given conditional. For our conditional, it will be determined by the following defined variables:</p> <p>\[ T_v = Brightness * k_v \] \[ T_s = 170 \text{ (estimated by trial and error ) } \]</p> <p>$T_v$ is a threshold based on the intensity with the value channel of the image in HSV color space. The paper defines $T_v$ as the product of Brightness (of the image) and a coefficient called $k_v$, but in reality, to compute $T_v$, we need to use the following linear equation:</p> <p>\[ y = 2x \]</p> <p>With y being equal to $T_v$ and $x$ being the calculated $Brightness$ of an image. </p> <p>After calculating both threshold values, we have to take into account the circumstances when a certain ratio of the image is white and adjust both our threshold values:</p> <p>\[ \text{ if } Histogram_{Value}(255) > (Image_{size} / 3) \]</p> <p>\[ \text{ then } T_s = 30 \text{ and } T_v = 245 \]</p> <p>Finally, we can apply the following conditionals in our actual thresholding function:</p> <p>\[ \text{ if } S(x) &lt; T_s \text{ and } V(x) > T_v \]</p> <p><strong>Note:</strong> S(x) and V(x) are the saturation and value of <strong>each pixel.</strong></p> <figure> <img src=post-res/image-filters-bling/mask_example.png alt="Example of thresholding with specular mask on righthand side and original image on the lefthand side"> <figcaption><b>Fig. 2</b> - This is an example of an image being processed and producing a specular mask</figcaption> </figure> <h2 id=post-processing>Post-processing</h2> <p>After the thresholding portion of the code has created a "specular" mask, we must create a method that extracts regions that could be candidates for a lens flare and then find a way of pruning excess candidates.</p> <p>To create the regions, I utilized the <code>cv2.findContours</code> function to find contours within the mask. The function returns a grouping of contours with a hierarchy, but my concern is just the groupings themselves.</p> <p>The majority of the contours are very small (1x1 or 2x2) in size, so I prune the contour candidates will less than an area of four out. After filtering, I sorted the contours from largest to smallest area and employed the following function to determine a rough estimate of usable contour candidates:</p> <p>\[ y = sqrt(2*(x+10))/2 \]</p> <figure> <img src=post-res/image-filters-bling/chart.png alt="Visualization of the equation with x-axis being the amount of contour candidates, y is the pruned candidates" width=400 height=400> <figcaption><b>Fig. 3</b> - This is a simple visualization of the equation. The x-axis is the amount of contour candidates (the input) and y is the pruned candidates. This equation is simply a sideways parabola with an x-offset</figcaption> </figure> <p>Obviously, there is an edge case where the amount of candidates is smaller than the number of pruned candidates, so I simply set the pruned to the original amount.</p> <figure> <img src=post-res/image-filters-bling/mask_example_2.png alt="Visualization of the entire pipeline"> <figcaption><b>Fig. 4</b> - This visualization of the entire pipeline of finding the specular regions and computing bounding boxes from these regions</figcaption> </figure> <p>This pruning method is incredibly important because too many lens flares can result in an very unaesthetically pleasing result.</p> <figure> <video autoplay loop muted> <source src=post-res/image-filters-bling/airplane_final_without_gaussian_blur.mp4 type=video/mp4> Your browser does not support the video tag. </video> <video autoplay loop muted> <source src=post-res/image-filters-bling/airplane_try_1.mp4 type=video/mp4> Your browser does not support the video tag. </video> <figcaption><b>Fig. 5</b> - The top video clip is with the aforementioned pruning equation, while the bottom clip is simply a linear cutoff (example: take top 30 candidates).</figcaption> </figure> <h2 id=caveats>Caveats</h2> <p>There is one major issue with my implementation of Morgan and Tammazoousti's paper. I did not fully implement the k-region based segementation with (Suziki and Satoshi, 1985) algorithm. This piece of post-processing would reduce the overall specular false positives (refer to the figure below for an example), but there are two main reasons as to why this post-processing function was not implemented.</p> <p>The first, and more important reason, was that the results produced without post-processing functioned well-enough and created a level of "aesthetically-satisfying" results that it was deemed unnecessary to continue refining specular detection. The other reason is that the re-implementation of the segmentation algorithm would not have fit the timeline for this project.</p> <figure> <img src=post-res/image-filters-bling/example_gradient.png alt="Visualization of the gradient fix"> <figcaption><b>Fig. 6</b> - The image on the left is the specular mask without the segmentation that was detailed in the paper; the image in the center is with the method; the image to the right is the original input image. These images were taken from the original paper.</figcaption> </figure> <p>There is another caveat that is not directly related to Morgan and Tammazoousti's algorithm. It is the fact that the utilization of this algorithm (in whatever form) makes this bling filter far more "strict" in applying lens flare to an image than the TikTok bling filter. You will be in environments where there is not necessarily any specular areas, but the TikTok bling filter will pick an object's edge and apply a lens flare. I believe this kind of scenario is much harder to replicate in my filter program.</p> <h1 id=adding-lens-flares>Adding "Lens Flares"</h1> <p>After generating the specular bounding boxes, the next step is to generate lens flares that are blended into the image and vary in terms of size.</p> <figure> <img src=post-res/image-filters-bling/flare_template.png alt="Example of lens filter"> <figcaption><b>Fig. 7</b> - This is an example of the lens flare template used in the algorithm. It is broken into its RGB channels to be in used in a 1-D mask</figcaption> </figure> <p>We can calculate the center position of the list of bounding boxes which will be used to generate a larger region of interest (RoI). This RoI is what will be used to determine the sizing of a lens flare. The lens flare template is used as a mask. Its complexity must be reduced from three dimensions to one, but if you already have a separable image format based on color channels, you can simply craft three unique lens flares on each channel. This is the reason why the image in the figure above has a color tinge on the border.</p> <p>A lens flare mask is randomly chosen, resized to fit inside a given RoI, and then, with a color selected, the mask itself is used in a manual blending of the RoI and the chosen color. The blending is very similar to <code>cv2.addWeighted</code>'s formula (<code>dst = src1*alpha + src2*beta + gamma</code>). </p> <p>Additionally, I computed the average hue in the given RoI and set the value very high to get a whited-out version of the average hue. Substituting that color for the aforementioned initial color can give the lens flares some tinged edges, but I have learned through testing that this effect is almost unnoticeable in video format due to compression. </p> <h1 id=bloom-effect>Bloom Effect</h1> <p>The last post-processing method for the bling filter is the bloom shader effect. This shader effect can be described as adding "haze" to an image, but not in a way that is uniform, like straight Gaussian Blur can to an image. I borrowed the bloom shader code from this article by <a href=https://programmersought.com/article/20244091636/ rel=nofollow target=_blank>ProgrammerSought</a>. The algorithm applies a simple Gaussian Blur on a copy instance of the image. Afterwards, it loops through the original image and applies this equation $(x+y)-(x*y/255)$, where $x$ is the pixel value from the <strong>original</strong> image and $y$ is the pixel value from the <strong>gaussian blur</strong> image. This formula will increase the image's brightness and blends the smoothness of the gaussian blur with the edges of the original image.</p> <pre class=language-><code class=language->gauss_img = cv2.GaussianBlur(img.copy(), (5,5), sigmaX=5, sigmaY=5).astype(np.float64)
for x in range(len(gauss_img)):
  for y in range(len(gauss_img[x])):
    gauss_img[x][y][0] = img[x][y][0]+gauss_img[x][y][0]-img[x][y][0]*gauss_img[x][y][0]/255
    gauss_img[x][y][1] = img[x][y][1]+gauss_img[x][y][1]-img[x][y][1]*gauss_img[x][y][1]/255
    gauss_img[x][y][2] = img[x][y][2]+gauss_img[x][y][2]-img[x][y][2]*gauss_img[x][y][2]/255
return gauss_img.astype(np.uint8)</code></pre><h1 id=results>Results</h1> <table> <tr> <th>Composite Video</th> <th>Specular Mask</th> </tr> <tr> <td> <video autoplay loop muted> <source src=post-res/image-filters-bling/airplane_final.mp4 type=video/mp4> Your browser does not support the video tag. </video> </td> <td> <video autoplay loop muted> <source src=post-res/image-filters-bling/airplane_final_mask.mp4 type=video/mp4> Your browser does not support the video tag. </video> </td> </tr> <tr> <td> <video autoplay loop muted> <source src=post-res/image-filters-bling/truck.mp4 type=video/mp4> Your browser does not support the video tag. </video> </td> <td> <video autoplay loop muted> <source src=post-res/image-filters-bling/truck_mask.mp4 type=video/mp4> Your browser does not support the video tag. </video> </td> </tr> <tr> <td> <img src=post-res/image-filters-bling/airplane2_out.png> </td> <td> <img src=post-res/image-filters-bling/airplane2_mask.png> </td> </tr> </table> <h1 id=notes>Notes</h1> <p><strong>[1]</strong> - Figure 1's image source taken from thumbnail of this video: <a href="https://www.youtube.com/watch?v=0GFSppL1CRQ" rel=nofollow target=_blank>https://www.youtube.com/watch?v=0GFSppL1CRQ</a></p> <p><strong>[2]</strong> - GitHub link: <a href=https://github.com/vjsrinivas/image-filter rel=nofollow target=_blank>https://github.com/vjsrinivas/image-filter</a> </p> <p><strong>[3]</strong> - The lens flares do not rotate due to project time constraint, but the rotated versions of lens flares do exist. The code just does not take into account these images.</p> <p><strong>[4]</strong> - Currently, the code is written in pure Python, and runs slow on any input due to the nature of Python <code>for</code> loops. Later on, I hope to port to Cython or pure C++ to get realtime speeds. </p> <p><strong>[5]</strong> - The video clips and images were taken from <a href=https://www.pexels.com/ rel=nofollow target=_blank>Pexels</a>, and I give my thanks to the creators of the videos I used, who put their work out for free use.</p> </div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{title:"Bling Filter",description:"This article is part of a larger thread (refer to image-filters.md)",slug:"image-filters-bling",html:"\u003Clink rel='stylesheet' href='post-res\u002Fimage-filters-bling\u002Fstyle.css'\u003E\n\n\u003Ch2 id=\"table-of-contents\"\u003ETable of Contents\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='style-introduction';\" href=\"javascript:;\"\u003EStyle Introduction\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='implementing-specular-detection';\" href=\"javascript:;\"\u003EImplementing Specular Detection\u003C\u002Fa\u003E\u003Cul\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='pre-processing';\" href=\"javascript:;\"\u003EPre-processing\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='thresholding';\" href=\"javascript:;\"\u003EThresholding\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='post-processing';\" href=\"javascript:;\"\u003EPost-processing\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='caveats';\" href=\"javascript:;\"\u003ECaveats\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='adding-lens-flares';\" href=\"javascript:;\"\u003EAdding &quot;Lens Flares&quot;\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='bloom-effect';\" href=\"javascript:;\"\u003EBloom Effect\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='results';\" href=\"javascript:;\"\u003EResults\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='notes';\" href=\"javascript:;\"\u003ENotes\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch1 id=\"style-introduction\"\u003EStyle Introduction\u003C\u002Fh1\u003E\n\u003Cp\u003EThis filter, which I will be referring to as the &quot;bling filter&quot;, is a filter I happened to see a lot on TikTok as of recently. Mainly used to stylize a person&#39;s face or a scenic view as something out of a late-2000s movie, the bling filter applies cartoonized lens flares and adds a bloom shader to an image.\u003C\u002Fp\u003E\n\u003Cp\u003EAlthough, it is important to note that some TikToks use a custom bling effect that is created with software suites such as Adobe After Effects. These types of TikToks have lens flares that are far more realistic, with the prism color effect on the fringes of each lens flare. For the sake of simplicity, I did not spend my time trying to achieve these kinds of results.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fimage-filters-bling\u002Fexample1.png\" width=\"300\"  alt=\"Example of simple bling filter\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 1\u003C\u002Fb\u003E - This is an example of the bling filter that can be used in the TikTok app\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cp\u003EIn order to even start an attempt at recreating this filter, I had to first determine the location of where hypothetical lens flares would go. How would I go about doing that? From briefly analyzing some of video clips with this filter, I came to the conclusion that computing regions where there were specularities could be a fair way of determining lens flare positions. \u003C\u002Fp\u003E\n\u003Cp\u003ETo go about detecting specularity algorithmically, accurately, and consistently was a daunting task for me at first. But by searching on Google and arxiv, I eventually found a paper by the name of \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fieeexplore.ieee.org\u002Fdocument\u002F7294821\"\u003E&quot;Generic and real-time detection of specular reflections in images&quot;\u003C\u002Fa\u003E. This paper was written by Alexandre Morgand and Mohamed Tamaazousti from the Vision &amp; Content Engineering Laboratory and CEA LIST, respectively.\u003C\u002Fp\u003E\n\u003Ch1 id=\"implementing-specular-detection\"\u003EImplementing Specular Detection\u003C\u002Fh1\u003E\n\u003Cp\u003EMorgan and Tamaazousti&#39;s paper discusses a method of dynamically creating a threshold for specular detection as well as some pre and post-processing functions that help remove false positives.\u003C\u002Fp\u003E\n\u003Ch2 id=\"pre-processing\"\u003EPre-processing\u003C\u002Fh2\u003E\n\u003Cp\u003EThe pre-processing of an image is intended to reduce noise and equalize any highly-saturated images, which could cause false specular detections or malformed specular masks. With according to the paper, I implemented the following contrast equalization algorithm but inverted the inequalities in both conditionals (\u003Ccode\u003E&gt;\u003C\u002Fcode\u003E instead of \u003Ccode\u003E≤\u003C\u002Fcode\u003E):\u003C\u002Fp\u003E\n\u003Cpre class=\"language-\"\u003E\u003Ccode class=\"language-\"\u003Econtrast = 1\nif Brightness ≤ Tb then\n  while Brightness ≤ Tb do\n    contrast &lt;- contrast - 0.01\n    Image_pixels = contrast * Image_pixels\n    Compute(Brightness)\n  end while\nend if\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EFor reference, the brightness of an image can be computed with the following calculation:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\nBrightness =  \\sqrt(0.241*(C_R^2)+0.691*(C_G^2) + 0.068*(C_B^2))\u002F(Width * Height)\n$$\u003C\u002Fp\u003E\n\u003Cpre class=\"language-\"\u003E\u003Ccode class=\"language-\"\u003Edef calculateBrightness(img,w,h):\n    # cr, cg, cb = red channel, green channel, blue channel\n    # normalizes luminance to 0-1 (Y\u002Fwidth*height)\n    rcoef, gcoef, bcoef = 0.241,0.691,0.068\n    r,g,b = img[:,:,2], img[:,:,1], img[:,:,0]\n\n    # need to square:\n    r = rcoef*(r**2)\n    g = gcoef*(g**2)\n    b = bcoef*(b**2)\n    _out = np.sqrt(r+g+b)\u002F(w*h)\n    return np.sum(_out)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EAfter this pre-processing is complete, we need to switch from RGB to HSV color space in order to utilize the value and saturation details of the image.\u003C\u002Fp\u003E\n\u003Ch2 id=\"thresholding\"\u003EThresholding\u003C\u002Fh2\u003E\n\u003Cp\u003EThresholding can be basically be described as a process of assigning 0 or 1 (or (0,0,0) and (255,255,255) in RGB space) to each pixel of an image based on a given conditional. For our conditional, it will be determined by the following defined variables:\u003C\u002Fp\u003E\n\u003Cp\u003E\\[ T_v = Brightness * k_v \\]\n\\[\nT_s  = 170\n\\text{ (estimated by trial and error ) }\n\\]\u003C\u002Fp\u003E\n\u003Cp\u003E$T_v$ is a threshold based on the intensity with the value channel of the image in HSV color space. The paper defines $T_v$ as the product of Brightness (of the image) and a coefficient called $k_v$, but in reality, to compute $T_v$, we need to use the following linear equation:\u003C\u002Fp\u003E\n\u003Cp\u003E\\[ y = 2x \\]\u003C\u002Fp\u003E\n\u003Cp\u003EWith y being equal to $T_v$ and $x$ being the calculated $Brightness$ of an image. \u003C\u002Fp\u003E\n\u003Cp\u003EAfter calculating both threshold values, we have to take into account the circumstances when a certain ratio of the image is white and adjust both our threshold values:\u003C\u002Fp\u003E\n\u003Cp\u003E\\[ \n\\text{ if }\nHistogram_{Value}(255) &gt; (Image_{size} \u002F 3)\n\\]\u003C\u002Fp\u003E\n\u003Cp\u003E\\[\n\\text{ then }\nT_s = 30\n\\text{ and }\nT_v = 245\n\\]\u003C\u002Fp\u003E\n\u003Cp\u003EFinally, we can apply the following conditionals in our actual thresholding function:\u003C\u002Fp\u003E\n\u003Cp\u003E\\[\n\\text{ if }\nS(x) &lt; T_s\n\\text{ and }\nV(x) &gt; T_v\n\\]\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ENote:\u003C\u002Fstrong\u003E S(x) and V(x) are the saturation and value of \u003Cstrong\u003Eeach pixel.\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Fimage-filters-bling\u002Fmask_example.png\" alt=\"Example of thresholding with specular mask on righthand side and original image on the lefthand side\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 2\u003C\u002Fb\u003E - This is an example of an image being processed and producing a specular mask\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch2 id=\"post-processing\"\u003EPost-processing\u003C\u002Fh2\u003E\n\u003Cp\u003EAfter the thresholding portion of the code has created a &quot;specular&quot; mask, we must create a method that extracts regions that could be candidates for a lens flare and then find a way of pruning excess candidates.\u003C\u002Fp\u003E\n\u003Cp\u003ETo create the regions, I utilized the \u003Ccode\u003Ecv2.findContours\u003C\u002Fcode\u003E function to find contours within the mask. The function returns a grouping of contours with a hierarchy, but my concern is just the groupings themselves.\u003C\u002Fp\u003E\n\u003Cp\u003EThe majority of the contours are very small (1x1 or 2x2) in size, so I prune the contour candidates will less than an area of four out. After filtering, I sorted the contours from largest to smallest area and employed the following function to determine a rough estimate of usable contour candidates:\u003C\u002Fp\u003E\n\u003Cp\u003E\\[ y = sqrt(2*(x+10))\u002F2 \\]\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg width=400 height=400 src=\"post-res\u002Fimage-filters-bling\u002Fchart.png\" alt=\"Visualization of the equation with x-axis being the amount of contour candidates, y is the pruned candidates\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 3\u003C\u002Fb\u003E - This is a simple visualization of the equation. The x-axis is the amount of contour candidates (the input) and y is the pruned candidates. This equation is simply a sideways parabola with an x-offset\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cp\u003EObviously, there is an edge case where the amount of candidates is smaller than the number of pruned candidates, so I simply set the pruned to the original amount.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Fimage-filters-bling\u002Fmask_example_2.png\" alt=\"Visualization of the entire pipeline\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 4\u003C\u002Fb\u003E - This visualization of the entire pipeline of finding the specular regions and computing bounding boxes from these regions\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cp\u003EThis pruning method is incredibly important because too many lens flares can result in an very unaesthetically pleasing result.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Fimage-filters-bling\u002Fairplane_final_without_gaussian_blur.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Fimage-filters-bling\u002Fairplane_try_1.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 5\u003C\u002Fb\u003E - The top video clip is with the aforementioned pruning equation, while the bottom clip is simply a linear cutoff (example: take top 30 candidates).\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch2 id=\"caveats\"\u003ECaveats\u003C\u002Fh2\u003E\n\u003Cp\u003EThere is one major issue with my implementation of Morgan and Tammazoousti&#39;s paper. I did not fully implement the k-region based segementation with (Suziki and Satoshi, 1985) algorithm. This piece of post-processing would reduce the overall specular false positives (refer to the figure below for an example), but there are two main reasons as to why this post-processing function was not implemented.\u003C\u002Fp\u003E\n\u003Cp\u003EThe first, and more important reason, was that the results produced without post-processing functioned well-enough and created a level of &quot;aesthetically-satisfying&quot; results that it was deemed unnecessary to continue refining specular detection. The other reason is that the re-implementation of the segmentation algorithm would not have fit the timeline for this project.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Fimage-filters-bling\u002Fexample_gradient.png\" alt=\"Visualization of the gradient fix\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 6\u003C\u002Fb\u003E - The image on the left is the specular mask without the segmentation that was detailed in the paper; the image in the center is with the method; the image to the right is the original input image. These images were taken from the original paper.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cp\u003EThere is another caveat that is not directly related to Morgan and Tammazoousti&#39;s algorithm. It is the fact that the utilization of this algorithm (in whatever form) makes this bling filter far more &quot;strict&quot; in applying lens flare to an image than the TikTok bling filter. You will be in environments where there is not necessarily any specular areas, but the TikTok bling filter will pick an object&#39;s edge and apply a lens flare. I believe this kind of scenario is much harder to replicate in my filter program.\u003C\u002Fp\u003E\n\u003Ch1 id=\"adding-lens-flares\"\u003EAdding &quot;Lens Flares&quot;\u003C\u002Fh1\u003E\n\u003Cp\u003EAfter generating the specular bounding boxes, the next step is to generate lens flares that are blended into the image and vary in terms of size.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Fimage-filters-bling\u002Fflare_template.png\" alt=\"Example of lens filter\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 7\u003C\u002Fb\u003E - This is an example of the lens flare template used in the algorithm. It is broken into its RGB channels to be in used in a 1-D mask\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cp\u003EWe can calculate the center position of the list of bounding boxes which will be used to generate a larger region of interest (RoI). This RoI is what will be used to determine the sizing of a lens flare. The lens flare template is used as a mask. Its complexity must be reduced from three dimensions to one, but if you already have a separable image format based on color channels, you can simply craft three unique lens flares on each channel. This is the reason why the image in the figure above has a color tinge on the border.\u003C\u002Fp\u003E\n\u003Cp\u003EA lens flare mask is randomly chosen, resized to fit inside a given RoI, and then, with a color selected, the mask itself is used in a manual blending of the RoI and the chosen color. The blending is very similar to \u003Ccode\u003Ecv2.addWeighted\u003C\u002Fcode\u003E&#39;s formula (\u003Ccode\u003Edst = src1*alpha + src2*beta + gamma\u003C\u002Fcode\u003E). \u003C\u002Fp\u003E\n\u003Cp\u003EAdditionally, I computed the average hue in the given RoI and set the value very high to get a whited-out version of the average hue. Substituting that color for the aforementioned initial color can give the lens flares some tinged edges, but I have learned through testing that this effect is almost unnoticeable in video format due to compression.  \u003C\u002Fp\u003E\n\u003Ch1 id=\"bloom-effect\"\u003EBloom Effect\u003C\u002Fh1\u003E\n\u003Cp\u003EThe last post-processing method for the bling filter is the bloom shader effect. This shader effect can be described as adding &quot;haze&quot; to an image, but not in a way that is uniform, like straight Gaussian Blur can to an image. I borrowed the bloom shader code from this article by \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fprogrammersought.com\u002Farticle\u002F20244091636\u002F\"\u003EProgrammerSought\u003C\u002Fa\u003E. The algorithm applies a simple Gaussian Blur on a copy instance of the image. Afterwards, it loops through the original image and applies this equation $(x+y)-(x*y\u002F255)$, where $x$ is the pixel value from the \u003Cstrong\u003Eoriginal\u003C\u002Fstrong\u003E image and $y$ is the pixel value from the \u003Cstrong\u003Egaussian blur\u003C\u002Fstrong\u003E image. This formula will increase the image&#39;s brightness and blends the smoothness of the gaussian blur with the edges of the original image.\u003C\u002Fp\u003E\n\u003Cpre class=\"language-\"\u003E\u003Ccode class=\"language-\"\u003Egauss_img = cv2.GaussianBlur(img.copy(), (5,5), sigmaX=5, sigmaY=5).astype(np.float64)\nfor x in range(len(gauss_img)):\n  for y in range(len(gauss_img[x])):\n    gauss_img[x][y][0] = img[x][y][0]+gauss_img[x][y][0]-img[x][y][0]*gauss_img[x][y][0]\u002F255\n    gauss_img[x][y][1] = img[x][y][1]+gauss_img[x][y][1]-img[x][y][1]*gauss_img[x][y][1]\u002F255\n    gauss_img[x][y][2] = img[x][y][2]+gauss_img[x][y][2]-img[x][y][2]*gauss_img[x][y][2]\u002F255\nreturn gauss_img.astype(np.uint8)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch1 id=\"results\"\u003EResults\u003C\u002Fh1\u003E\n\u003Ctable\u003E\n  \u003Ctr\u003E\n    \u003Cth\u003EComposite Video\u003C\u002Fth\u003E\n    \u003Cth\u003ESpecular Mask\u003C\u002Fth\u003E\n  \u003C\u002Ftr\u003E\n  \u003Ctr\u003E\n    \u003Ctd\u003E\n      \u003Cvideo autoplay muted loop\u003E\n        \u003Csource src=\"post-res\u002Fimage-filters-bling\u002Fairplane_final.mp4\" type=\"video\u002Fmp4\"\u003E\n        Your browser does not support the video tag.\n      \u003C\u002Fvideo\u003E\n    \u003C\u002Ftd\u003E\n    \u003Ctd\u003E\n      \u003Cvideo autoplay muted loop\u003E\n        \u003Csource src=\"post-res\u002Fimage-filters-bling\u002Fairplane_final_mask.mp4\" type=\"video\u002Fmp4\"\u003E\n        Your browser does not support the video tag.\n      \u003C\u002Fvideo\u003E\n    \u003C\u002Ftd\u003E\n  \u003C\u002Ftr\u003E\n  \u003Ctr\u003E\n    \u003Ctd\u003E\n      \u003Cvideo autoplay muted loop\u003E\n        \u003Csource src=\"post-res\u002Fimage-filters-bling\u002Ftruck.mp4\" type=\"video\u002Fmp4\"\u003E\n        Your browser does not support the video tag.\n      \u003C\u002Fvideo\u003E\n    \u003C\u002Ftd\u003E\n    \u003Ctd\u003E\n      \u003Cvideo autoplay muted loop\u003E\n        \u003Csource src=\"post-res\u002Fimage-filters-bling\u002Ftruck_mask.mp4\" type=\"video\u002Fmp4\"\u003E\n        Your browser does not support the video tag.\n      \u003C\u002Fvideo\u003E\n    \u003C\u002Ftd\u003E\n  \u003C\u002Ftr\u003E\n    \u003Ctr\u003E\n    \u003Ctd\u003E\n      \u003Cimg src=\"post-res\u002Fimage-filters-bling\u002Fairplane2_out.png\"\u002F\u003E\n    \u003C\u002Ftd\u003E\n    \u003Ctd\u003E\n      \u003Cimg src=\"post-res\u002Fimage-filters-bling\u002Fairplane2_mask.png\" \u002F\u003E\n    \u003C\u002Ftd\u003E\n  \u003C\u002Ftr\u003E\n\u003C\u002Ftable\u003E\n\n\n\u003Ch1 id=\"notes\"\u003ENotes\u003C\u002Fh1\u003E\n\u003Cp\u003E\u003Cstrong\u003E[1]\u003C\u002Fstrong\u003E - Figure 1&#39;s image source taken from thumbnail of this video: \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=0GFSppL1CRQ\"\u003Ehttps:\u002F\u002Fwww.youtube.com\u002Fwatch?v=0GFSppL1CRQ\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[2]\u003C\u002Fstrong\u003E - GitHub link: \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fgithub.com\u002Fvjsrinivas\u002Fimage-filter\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fvjsrinivas\u002Fimage-filter\u003C\u002Fa\u003E \u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[3]\u003C\u002Fstrong\u003E - The lens flares do not rotate due to project time constraint, but the rotated versions of lens flares do exist. The code just does not take into account these images.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[4]\u003C\u002Fstrong\u003E - Currently, the code is written in pure Python, and runs slow on any input due to the nature of Python \u003Ccode\u003Efor\u003C\u002Fcode\u003E loops. Later on, I hope to port to Cython or pure C++ to get realtime speeds. \u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[5]\u003C\u002Fstrong\u003E - The video clips and images were taken from \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fwww.pexels.com\u002F\"\u003EPexels\u003C\u002Fa\u003E, and I give my thanks to the creators of the videos I used, who put their work out for free use.\u003C\u002Fp\u003E\n",created:"Sun, 28 Feb 1999 19:45:28 GMT",excerpt:"",author:a,readingTime:"11 min read",mediaFilePath:"post-res\u002Fimage-filters-bling\u002Fimage-filters-bling_thumb.mp4",tags:["art","topic"],art_credit:a}}}("Vijay Rajagopal"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');var s=document.createElement("script");try{new Function("if(0)import('')")();s.src="/client/client.4da8ec3a.js";s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main","/client/client.4da8ec3a.js")}document.head.appendChild(s)</script> 