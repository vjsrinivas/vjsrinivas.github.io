<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=logo.png rel=icon type=image/png> <script> MathJax = {
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				processEscapes: true
			},
			svg: {
				fontCache: 'global'
			}
		}; </script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async id=MathJax-script></script> <script src=https://kit.fontawesome.com/d1e7542e7f.js crossorigin=anonymous></script> <link href=client/main.677622768.css rel=stylesheet><link href=client/[slug].d2f2c26c.css rel=stylesheet><link href=client/client.4da8ec3a.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>EmojiCam ðŸ˜’ðŸ¤«ðŸ¥³</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class=svelte-wyfi55><ul class=svelte-wyfi55><li class=svelte-wyfi55><a href=blog class="svelte-wyfi55 btn-gen" aria-current=page rel=prefetch>Blog</a></li> <li class=svelte-wyfi55><a href=. class="svelte-wyfi55 btn-gen">Home</a></li> </ul></nav> <main class=svelte-1xjjggr> <div class="content svelte-1d9fknd"><h1>EmojiCam ðŸ˜’ðŸ¤«ðŸ¥³</h1> <hr> <p>This project was done for fun and was done within 24-hours. EmojiCam takes in a RGB frame and creates a frame with emojis replacing pixels. It runs in real-time and has a very simple image processing pipeline.</p> <h2 id=table-of-contents>Table of Contents</h2> <ul> <li><a href=javascript:; onclick="document.location.hash='processing-pipeline';">Processing Pipeline</a></li> <li><a href=javascript:; onclick="document.location.hash='quick-notes-about-implementation';">Quick Notes About Implementation</a></li> <li><a href=javascript:; onclick="document.location.hash='results';">Results</a></li> <li><a href=javascript:; onclick="document.location.hash='future';">Future</a></li> <li><a href=javascript:; onclick="document.location.hash='source';">Source</a></li> </ul> <h2 id=processing-pipeline>Processing Pipeline</h2> <figure> <img , alt="Image of cache processing for Emojicam" src=post-res/emojicam/emojicamprocesspipeline.png> <figcaption><b>Fig. 1</b> - Shows the general process of generating the "emoji mapping", where each color combination is assigned an emoji. To determine the best color-to-emoji match, I find the major colors within each emoji, do a ranking system, match them to the 256x256x256 matrix. Unresolved spaces are filled in with a simple distance calculation. The mapping is saved to file when the camera portion (Fig. 2) is started.</figcaption> </figure> <figure> <img , alt="Image of pipeline processing for Emojicam" src=post-res/emojicam/emojicamprocesspipeline2.png> <figcaption><b>Fig. 2</b> - Shows the general process of the actual mapping of emojis to a group of pixels. First, the incoming frame is pixelized to where the pixel box area is equal to the final size of the emoji that will overlay it. A black canvas with equal dimensions to the frame is created. We reference the emoji mapping from Figure 1, load the emoji, and place it in the same location as the given pixelized block (but within the black canvas). "Alpha blending" from the pixelized frame is done to tint the emojis. Refer to the Results section to see the difference between "Alpha blending" and the absence of it. </figcaption> </figure> <p>The processing pipeline of EmojiCam consists of a pre-launch process that computes a color-to-emoji mapping and caches this large table for the launch process. The launch process simply consists of pixelizing the image with a filter that represented the predefined size of an emoji. Each block from the pixelization is matched with an emoji from the cached table. The emoji is then masked onto an equally sized black canvas to the same location as the chunk. An alpha blend of the pixelated block is bled into this black canvas for better results.</p> <h2 id=quick-notes-about-implementation>Quick Notes About Implementation</h2> <p><strong>kMeans</strong> - So kMeans is a basic unsupervised clustering algorithm. More simply, given a number of cluster points to converge to, the algorithm can independently sort samples into assignment to one of the clusters. I used the <code>sklearn</code> implementation of kMeans, which will not guarantee to return a point for every cluster.</p> <p>I forced a three cluster ranking per emoji, which means each emoji has three color coordinates (R,G,B) with an associated percentage term that represents how many pixels out of the total pixels went to that cluster. This three-element list is ordered by this percentage term for the ranking system. </p> <p><strong>Ranking System</strong> - You can probably figure out that a 256x256x256 color matrix gets you a color space of <strong>16777216 unique colors</strong>. Unfortunately, there are only around 4000 emojis within the Twitter Emoji set. So every emoji will own multiple colors within the matrix. The ranking system starts by scrolling through every emoji, looking at its top-1 color and looking up this color in the matrix.</p> <p>If the matrix element is empty, then that emoji will be assigned to that element. If there is already another emoji in the element, we compare the percentages between the two, and the larger ratio wins. The losing emoji is placed in the nearest matrix element that isn't occupied (this is non-optimal since neighbors of the conflicting space can have a smaller ratio than the losing emoji). For the leftover spaces, we loop through these spaces and compute the distance from it to all the originally occupied areas. The closest emoji is assigned to the leftover spaces.</p> <h2 id=results>Results</h2> <video autoplay loop muted> <source src=post-res/emojicam/test_out-converted.mp4 type=video/mp4> Your browser does not support the video tag. </video> <p><strong>With Alpha-blending</strong></p> <video autoplay loop muted> <source src=post-res/emojicam/test_out_no_weights-converted.mp4 type=video/mp4> Your browser does not support the video tag. </video> <p><strong>Without Alpha-blending</strong></p> <video autoplay loop muted> <source src=post-res/emojicam/test_out_stream-converted.mp4 type=video/mp4> Your browser does not support the video tag. </video> <p><strong>Live camera feed with Alpha-blending</strong></p> <video autoplay loop muted> <source src=post-res/emojicam/test_out_stream_wo_weight-converted.mp4 type=video/mp4> Your browser does not support the video tag. </video> <p><strong>Live camera feed without Alpha-blending</strong></p> <br> <br> <h2 id=future>Future</h2> <p>As mentioned previously, there are areas of improvements. This is mainly for the emoji-mapping algorithm. The main pitfall is how we deal with losing emojis within the ranking system. Having the losing emoji fight other emojis around the initial location for their location would probably yield in better color matching. Alpha blending, in my opinion, is kind of cheating even though the only main metric is visual appeal. I would actually mask the blending to just the opaque areas of the emoji matrix. For clarification, the emojis are in PNG format and have 4 channels (blue, green, red, alpha). This would give a fairer blending to just the emoji, rather than the black canvas.</p> <p>Finally, this could easily be a filter in an application like Snapchat or Instagram (if it already isn't). I would like to investigate implementing this algorithm on Android and seeing the performance difference, and if it retains its real-time speeds.</p> <h2 id=source>Source</h2> <p>You can find the project here: <a href=https://github.com/vjsrinivas/emojicam rel=nofollow target=_blank>https://github.com/vjsrinivas/emojicam</a></p> </div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{title:"EmojiCam ðŸ˜’ðŸ¤«ðŸ¥³",description:"This project was done for fun and was completed within 24-hours. EmojiCam takes in a RGB frame and creates a frame with emojis replacing pixels. It runs in real-time and has a very simple image processing pipeline.",slug:"emojicam",html:"\u003Cp\u003EThis project was done for fun and was done within 24-hours. EmojiCam takes in a RGB frame and creates a frame with emojis replacing pixels. It runs in real-time and has a very simple image processing pipeline.\u003C\u002Fp\u003E\n\u003Ch2 id=\"table-of-contents\"\u003ETable of Contents\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='processing-pipeline';\" href=\"javascript:;\"\u003EProcessing Pipeline\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='quick-notes-about-implementation';\" href=\"javascript:;\"\u003EQuick Notes About Implementation\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='results';\" href=\"javascript:;\"\u003EResults\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='future';\" href=\"javascript:;\"\u003EFuture\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='source';\" href=\"javascript:;\"\u003ESource\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"processing-pipeline\"\u003EProcessing Pipeline\u003C\u002Fh2\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Femojicam\u002Femojicamprocesspipeline.png\", alt=\"Image of cache processing for Emojicam\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 1\u003C\u002Fb\u003E - Shows the general process of generating the \"emoji mapping\", where each color combination is assigned an emoji. To determine the best color-to-emoji match, I find the major colors within each emoji, do a ranking system, match them to the 256x256x256 matrix. Unresolved spaces are filled in with a simple distance calculation. The mapping is saved to file when the camera portion (Fig. 2) is started.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Femojicam\u002Femojicamprocesspipeline2.png\", alt=\"Image of pipeline processing for Emojicam\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 2\u003C\u002Fb\u003E - Shows the general process of the actual mapping of emojis to a group of pixels. First, the incoming frame is pixelized to where the pixel box area is equal to the final size of the emoji that will overlay it. A black canvas with equal dimensions to the frame is created. We reference the emoji mapping from Figure 1, load the emoji, and place it in the same location as the given pixelized block (but within the black canvas). \"Alpha blending\" from the pixelized frame is done to tint the emojis. Refer to the Results section to see the difference between \"Alpha blending\" and the absence of it. \u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cp\u003EThe processing pipeline of EmojiCam consists of a pre-launch process that computes a color-to-emoji mapping and caches this large table for the launch process. The launch process simply consists of pixelizing the image with a filter that represented the predefined size of an emoji. Each block from the pixelization is matched with an emoji from the cached table. The emoji is then masked onto an equally sized black canvas to the same location as the chunk. An alpha blend of the pixelated block is bled into this black canvas for better results.\u003C\u002Fp\u003E\n\u003Ch2 id=\"quick-notes-about-implementation\"\u003EQuick Notes About Implementation\u003C\u002Fh2\u003E\n\u003Cp\u003E\u003Cstrong\u003EkMeans\u003C\u002Fstrong\u003E - So kMeans is a basic unsupervised clustering algorithm. More simply, given a number of cluster points to converge to, the algorithm can independently sort samples into assignment to one of the clusters. I used the \u003Ccode\u003Esklearn\u003C\u002Fcode\u003E implementation of kMeans, which will not guarantee to return a point for every cluster.\u003C\u002Fp\u003E\n\u003Cp\u003EI forced a three cluster ranking per emoji, which means each emoji has three color coordinates (R,G,B) with an associated percentage term that represents how many pixels out of the total pixels went to that cluster. This three-element list is ordered by this percentage term for the ranking system. \u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ERanking System\u003C\u002Fstrong\u003E - You can probably figure out that a 256x256x256 color matrix gets you a color space of \u003Cstrong\u003E16777216 unique colors\u003C\u002Fstrong\u003E. Unfortunately, there are only around 4000 emojis within the Twitter Emoji set. So every emoji will own multiple colors within the matrix. The ranking system starts by scrolling through every emoji, looking at its top-1 color and looking up this color in the matrix.\u003C\u002Fp\u003E\n\u003Cp\u003EIf the matrix element is empty, then that emoji will be assigned to that element. If there is already another emoji in the element, we compare the percentages between the two, and the larger ratio wins. The losing emoji is placed in the nearest matrix element that isn&#39;t occupied (this is non-optimal since neighbors of the conflicting space can have a smaller ratio than the losing emoji). For the leftover spaces, we loop through these spaces and compute the distance from it to all the originally occupied areas. The closest emoji is assigned to the leftover spaces.\u003C\u002Fp\u003E\n\u003Ch2 id=\"results\"\u003EResults\u003C\u002Fh2\u003E\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Femojicam\u002Ftest_out-converted.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EWith Alpha-blending\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Femojicam\u002Ftest_out_no_weights-converted.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EWithout Alpha-blending\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Femojicam\u002Ftest_out_stream-converted.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\u003Cp\u003E\u003Cstrong\u003ELive camera feed with Alpha-blending\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Femojicam\u002Ftest_out_stream_wo_weight-converted.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\u003Cp\u003E\u003Cstrong\u003ELive camera feed without Alpha-blending\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cbr\u003E\n\u003Cbr\u003E\n\n\u003Ch2 id=\"future\"\u003EFuture\u003C\u002Fh2\u003E\n\u003Cp\u003EAs mentioned previously, there are areas of improvements. This is mainly for the emoji-mapping algorithm. The main pitfall is how we deal with losing emojis within the ranking system. Having the losing emoji fight other emojis around the initial location for their location would probably yield in better color matching. Alpha blending, in my opinion, is kind of cheating even though the only main metric is visual appeal. I would actually mask the blending to just the opaque areas of the emoji matrix. For clarification, the emojis are in PNG format and have 4 channels (blue, green, red, alpha). This would give a fairer blending to just the emoji, rather than the black canvas.\u003C\u002Fp\u003E\n\u003Cp\u003EFinally, this could easily be a filter in an application like Snapchat or Instagram (if it already isn&#39;t). I would like to investigate implementing this algorithm on Android and seeing the performance difference, and if it retains its real-time speeds.\u003C\u002Fp\u003E\n\u003Ch2 id=\"source\"\u003ESource\u003C\u002Fh2\u003E\n\u003Cp\u003EYou can find the project here: \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fgithub.com\u002Fvjsrinivas\u002Femojicam\"\u003Ehttps:\u002F\u002Fgithub.com\u002Fvjsrinivas\u002Femojicam\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n",created:"Sat, 12 Feb 2022 12:15:49 GMT",excerpt:"This project was done for fun and was done within 24-hours. EmojiCam takes in a RGB frame and creates a frame with emojis replacing pixels. It runs in real-time and has a very simple image processing pipeline.\n",author:a,readingTime:"5 min read",mediaFilePath:"post-res\u002Femojicam\u002Femojicam_thumb.mp4",tags:["image processing"],art_credit:a}}}("Vijay Rajagopal"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');var s=document.createElement("script");try{new Function("if(0)import('')")();s.src="/client/client.4da8ec3a.js";s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main","/client/client.4da8ec3a.js")}document.head.appendChild(s)</script> 