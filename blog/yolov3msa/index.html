<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=logo.png rel=icon type=image/png> <script> MathJax = {
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				processEscapes: true
			},
			svg: {
				fontCache: 'global'
			}
		}; </script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async id=MathJax-script></script> <script src=https://kit.fontawesome.com/d1e7542e7f.js crossorigin=anonymous></script> <link href=client/main.677622768.css rel=stylesheet><link href=client/[slug].d2f2c26c.css rel=stylesheet><link href=client/client.4da8ec3a.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Effects of Multiheaded Self-Attention on YOLOv3</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class=svelte-wyfi55><ul class=svelte-wyfi55><li class=svelte-wyfi55><a href=blog class="svelte-wyfi55 btn-gen" aria-current=page rel=prefetch>Blog</a></li> <li class=svelte-wyfi55><a href=. class="svelte-wyfi55 btn-gen">Home</a></li> </ul></nav> <main class=svelte-1xjjggr> <div class="content svelte-1d9fknd"><h1>Effects of Multiheaded Self-Attention on YOLOv3</h1> <hr> <link href=post-res/yolov3msa/style.css rel=stylesheet> <p>For my graduate Deep Learning (DL) class, we had a final project assignment that tasked us to conduct novel experiments on a DL area. I decided to implement a MSA module to a common, generic, fast object detector called YOLOv3. We see improvements in classification but significant decrease in localization performance for most network configurations.</p> <h1 id=introduction>Introduction</h1> <p>With the introduction of Transformers into deep learning, tasks related to natural language processing (NLP) have im-proved tremendously compared to previous types of networks and architecture. Transformers calculate ”self-attention” in order to contextualize the sequence of input data they are given. The Transformer architecture is now being utilized in computer vision (CV) tasks, such as classification and object detection. With the introduction of ViT for image classification [9], Transformers seem to comparable or even better performance compared to convolutional neural networks (CNNs). Naturally, there has also been work done on combining the Transformer’s self attention mechanism with convolutional layers. Particularly, the work done by Park et al. ([9]) has shown a significant improvement in classification accuracy of a blend of convolutional layers and multi-headed self attention (MSA) modules.</p> <p>For residual-based network, [9] empirically established that replacing a residual block with MSA modules at the end of each stage in an alternating fashion helped those networks improve classification accuracy. In this paper, we extended the work done in [9] to object detection and attempted to apply the same MSA modules toward object detection networks. More specifically, we modified a real-time, single stage object detector called YOLOv3 with differing layouts of MSA modules and observe the detection performance. </p> <p>Through our analysis, we conclude that MSAs could increase mAP performance, but directly applying them to any object detection network is not a straight-forward pro- cess. Our experiments show that <strong>the majority of the YOLOv3 MSA configurations decrease performance.</strong> We attribute that to non-optimal augmentations, non-optimal hyperparameters within the MSA modules, and non-convex loss surfaces for localization-based functions that are present in YOLOv3.</p> <h1 id=architectures>Architectures</h1> <figure> <img alt="A diagram describing the general structure of YOLOv3" src=./post-res/yolov3msa/diagrams/yolov3_structure.svg width=300> <figcaption><b>Fig. 1</b> - <b>General structure of YOLOv3.</b> The network is composed of a feature extractor called Darknet53. The feature map output from the feature extractor is actually composed of three different feature maps. Each feature maps are from different parts of the feature extractor and are supposed to represent objects at different ”scales”. Each feature map scale is processed in parallel in the YOLO head. The output of the network is three different sets of predictions.</figcaption> </figure> <figure> <img alt="A diagram describing the different structures of YOLOv3 MSA" src=./post-res/yolov3msa/diagrams/yolov3_structure_all_msas.svg width=300> <figcaption><b>Fig. 2</b> - <b>Structure of Darknet53 as a classifier.</b> The classifier reduces the feature map output from the Residual structure into a vector of class probabilities with Global Average Pooling (GAP). This allows the CNN to have flexible input sizes while maintaining the consistent class-based probability output.</figcaption> </figure> <h1 id=methodology>Methodology</h1> <h2 id=classification-training-procedures>Classification Training Procedures</h2> <p>For classification training, we utilized Adam as our primary optimization method because of established literature showing quicker loss convergence compared to SGD. We trained for 350 epochs with a learning rate of 0.0005 and a Cosine Annealing learning rate scheduler. Both networks were trained with a batchsize of 1024, and the input size was 32 by 32.</p> <p>Both networks used Categorical Cross Entropy (CCE) for their loss function. We take the model with the best validation performance and display the test results.</p> <h2 id=object-detection-training-procedures>Object Detection Training Procedures</h2> <p>For object detection training, we kept in-line with the default training procedures. YOLOv3 loss function is a multi-task loss function composed of three different losses. Each of the losses and their descriptions are as follows:</p> <ul> <li><strong>Classification</strong> - Applying CCE with the prediction and ground truth labels for corresponding boxes</li> <li><strong>Coordinate Localization</strong> - The L2 distance between a matched prediction box coordinates and its corresponding ground truth box's coordinates </li> <li><strong>Objectness</strong> - A logistic regression using BCE to determine if a given area of an image contains an object or not</li> </ul> <p>For Oxford IIT Pet dataset, we trained each network for 350 epochs at a learning rate of 0.0001 with a batchsize of 16. For VOC, we trained each network for 56 epochs at a learning rate of 0.0001. Both training sets are using the same learning rate scheduler for all three loss functions. Both networks had an input size of 416 by 416.</p> <h1 id=datasets>Datasets</h1> <p>For this work, we utilize two main object detection datasets and one classification dataset. For the object detection task, we trained YOLOv3 on Oxford IIT Pet dataset and Visual Objects Challenge (VOC) dataset.</p> <h2 id=oxford-iit-pet-dataset>Oxford IIT Pet Dataset</h2> <p>The Oxford IIT Pet dataset is composed of "37 [pet categories] with roughly 200 images for each class" [10]. The annotation style is visualized in Figure 3, and a given method should output either the localization of the head or segmentation of the whole pet and the classification of the specific pet breed.</p> <figure> <img alt="Oxford IIT Pet annotation examples." src=./post-res/yolov3msa/diagrams/pet_annotations.jpg width=300> <figcaption><b>Fig. 3</b> - <b>Illustration of annotations provided in the Oxford IIT Pet dataset.</b> Each image is of one pet, and for our purposes, we focused on localizing the head of the animal and classifying the animal by breed. This annotation diagram was taken directly from [10]. </figcaption> </figure> <h2 id=visual-object-challenge-dataset>Visual Object Challenge Dataset</h2> <p>The VOC datasets are a set of multiple datasets based on various years of the challenges. In terms of literature importance, VOC2007 and VOC2012 were the most utilized datasets among the VOC family, which is why we are using them in our work. VOC2007 is composed of 20 classes with a total of 4,952 training images and 2,501 testing images. </p> <p>Similarly, VOC2012 contains the same 20 classes but contains a total of 5,171 training images and 5,823 validation images. From Figure 4, the annotation style is similar to the Oxford IIT Pet dataset, where the objects are labeled with a bounding box and each box is assigned one of the 20 classes. In this work, we combine VOC2007 and VOC2012 training set and test on the VOC2007 test set.</p> <figure> <img alt="VOC (Visual Object Context) annotation example" src=./post-res/yolov3msa/diagrams/voc2007_annotation_example.png width=300> <figcaption><b>Fig. 4</b> - <b>Illustration of annotations provided in VOC2007 dataset.</b> The dataset is composed of bounding boxes and segmentation masks. For our purposes, we are focusing on the bounding box. The image is taken from [3]. </figcaption> </figure> <h2 id=cifar-10>CIFAR-10</h2> <p>CIFAR-10 is a popular and very simple dataset that contains 10 classes. Each class is a generic object or subject, and the dataset is composed of 60,000 32x32 images [5].</p> <h1 id=results>Results</h1> <h2 id=classification-results>Classification Results</h2> <p>As mentioned in the Implementation section, we implement Darknet53 as a classifier and add MSA modifications seen in Figure 4. We trained on CIFAR-10 and see a notable improvement of 1.23% on Darknet53 with MSA compared to the original Darknet53 network.</p> <table> <thead> <tr> <th><strong>Model Type</strong></th> <th><strong>Top-1 Accuracy on CIFAR-10</strong></th> </tr> </thead> <tr> <td>Darknet53</td> <td>0.8803</td> </tr> <tr> <td>Darknet53 with MSA</td> <td><strong>0.8926 (+0.0123)</strong></td> </tr> </table> <br> <h2 id=object-detection-results>Object Detection Results</h2> <figure> <img alt="Graph showing mAP scores of all the networks trained on the Oxford IIT Pet dataset." src=./post-res/yolov3msa/diagrams/all_dog_maps.png width=300> <figcaption><b>Fig. 5</b> - <b>The mAP trend during training for Oxford IIT Pet testset and VOC2007 testset.</b> </figcaption> </figure> <figure> <img alt="" src=./post-res/yolov3msa/diagrams/all_voc_maps.png width=300> <figcaption><b>Fig. 6</b> - <b>The mAP trend during training for Oxford IIT Pet testset and VOC2007 testset.</b> </figcaption> </figure> <figure> <img alt="" src=./post-res/yolov3msa/diagrams/map_delta_dog.svg style=width:350px;display:inline> <img alt="" src=./post-res/yolov3msa/diagrams/map_delta_voc.svg style=width:350px;display:inline> <figcaption><b>Fig. 7</b> - <b>The mAP deltas for YOLOv3 MSAs on Oxford IIT Pet testset and VOC2007 testset.</b> The number of MSA modules is not always correlated with the loss or gain of performance in this particular network. </figcaption> </figure> <table> <thead> <tr> <th><strong>Model Type</strong></th> <th><strong>Oxford IIT Pet testset mAP @ 0.5</strong></th> <th><strong>VOC2007 testset mAP @ 0.5</strong></th> </tr> </thead> <tr> <td>YOLOv3</td> <td>0.92</td> <td>0.747</td> </tr> <tr> <td>YOLOv3 MSA 4</td> <td>0.909 (-0.011)</td> <td>0.739 (-0.008)</td> </tr> <tr> <td>YOLOv3 MSA 3</td> <td>0.928 (+0.008)</td> <td>0.726 (-0.021)</td> </tr> <tr> <td>YOLOv3 MSA 2</td> <td>0.905 (-0.015)</td> <td>0.732 (-0.015)</td> </tr> <tr> <td>YOLOv3 MSA 1</td> <td>0.919 (-0.001)</td> <td>0.739 (-0.008)</td> </tr> </table> <br> <table> <thead> <tr> <th><strong>Model Type + Remove Augmentations</strong></th> <th><strong>VOC2007 Testset mAP @ 0.5</strong></th> </tr> </thead> <tr> <td>YOLOv3 (No Augmentations)</td> <td>0.4564</td> </tr> <tr> <td>YOLOv3 MSA 4 (No Augmentations)</td> <td><strong>0.4871 (+0.0307)</strong></td> </tr> <tr> <td>YOLOv3 (No Mosaic)</td> <td><strong>0.7105</strong></td> </tr> <tr> <td>YOLOv3 MSA 4 (No Mosaic)</td> <td>0.6859 (-0.0246)</td> </tr> <tr> <td>YOLOv3 (No Mosaic + No EMA)</td> <td><strong>0.7060</strong></td> </tr> <tr> <td>YOLOv3 MSA 4 (No Mosaic + No EMA)</td> <td>0.6873 (-0.0187)</td> </tr> </table> <br> <h1 id=conclusions>Conclusions</h1> <p>From our experiments, we conclude the following:</p> <ul> <li>When applied to classifiers, MSA modules make a notable improvement in classification accuracy (reaffirming work done by [9])</li> <li>When applied to YOLOv3, MSA modules marginally decrease mAP performance</li> <li>In some cases, MSA modules can improve mAP performance, but these improvements are not in alignment with the number of MSAs present in the network and are marginal</li> <li>In terms of lost mAP, the number of MSA modules did not always correlate to how much mAP was lost.</li> <li>Training trends show that YOLOv3 MSAs with any number of MSA heads increase localization and objectness loss</li> <li>The more MSA heads a network has, the greater the objectness loss</li> </ul> <p><strong>1) Bad Localization:</strong> From the experimental results, we can see that MSA modules had an outsized negative affect on the localization and objectness portions of the loss function. But why? From [9], MSAs improve network metric performance by ”smoothing” the loss surface of a convex function. If the function is non-convex, MSA does not perform as well as a Conv layer. Unfortunately, L2 distance and logistic regression even with a convex loss function) are not guaranteed to be convex [8]. From Figure 13, it seems that both loss surfaces for the tested datasets (particularly for VOC) were not convex.</p> <p><strong>2) Training Procedures Matter:</strong> As stated in the list above, we did not see any notable improvements with MSA, and so we decided to investigate further into why. It is well-established that augmentations can drastically help with mAP performance, so we decided to investigate if a set of augmentations were holding back the MSA network rather than helping. Firstly, we removed ALL augmentations and trained YOLOv3 and YOLOv3 MSA 4. In this case, YOLOv3 MSA 4 outperformed YOLOv3 by 3% mAP.</p> <p>This is the biggest change in mAP recorded in this work thus far. Although the overall mAP for both networks is drastically lower than with any augmentations, it gave credence to our hypothesis that MSA-based networks were being negatively affected by a non-optimal set of augmentations. We then tried training the same networks without the Mosaic augmentation, which has helped CNN performance but had no conclusive effect on Transformer-based networks.</p> <p>Based on [7], EMA can negatively affect Transformer learning, so we also ran a training without Mosaic and EMA. All the results are located in Table III, but removal of these two augmentations still showed the MSA network lagging behind the CNN network. In this case, the mAP delta is greater than any of the deltas in Table II. Although, we affirm that EMA did have a negative effect on the MSA.</p> <p><strong>3) MSA Head Dimensions:</strong> From our Technical Approach section, we stated that each MSA module has a dimension length for a given head. We had set all the heads to a dimension of 64 to fit the models on the present hardware. From our literature review, networks such as ViT YOLO have their head dimensionality at 1024. A possible way of increasing performance would be to increase the head dimensions. It is clear that conventional CNN networks can benefit from the addition of MSAs. But from our experiments and conclusions, it comes with a bold asterisk. Regardless, we see the future of object detection incorporating a multitude of different layers to increase true scene interpretation.</p> <h1 id=references>References</h1> <p><strong>[1]</strong> Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection, 2020.</p> <p><strong>[2]</strong> Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p> <p><strong>[3]</strong> M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,and A. Zisserman. The PASCAL Visual Object ClassesChallenge 2007 (VOC2007) Results. <a href=http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html rel=nofollow target=_blank>http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html</a>.</p> <p><strong>[4]</strong> Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth, 2016.</p> <p><strong>[5]</strong> Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.</p> <p><strong>[6]</strong> Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to vision transformers. arXiv preprint arXiv:2104.05707, 2021.</p> <p><strong>[7]</strong> Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.</p> <p><strong>[8]</strong> Gabriele De Luca. Why does the cost function of logistic regression have a logarithmic expression?, Jul 2020.</p> <p><strong>[9]</strong> Namuk Park and Songkuk Kim. How do vision transformers work? arXiv preprint arXiv:2202.06709, 2022.</p> <p><strong>[10]</strong> Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.</p> <p><strong>[11]</strong> Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018.</p> <p><strong>[12]</strong> Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347–10357. PMLR, 2021.</p> <p><strong>[13]</strong> Zixiao Zhang, Xiaoqiang Lu, Guojin Cao, Yuting Yang, Licheng Jiao,and Fang Liu. Vit-yolo: Transformer-based yolo for object detection. InProceedings of the IEEE/CVF International Conference on ComputerVision, pages 2799–2808, 2021.</p> <p><strong>[14]</strong> Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.Random erasing data augmentation, 2017.</p> </div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Effects of Multiheaded Self-Attention on YOLOv3",description:"(Still a WIP) I tried to apply Multiheaded Self-Attention (MSA) to a generic object detector (YOLOv3). Here are some of my results.",slug:"yolov3msa",html:"\u003Clink rel='stylesheet' href='post-res\u002Fyolov3msa\u002Fstyle.css'\u003E\n\n\u003Cp\u003EFor my graduate Deep Learning (DL) class, we had a final project assignment that tasked us to conduct novel experiments on a DL area. I decided to implement a MSA module to a common, generic, fast object detector called YOLOv3. We see improvements in classification but significant decrease in localization performance for most network configurations.\u003C\u002Fp\u003E\n\u003Ch1 id=\"introduction\"\u003EIntroduction\u003C\u002Fh1\u003E\n\u003Cp\u003EWith the introduction of Transformers into deep learning, tasks related to natural language processing (NLP) have im-proved tremendously compared to previous types of networks and architecture. Transformers calculate ”self-attention” in order to contextualize the sequence of input data they are given. The Transformer architecture is now being utilized in computer vision (CV) tasks, such as classification and object detection. With the introduction of ViT for image classification [9], Transformers seem to comparable or even better performance compared to convolutional neural networks (CNNs). Naturally, there has also been work done on combining the Transformer’s self attention mechanism with convolutional layers. Particularly, the work done by Park et al. ([9]) has shown a significant improvement in classification accuracy of a blend of convolutional layers and multi-headed self attention (MSA) modules.\u003C\u002Fp\u003E\n\u003Cp\u003EFor residual-based network, [9] empirically established that replacing a residual block with MSA modules at the end of each stage in an alternating fashion helped those networks improve classification accuracy. In this paper, we extended the work done in [9] to object detection and attempted to apply the same MSA modules toward object detection networks. More specifically, we modified a real-time, single stage object detector called YOLOv3 with differing layouts of MSA modules and observe the detection performance. \u003C\u002Fp\u003E\n\u003Cp\u003EThrough our analysis, we conclude that MSAs could increase mAP performance, but directly applying them to any object detection network is not a straight-forward pro- cess. Our experiments show that \u003Cstrong\u003Ethe majority of the YOLOv3 MSA configurations decrease performance.\u003C\u002Fstrong\u003E We attribute that to non-optimal augmentations, non-optimal hyperparameters within the MSA modules, and non-convex loss surfaces for localization-based functions that are present in YOLOv3.\u003C\u002Fp\u003E\n\u003Ch1 id=\"architectures\"\u003EArchitectures\u003C\u002Fh1\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fyolov3msa\u002Fdiagrams\u002Fyolov3_structure.svg\" width=\"300\"  alt=\"A diagram describing the general structure of YOLOv3\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 1\u003C\u002Fb\u003E - \u003Cb\u003EGeneral structure of YOLOv3.\u003C\u002Fb\u003E The network is composed of a feature extractor called Darknet53. The feature map output from the feature extractor is actually composed of three different feature maps. Each feature maps are from different parts of the feature extractor and are supposed to represent objects at different ”scales”. Each feature map scale is processed in parallel in the YOLO head. The output of the network is three different sets of predictions.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fyolov3msa\u002Fdiagrams\u002Fyolov3_structure_all_msas.svg\" width=\"300\"  alt=\"A diagram describing the different structures of YOLOv3 MSA\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 2\u003C\u002Fb\u003E - \u003Cb\u003EStructure of Darknet53 as a classifier.\u003C\u002Fb\u003E The classifier reduces the feature map output from the Residual structure into a vector of class probabilities with Global Average Pooling (GAP). This allows the CNN to have flexible input sizes while maintaining the consistent class-based probability output.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch1 id=\"methodology\"\u003EMethodology\u003C\u002Fh1\u003E\n\u003Ch2 id=\"classification-training-procedures\"\u003EClassification Training Procedures\u003C\u002Fh2\u003E\n\u003Cp\u003EFor classification training, we utilized Adam as our primary optimization method because of established literature showing quicker loss convergence compared to SGD. We trained for 350 epochs with a learning rate of 0.0005 and a Cosine Annealing learning rate scheduler. Both networks were trained with a batchsize of 1024, and the input size was 32 by 32.\u003C\u002Fp\u003E\n\u003Cp\u003EBoth networks used Categorical Cross Entropy (CCE) for their loss function. We take the model with the best validation performance and display the test results.\u003C\u002Fp\u003E\n\u003Ch2 id=\"object-detection-training-procedures\"\u003EObject Detection Training Procedures\u003C\u002Fh2\u003E\n\u003Cp\u003EFor object detection training, we kept in-line with the default training procedures. YOLOv3 loss function is a multi-task loss function composed of three different losses. Each of the losses and their descriptions are as follows:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EClassification\u003C\u002Fstrong\u003E - Applying CCE with the prediction and ground truth labels for corresponding boxes\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003ECoordinate Localization\u003C\u002Fstrong\u003E - The L2 distance between a  matched prediction box coordinates and its corresponding ground truth box&#39;s coordinates \u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EObjectness\u003C\u002Fstrong\u003E - A logistic regression using BCE to determine if a given area of an image contains an object or not\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EFor Oxford IIT Pet dataset, we trained each network for 350 epochs at a learning rate of 0.0001 with a batchsize of 16. For VOC, we trained each network for 56 epochs at a learning rate of 0.0001. Both training sets are using the same learning rate scheduler for all three loss functions. Both networks had an input size of 416 by 416.\u003C\u002Fp\u003E\n\u003Ch1 id=\"datasets\"\u003EDatasets\u003C\u002Fh1\u003E\n\u003Cp\u003EFor this work, we utilize two main object detection datasets and one classification dataset. For the object detection task, we trained YOLOv3 on Oxford IIT Pet dataset and Visual Objects Challenge (VOC) dataset.\u003C\u002Fp\u003E\n\u003Ch2 id=\"oxford-iit-pet-dataset\"\u003EOxford IIT Pet Dataset\u003C\u002Fh2\u003E\n\u003Cp\u003EThe Oxford IIT Pet dataset is composed of &quot;37 [pet categories] with roughly 200 images for each class&quot; [10]. The annotation style is visualized in Figure 3, and a given method should output either the localization of the head or segmentation of the whole pet and the classification of the specific pet breed.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fyolov3msa\u002Fdiagrams\u002Fpet_annotations.jpg\" width=\"300\"  alt=\"Oxford IIT Pet annotation examples.\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 3\u003C\u002Fb\u003E - \u003Cb\u003EIllustration of annotations provided in the Oxford IIT Pet dataset.\u003C\u002Fb\u003E\nEach image is of one pet, and for our purposes, we focused on localizing the head of the animal and classifying the animal by breed. This annotation diagram was taken directly from [10].\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch2 id=\"visual-object-challenge-dataset\"\u003EVisual Object Challenge Dataset\u003C\u002Fh2\u003E\n\u003Cp\u003EThe VOC datasets are a set of multiple datasets based on various years of the challenges. In terms of literature importance, VOC2007 and VOC2012 were the most utilized datasets among the VOC family, which is why we are using them in our work. VOC2007 is composed of 20 classes with a total of 4,952 training images and 2,501 testing images. \u003C\u002Fp\u003E\n\u003Cp\u003ESimilarly, VOC2012 contains the same 20 classes but contains a total of 5,171 training images and 5,823 validation images. From Figure 4, the annotation style is similar to the Oxford IIT Pet dataset, where the objects are labeled with a bounding box and each box is assigned one of the 20 classes. In this work, we combine VOC2007 and VOC2012 training set and test on the VOC2007 test set.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fyolov3msa\u002Fdiagrams\u002Fvoc2007_annotation_example.png\" width=\"300\"  alt=\"VOC (Visual Object Context) annotation example\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 4\u003C\u002Fb\u003E - \u003Cb\u003EIllustration of annotations provided in VOC2007 dataset.\u003C\u002Fb\u003E The dataset is composed of bounding boxes and segmentation masks. For our purposes, we are focusing on the bounding box. The image is taken from [3].\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch2 id=\"cifar-10\"\u003ECIFAR-10\u003C\u002Fh2\u003E\n\u003Cp\u003ECIFAR-10 is a popular and very simple dataset that contains 10 classes. Each class is a generic object or subject, and the dataset is composed of 60,000 32x32 images [5].\u003C\u002Fp\u003E\n\u003Ch1 id=\"results\"\u003EResults\u003C\u002Fh1\u003E\n\u003Ch2 id=\"classification-results\"\u003EClassification Results\u003C\u002Fh2\u003E\n\u003Cp\u003EAs mentioned in the Implementation section, we implement Darknet53 as a classifier and add MSA modifications seen in Figure 4. We trained on CIFAR-10 and see a notable improvement of 1.23% on Darknet53 with MSA compared to the original Darknet53 network.\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003E\u003Cstrong\u003EModel Type\u003C\u002Fstrong\u003E\u003C\u002Fth\u003E\n\u003Cth\u003E\u003Cstrong\u003ETop-1 Accuracy on CIFAR-10\u003C\u002Fstrong\u003E\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003EDarknet53\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.8803\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EDarknet53 with MSA\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Cstrong\u003E0.8926 (+0.0123)\u003C\u002Fstrong\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cbr\u003E\n\n\u003Ch2 id=\"object-detection-results\"\u003EObject Detection Results\u003C\u002Fh2\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fyolov3msa\u002Fdiagrams\u002Fall_dog_maps.png\" width=\"300\"  alt=\"Graph showing mAP scores of all the networks trained on the Oxford IIT Pet dataset.\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 5\u003C\u002Fb\u003E - \u003Cb\u003EThe mAP trend during training for Oxford IIT Pet testset and\nVOC2007 testset.\u003C\u002Fb\u003E\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fyolov3msa\u002Fdiagrams\u002Fall_voc_maps.png\" width=\"300\"  alt=\"\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 6\u003C\u002Fb\u003E - \u003Cb\u003EThe mAP trend during training for Oxford IIT Pet testset and VOC2007 testset.\u003C\u002Fb\u003E\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fyolov3msa\u002Fdiagrams\u002Fmap_delta_dog.svg\" style=\"width: 350px; display: inline\"  alt=\"\"\u002F\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fyolov3msa\u002Fdiagrams\u002Fmap_delta_voc.svg\" style=\"width: 350px; display: inline\"  alt=\"\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 7\u003C\u002Fb\u003E - \u003Cb\u003EThe mAP deltas for YOLOv3 MSAs on Oxford IIT Pet testset and VOC2007 testset.\u003C\u002Fb\u003E The number of MSA modules is not always correlated with the loss or gain of performance in this particular network.\n\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003E\u003Cstrong\u003EModel Type\u003C\u002Fstrong\u003E\u003C\u002Fth\u003E\n\u003Cth\u003E\u003Cstrong\u003EOxford IIT Pet testset mAP @ 0.5\u003C\u002Fstrong\u003E\u003C\u002Fth\u003E\n\u003Cth\u003E\u003Cstrong\u003EVOC2007 testset mAP @ 0.5\u003C\u002Fstrong\u003E\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.92\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.747\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 MSA 4\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.909 (-0.011)\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.739 (-0.008)\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 MSA 3\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.928 (+0.008)\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.726 (-0.021)\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 MSA 2\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.905 (-0.015)\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.732 (-0.015)\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 MSA 1\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.919 (-0.001)\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.739 (-0.008)\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cbr\u003E\n\n\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003E\u003Cstrong\u003EModel Type + Remove Augmentations\u003C\u002Fstrong\u003E\u003C\u002Fth\u003E\n\u003Cth\u003E\u003Cstrong\u003EVOC2007 Testset mAP @ 0.5\u003C\u002Fstrong\u003E\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 (No Augmentations)\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.4564\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 MSA 4 (No Augmentations)\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Cstrong\u003E0.4871 (+0.0307)\u003C\u002Fstrong\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 (No Mosaic)\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Cstrong\u003E0.7105\u003C\u002Fstrong\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 MSA 4 (No Mosaic)\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.6859 (-0.0246)\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 (No Mosaic + No EMA)\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Cstrong\u003E0.7060\u003C\u002Fstrong\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EYOLOv3 MSA 4 (No Mosaic + No EMA)\u003C\u002Ftd\u003E\n\u003Ctd\u003E0.6873 (-0.0187)\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cbr\u003E\n\n\u003Ch1 id=\"conclusions\"\u003EConclusions\u003C\u002Fh1\u003E\n\u003Cp\u003EFrom our experiments, we conclude the following:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EWhen applied to classifiers, MSA modules make a notable improvement in classification accuracy (reaffirming work done by [9])\u003C\u002Fli\u003E\n\u003Cli\u003EWhen applied to YOLOv3, MSA modules marginally decrease mAP performance\u003C\u002Fli\u003E\n\u003Cli\u003EIn some cases, MSA modules can improve mAP performance, but these improvements are not in alignment with the number of MSAs present in the network and are marginal\u003C\u002Fli\u003E\n\u003Cli\u003EIn terms of lost mAP, the number of MSA modules did not always correlate to how much mAP was lost.\u003C\u002Fli\u003E\n\u003Cli\u003ETraining trends show that YOLOv3 MSAs with any number of MSA heads increase localization and objectness\nloss\u003C\u002Fli\u003E\n\u003Cli\u003EThe more MSA heads a network has, the greater the objectness loss\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003E\u003Cstrong\u003E1) Bad Localization:\u003C\u002Fstrong\u003E From the experimental results, we can see that MSA modules had an outsized negative affect on the localization and objectness portions of the loss function. But why? From [9], MSAs improve network metric performance by ”smoothing” the loss surface of a convex function. If the function is non-convex, MSA does not perform as well as a Conv layer. Unfortunately, L2 distance and logistic regression even with a convex loss function) are not guaranteed to be convex [8]. From Figure 13, it seems that both loss surfaces for the tested datasets (particularly for VOC) were not convex.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E2) Training Procedures Matter:\u003C\u002Fstrong\u003E As stated in the list above, we did not see any notable improvements with MSA, and so we decided to investigate further into why. It is well-established that augmentations can drastically help with mAP performance, so we decided to investigate if a set of augmentations were holding back the MSA network rather than helping. Firstly, we removed ALL augmentations and trained YOLOv3 and YOLOv3 MSA 4. In this case, YOLOv3 MSA 4 outperformed YOLOv3 by 3% mAP.\u003C\u002Fp\u003E\n\u003Cp\u003EThis is the biggest change in mAP recorded in this work thus far. Although the overall mAP for both networks is drastically lower than with any augmentations, it gave credence to our hypothesis that MSA-based networks were being negatively affected by a non-optimal set of augmentations. We then tried training the same networks without the Mosaic augmentation, which has helped CNN performance but had no conclusive effect on Transformer-based networks.\u003C\u002Fp\u003E\n\u003Cp\u003EBased on [7], EMA can negatively affect Transformer learning, so we also ran a training without Mosaic and EMA. All the results are located in Table III, but removal of these two augmentations still showed the MSA network lagging behind the CNN network. In this case, the mAP delta is greater than any of the deltas in Table II. Although, we affirm that EMA did have a negative effect on the MSA.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E3) MSA Head Dimensions:\u003C\u002Fstrong\u003E From our Technical Approach section, we stated that each MSA module has a dimension length for a given head. We had set all the heads to a dimension of 64 to fit the models on the present hardware. From our literature review, networks such as ViT YOLO have their head dimensionality at 1024. A possible way of increasing performance would be to increase the head dimensions. It is clear that conventional CNN networks can benefit from the addition of MSAs. But from our experiments and conclusions, it comes with a bold asterisk. Regardless, we see the future of object detection incorporating a multitude of different layers to increase true scene interpretation.\u003C\u002Fp\u003E\n\u003Ch1 id=\"references\"\u003EReferences\u003C\u002Fh1\u003E\n\u003Cp\u003E\u003Cstrong\u003E[1]\u003C\u002Fstrong\u003E Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection, 2020.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[2]\u003C\u002Fstrong\u003E Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[3]\u003C\u002Fstrong\u003E M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,and A. Zisserman. The PASCAL Visual Object ClassesChallenge 2007 (VOC2007) Results. \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"http:\u002F\u002Fwww.pascal-network.org\u002Fchallenges\u002FVOC\u002Fvoc2007\u002Fworkshop\u002Findex.html\"\u003Ehttp:\u002F\u002Fwww.pascal-network.org\u002Fchallenges\u002FVOC\u002Fvoc2007\u002Fworkshop\u002Findex.html\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[4]\u003C\u002Fstrong\u003E Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth, 2016.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[5]\u003C\u002Fstrong\u003E Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[6]\u003C\u002Fstrong\u003E Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to vision transformers. arXiv preprint arXiv:2104.05707, 2021.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[7]\u003C\u002Fstrong\u003E Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows, 2021.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[8]\u003C\u002Fstrong\u003E Gabriele De Luca. Why does the cost function of logistic regression have a logarithmic expression?, Jul 2020.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[9]\u003C\u002Fstrong\u003E Namuk Park and Songkuk Kim. How do vision transformers work?\narXiv preprint arXiv:2202.06709, 2022.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[10]\u003C\u002Fstrong\u003E Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[11]\u003C\u002Fstrong\u003E Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[12]\u003C\u002Fstrong\u003E Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ́e J ́egou. Training data-efficient image transformers &amp; distillation through attention. In International Conference on Machine Learning, pages 10347–10357. PMLR, 2021.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[13]\u003C\u002Fstrong\u003E Zixiao Zhang, Xiaoqiang Lu, Guojin Cao, Yuting Yang, Licheng Jiao,and Fang Liu. Vit-yolo: Transformer-based yolo for object detection. InProceedings of the IEEE\u002FCVF International Conference on ComputerVision, pages 2799–2808, 2021.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E[14]\u003C\u002Fstrong\u003E Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.Random erasing data augmentation, 2017.\u003C\u002Fp\u003E\n",created:"Mon, 25 Apr 2022 19:45:28 GMT",excerpt:"\n\nFor my graduate Deep Learning (DL) class, we had a final project assignment that tasked us to conduct novel experiments on a DL area. I decided to implement a MSA module to a common, generic, fast object detector called YOLOv3. We see improvements in classification but significant decrease in localization performance for most network configurations.\n",author:"Vijay Rajagopal",readingTime:"12 min read",mediaFilePath:"post-res\u002Fyolov3msa\u002Fyolov3msa_thumb.png",tags:["machine learning "," wip"],art_credit:"DALL-E"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');var s=document.createElement("script");try{new Function("if(0)import('')")();s.src="/client/client.4da8ec3a.js";s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main","/client/client.4da8ec3a.js")}document.head.appendChild(s)</script> 