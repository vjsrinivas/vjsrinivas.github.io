<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=logo.png rel=icon type=image/png> <script> MathJax = {
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				processEscapes: true
			},
			svg: {
				fontCache: 'global'
			}
		}; </script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async id=MathJax-script></script> <script src=https://kit.fontawesome.com/d1e7542e7f.js crossorigin=anonymous></script> <link href=client/main.677622768.css rel=stylesheet><link href=client/[slug].d2f2c26c.css rel=stylesheet><link href=client/client.4da8ec3a.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>DeepSparse vs OpenVINO Benchmarking</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class=svelte-wyfi55><ul class=svelte-wyfi55><li class=svelte-wyfi55><a href=blog rel=prefetch aria-current=page class="svelte-wyfi55 btn-gen">Blog</a></li> <li class=svelte-wyfi55><a href=. class="svelte-wyfi55 btn-gen">Home</a></li> </ul></nav> <main class=svelte-1xjjggr> <div class="content svelte-1d9fknd"><h1>DeepSparse vs OpenVINO Benchmarking</h1> <hr> <p>To support low-latency applications, there has been extensive work in the Deep Learning space to accelerate neural networks.</p> <p>In general, these works break down into 3 camps:</p> <ul> <li><strong>Hardware-Assisted Acceleration:</strong> Hardware that targets the acceleration of common neural network operations; think TensorCores, TPUs, and NPUs.</li> <li><strong>Architecture-based Optimizations:</strong> Neural network architectures that focus on reduced operations or extracting more features from less parameters. Think architectures such as MobileNet and EfficientNet.</li> <li><strong>Runtime Optimizations:</strong> Any inference-time or inference-level optimization, such as layer fusion, quantization (PTQ or QAT), and pruning</li> </ul> <p>This post will benchmark an aspect of that last camp. Previous research has shown that sparse weight matrices introduced by unstructured pruning can be used to accelerate network inferencing by <a href=https://neuralmagic.com/blog/how-neural-magics-deep-sparse-technology-works/ rel=nofollow target=_blank>packing more data into CPU cache</a> and taking advantage of GEMM instructions (ex: AVX2, AVX512-VNNI, and etc.). The main comparison is <strong>not</strong> between different networks or pruning techniques. Rather, we are comparing OpenVINO and DeepSparse - two popular CPU-based inference engines with support for pruned networks.</p> <p><strong>TLDR:</strong> DeepSparse outperformed OpenVINO in throughput (img/s) by an average of 12%. It also excelled in all the lower-end compute scenarios tested and with most network architectures. However, on the m7i.xlarge instance (Intel Platinum 8488C), OpenVINO outperformed DeepSparse by an average of 17%. Does this mean I will use DeepSparse? Probably not. </p> <figure> <p><img src=post-res/pruning_experiment/overview_line_graph.png alt=""></p> <figcaption></figcaption> </figure> <h2 id=methodology>Methodology</h2> <p>I collected common “real-time” convolution-based classification networks and a transformer-based network called MobileViT in their base and pruned versions. Of the pruned models, MobileViT-xs, EfficientNet-B0, and EfficientNet-B2 were pruned by me, and the rest were collected from NeuralMagic's model zoo. All models were pruned using SparseML with the default Gradient Magnitude Pruning (GMP) method applied to convolutional and linear layers. Since hyperparameters for pruning are architecture-specific, there is no strict paradigm followed for the models I pruned. The top 10 layers of those models were analyzed for sensitivity by randomly masking the weights and calculating the validation accuracy on Imagenet-1k.</p> <p>Sparsity values for each layer were subjectively chosen based on the sensitivity analysis. Following NeuralMagic's recommendation, the pruning process goes through a stabilization, pruning, and finetuning period with an initial learning rate of 0.01. I applied a gamma-based learning rate scheduler and reduced the LR during different parts of the pruning and finetuning stages. The pruning process I implemented requires additional work, as the pruned networks show notable decreases in accuracy on Imagenet-1k, but they serve as reasonable models for benchmarking.</p> <figure> <img src=post-res/pruning_experiment/efficientnet_b0_sensitivity_graph_labeled.png style=width:75%!important> <figcaption></figcaption> </figure> <figure> <img src=post-res/pruning_experiment/efficientnet_b2_sensitivity_graph_labeled.png style=width:75%!important> <figcaption></figcaption> </figure> <figure> <img src=post-res/pruning_experiment/mobilenet_xs_sensitivity_graph_labeled.png style=width:75%!important> <figcaption></figcaption> </figure> <figure> <table> <thead> <tr> <th>Network Name</th> <th>Sparsity</th> <th>ImageNet-1k Validation Accuracy</th> </tr> </thead> <tr> <td>MobileViT-xs</td> <td>54%</td> <td>65.09% <b style=color:red>(-9.52%)</b></td> </tr> <tr> <td>EfficientNet-B0</td> <td>61%</td> <td>71.75% <b style=color:red>(-5.15%)</b></td> </tr> <tr> <td>EfficientNet-B2</td> <td>62%</td> <td>77.37% <b style=color:red>(-1.83%)</b></td> </tr> </table> <figcaption>Table 1. Table shows all models that were pruned by me, their sparsity, the ImageNet-1k validation accuracy, and the baseline delta.</figcaption> </figure> <p>For each model, we export an ONNX version and run dummy inputs at the models' respective input size on ONNXRuntime, OpenVINO, and DeepStream (on the provided benchmarking tool as well as a separate Python that uses the API) inference engines. While testing, I saw that both DeepSparse methods yielded essentially the same results, but both were kept for the sake of completeness. Timing is measured with a simple <code>time.time()</code> before and after each inference call. Each inference engine was ran for 60 seconds (excluding the DeepSparse benchmark tool) and repeated 5 times to account for any noise. These runs were conducted on the following AWS EC2 instances:</p> <ul> <li><a href=https://instances.vantage.sh/aws/ec2/t3.xlarge rel=nofollow target=_blank>t3.xlarge</a></li> <li><a href=https://instances.vantage.sh/aws/ec2/c4.xlarge rel=nofollow target=_blank>c4.xlarge</a></li> <li><a href=https://instances.vantage.sh/aws/ec2/m6a.xlarge rel=nofollow target=_blank>m6a.xlarge</a></li> <li><a href=https://instances.vantage.sh/aws/ec2/m5.xlarge rel=nofollow target=_blank>m5.xlarge</a></li> <li><a href=https://instances.vantage.sh/aws/ec2/m5n.xlarge rel=nofollow target=_blank>m5n.xlarge</a></li> <li><a href=https://instances.vantage.sh/aws/ec2/m7i.xlarge rel=nofollow target=_blank>m7i.xlarge</a></li> </ul> <h2 id=results>Results</h2> <p>Across all CPUs except the m7i.large, DeepSparse outperforms OpenVINO by an average of 12% in terms of throughput. On m7i.xlarge, OpenVINO sees a 28% jump in throughput compared to its 2nd best performing AWS instance (m6a.xlarge). On a per model basis, EfficientNet-b0, EfficientNet-b2, and MobileViT did not see significant speed-up when comparing the baseline model and pruned models. These models were ones that I had pruned, so it could be down to the sparisification recipe as well as the sparsity target not hitting an optimization threshold for any of the inference engine. In terms of performance across AWS instances, the increase in throughput was only seen on the higher-cost instances, which typically have stronger CPUs.</p> <p>As an aside, the compressive affects of pruning can be seen in Table 2. I use gzip with the compression ratio set to 9 (highest available compression ratio) on all base and pruned models. Excluding ResNet50, which has a higher reduction ratio due to the INT8 datatype affecting filesize, the average reduction ratio is 2.99.</p> <figure> <table> <thead> <tr> <th>Network Name</th> <th>Original Size (MB)</th> <th>Optimized Size (MB)</th> <th>Reduction Ratio</th> </tr> </thead> <tr> <td>MobileViT-xs</td> <td>8.62</td> <td>4.24</td> <td>2.03x</td> </tr> <tr> <td>EfficientNet-B0</td> <td>19.59</td> <td>8.60</td> <td>2.28x</td> </tr> <tr> <td>EfficientNet-B2</td> <td>33.76</td> <td>17.82</td> <td>1.89x</td> </tr> <tr> <td>ResNet50 <b style=color:red>*</b></td> <td>95.16</td> <td>10.48</td> <td>9.08x</td> </tr> <tr> <td>MobileNetv1</td> <td>16.26</td> <td>4.70</td> <td>3.45x</td> </tr> <tr> <td>VGG19</td> <td>533.10</td> <td>110.34</td> <td>4.83x</td> </tr> <tr> <td>InceptionV3</td> <td>88.63</td> <td>25.37</td> <td>3.49x</td> </tr> </table> <figcaption>Table 2. Table shows the different sparsity levels and their affect on file size after gzip compression with the compression ratio set to 9 (the highest).</figcaption> <figcaption><b style=color:red>*</b> - INT8 quantization was also applied. </figcaption> </figure> <figure> <p><img src=post-res/pruning_experiment/overview_graph_intel_xeon_e5-2666_v3_cpu.png alt=""></p> <p><img src=post-res/pruning_experiment/overview_graph_amd_epyc_7r13_cpu.png alt=""></p> <p><img src=post-res/pruning_experiment/overview_graph_intel_xeon_platinum_8175m_cpu.png alt=""></p> <p><img src=post-res/pruning_experiment/overview_graph_intel_xeon_platinum_8175m_cpu_2.png alt=""></p> <p><img src=post-res/pruning_experiment/overview_graph_intel_xeon_platinum_8259cl_cpu.png alt=""></p> <p><img src=post-res/pruning_experiment/overview_graph_intel_xeon_platinum_8488c_cpu.png alt=""></p> <figcaption></figcaption> </figure> <h1 id=discussion>Discussion</h1> <p><strong>1. Which inference engine was faster?</strong></p> <p>From the results, it's <strong>typically</strong> DeepSparse. While there is a lot of naunce in terms of hardware and model architecutre, all but one AWS instance type consistently showed higher throughput for models on DeepSparse.</p> <p><strong>2. What is the reason for the advantage?</strong></p> <p>Since both inference engines are closed source, I can't tell you. But I would not be surprised if certain kernels in DeepSparse's arsenal were more optimized for these kinds of networks.</p> <p><strong>3. Why didn't the EfficientNet and MobileViT networks see any throughput improvements after pruning?</strong></p> <p>In the "Results" section, I alluded to the sparsity level not being high enough for any engine to take advantage of. I went back and did a pruning session on all three networks at 85% and 99% sparsity. This pruning attempt only focused on sparsity and not the model performance:</p> <figure> <img src=post-res/pruning_experiment/fake_prune_throughput.svg style=width:75%!important> <figcaption>N.B. CPU used was an AMD Ryzen 7 2700X</figcaption> </figure> <p>Sparsity had little effect on the throughput speed for these networks. EfficientNet-b0 was the only network that saw an increase in throughput, but it was on ONNXRuntime. I don't fully understand why.</p> <p><strong>4. How come OpenVINO was faster on m7i.xlarge?</strong></p> <p>Similar to the points made in #2, I can't provide a definitive answer, but I believe it comes down to the CPU. After retrieving some public specifications for each CPU, I saw that the Intel Xeon Platinum 8488C is the newest CPU with VNNI support and a large L2 & L3 cache. It also had the highest maximum clock frequency amongst all the listed CPUs. These factors are likely the main reason why OpenVINO was faster.</p> <table> <thead> <tr> <th>Instance Type</th> <th>CPU Type</th> <th>CPU Family</th> <th>AVX2 Support</th> <th>AVX512 Support</th> <th>VNNI Support</th> <th>L1 Cache (Data)</th> <th>L1 Cache (Instruct)</th> <th>L2 Cache</th> <th>L3 Cache</th> <th>Maximum Frequency</th> <th>Cores</th> <th>Sources</th> </tr> </thead> <tr> <td>m7i.xlarge</td> <td>Intel Xeon Platinum 8488C</td> <td>Sapphire Rapids-SP</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>48KB</td> <td>32KB</td> <td>2MB</td> <td>105MB</td> <td>3.8 GHz</td> <td>56</td> <td><a href=https://www.intel.com/content/www/us/en/products/sku/231730/intel-xeon-platinum-8480c-processor-105m-cache-2-00-GHz/specifications.html rel=nofollow target=_blank>Link</a></td> </tr> <tr> <td>t3.xlarge</td> <td>Intel Xeon Platinum 8175M</td> <td>Skylake</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>24KB</td> <td>24KB</td> <td>1MB</td> <td>33MB</td> <td>2.5 GHz</td> <td>24</td> <td><a href=https://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208175M.html rel=nofollow target=_blank>Link</a></td> </tr> <tr> <td>m5.xlarge</td> <td>Intel Xeon Platinum 8175M</td> <td>Skylake</td> <td>Yes</td> <td>Yes</td> <td>No</td> <td>24KB</td> <td>24KB</td> <td>1MB</td> <td>33MB</td> <td>2.5 GHz</td> <td>24</td> <td><a href=https://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208175M.html rel=nofollow target=_blank>Link</a></td> </tr> <tr> <td>m5n.xlarge</td> <td>Intel Xeon Platinum 8259CL</td> <td>Cascade Lake</td> <td>Yes</td> <td>Yes</td> <td>Yes</td> <td>24KB</td> <td>24KB</td> <td>2MB</td> <td>33MB</td> <td>2.5 GHz</td> <td>32</td> <td><a href=https://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208259CL.html rel=nofollow target=_blank>Link</a></td> </tr> <tr> <td>m6a.xlarge</td> <td>AMD EPYC 7R13</td> <td>Zen3 (EPYC 7003 series)</td> <td>Yes</td> <td>No</td> <td>No</td> <td>32KB</td> <td>32KB</td> <td>512KB(?)</td> <td>33MB(?)</td> <td>3.5 GHz</td> <td>48</td> <td><a href=https://ldbcouncil.org/benchmarks/snb/LDBC_SNB_BI_20230406_SF10000_tigergraph.pdf rel=nofollow target=_blank>Link</a></td> </tr> <tr> <td>c4.xlarge</td> <td>Intel Xeon E5-2666V3</td> <td>Haswell-EP (?)</td> <td>Yes</td> <td>No</td> <td>No</td> <td>32KB</td> <td>32KB</td> <td>1MB</td> <td>45MB</td> <td>3.5 GHz</td> <td>10</td> <td><a href=https://www.techpowerup.com/cpu-specs/xeon-e5-2666-v3.c2876 rel=nofollow target=_blank>Link</a></td> </tr> </table> <p><strong>5. Does this mean I'll actually use DeepSparse?</strong></p> <p>Since entering the industry, my prespective has changed from using tools that will give me the best performance (speed, task performance, etc.) to tools that give me flexiblity. Currently, DeepSparse lacks the level of integration that OpenVINO has - particularly, an integration with ONNXRuntime and deployment stacks like Triton. DeepSparse does have its own server program, but that - again - limits your flexbility. Why have a deployment stack only geared towards CPU? Finally, based on OpenVINO's performance on the 8488C, we might see that 12% advantage dwindle with newer Intel CPUs. </p> <p><strong>6. Anything else, Vijay?</strong></p> <p>I have a nagging feeling that there's some bias being introduced when utilizing SparseML for network pruning, since it is a tool developed by NeuralMagic. I guess a proper follow up would be to reimplement GMP with pure PyTorch and re-run all the experiments, but PyTorch's pruning API is much more complicated compared to SparseML.</p> </div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{title:"DeepSparse vs OpenVINO Benchmarking",description:"Benchmarking of DeepSparse against OpenVINO on sparse classification networks",slug:"pruning_experiment",html:"\u003Cp\u003ETo support low-latency applications, there has been extensive work in the Deep Learning space to accelerate neural networks.\u003C\u002Fp\u003E\n\u003Cp\u003EIn general, these works break down into 3 camps:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Cstrong\u003EHardware-Assisted Acceleration:\u003C\u002Fstrong\u003E Hardware that targets the acceleration of common neural network operations; think TensorCores, TPUs, and NPUs.\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003EArchitecture-based Optimizations:\u003C\u002Fstrong\u003E Neural network architectures that focus on reduced operations or extracting more features from less parameters. Think architectures such as MobileNet and EfficientNet.\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Cstrong\u003ERuntime Optimizations:\u003C\u002Fstrong\u003E Any inference-time or inference-level optimization, such as layer fusion, quantization (PTQ or QAT), and pruning\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp\u003EThis post will benchmark an aspect of that last camp. Previous research has shown that sparse weight matrices introduced by unstructured pruning can be used to accelerate network inferencing by \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fneuralmagic.com\u002Fblog\u002Fhow-neural-magics-deep-sparse-technology-works\u002F\"\u003Epacking more data into CPU cache\u003C\u002Fa\u003E and taking advantage of GEMM instructions (ex: AVX2, AVX512-VNNI, and etc.). The main comparison is \u003Cstrong\u003Enot\u003C\u002Fstrong\u003E between different networks or pruning techniques. Rather, we are comparing OpenVINO and DeepSparse - two popular CPU-based inference engines with support for pruned networks.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ETLDR:\u003C\u002Fstrong\u003E DeepSparse outperformed OpenVINO in throughput (img\u002Fs) by an average of 12%. It also excelled in all the lower-end compute scenarios tested and with most network architectures. However, on the m7i.xlarge instance (Intel Platinum 8488C), OpenVINO outperformed DeepSparse by an average of 17%. Does this mean I will use DeepSparse? Probably not. \u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\n\u003Cp\u003E\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Foverview_line_graph.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\n\u003Ch2 id=\"methodology\"\u003EMethodology\u003C\u002Fh2\u003E\n\u003Cp\u003EI collected common “real-time” convolution-based classification networks and a transformer-based network called MobileViT in their base and pruned versions. Of the pruned models, MobileViT-xs, EfficientNet-B0, and EfficientNet-B2 were pruned by me, and the rest were collected from NeuralMagic&#39;s model zoo. All models were pruned using SparseML with the default Gradient Magnitude Pruning (GMP) method applied to convolutional and linear layers. Since hyperparameters for pruning are architecture-specific, there is no strict paradigm followed for the models I pruned. The top 10 layers of those models were analyzed for sensitivity by randomly masking the weights and calculating the validation accuracy on Imagenet-1k.\u003C\u002Fp\u003E\n\u003Cp\u003ESparsity values for each layer were subjectively chosen based on the sensitivity analysis. Following NeuralMagic&#39;s recommendation, the pruning process goes through a stabilization, pruning, and finetuning period with an initial learning rate of 0.01. I applied a gamma-based learning rate scheduler and reduced the LR during different parts of the pruning and finetuning stages. The pruning process I implemented requires additional work, as the pruned networks show notable decreases in accuracy on Imagenet-1k, but they serve as reasonable models for benchmarking.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\n\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Fefficientnet_b0_sensitivity_graph_labeled.png\" style=\"width: 75% !important\"\u002F\u003E\n\n\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\n\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Fefficientnet_b2_sensitivity_graph_labeled.png\" style=\"width: 75% !important\"\u002F\u003E\n\n\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\n\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Fmobilenet_xs_sensitivity_graph_labeled.png\" style=\"width: 75% !important\"\u002F\u003E\n\n\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003ENetwork Name\u003C\u002Fth\u003E\n\u003Cth\u003ESparsity\u003C\u002Fth\u003E\n\u003Cth\u003EImageNet-1k Validation Accuracy\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003EMobileViT-xs\u003C\u002Ftd\u003E\n\u003Ctd\u003E54%\u003C\u002Ftd\u003E\n\u003Ctd\u003E65.09% \u003Cb style=\"color: red;\"\u003E(-9.52%)\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EEfficientNet-B0\u003C\u002Ftd\u003E\n\u003Ctd\u003E61%\u003C\u002Ftd\u003E\n\u003Ctd\u003E71.75% \u003Cb style=\"color: red;\"\u003E(-5.15%)\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EEfficientNet-B2\u003C\u002Ftd\u003E\n\u003Ctd\u003E62%\u003C\u002Ftd\u003E\n\u003Ctd\u003E77.37% \u003Cb style=\"color: red;\"\u003E(-1.83%)\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cfigcaption\u003ETable 1. Table shows all models that were pruned by me, their sparsity, the ImageNet-1k validation accuracy, and the baseline delta.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cp\u003EFor each model, we export an ONNX version and run dummy inputs at the models&#39; respective input size on ONNXRuntime, OpenVINO, and DeepStream (on the provided benchmarking tool as well as a separate Python that uses the API) inference engines. While testing, I saw that both DeepSparse methods yielded essentially the same results, but both were kept for the sake of completeness. Timing is measured with a simple \u003Ccode\u003Etime.time()\u003C\u002Fcode\u003E before and after each inference call. Each inference engine was ran for 60 seconds (excluding the DeepSparse benchmark tool) and repeated 5 times to account for any noise. These runs were conducted on the following AWS EC2 instances:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Finstances.vantage.sh\u002Faws\u002Fec2\u002Ft3.xlarge\"\u003Et3.xlarge\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Finstances.vantage.sh\u002Faws\u002Fec2\u002Fc4.xlarge\"\u003Ec4.xlarge\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Finstances.vantage.sh\u002Faws\u002Fec2\u002Fm6a.xlarge\"\u003Em6a.xlarge\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Finstances.vantage.sh\u002Faws\u002Fec2\u002Fm5.xlarge\"\u003Em5.xlarge\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Finstances.vantage.sh\u002Faws\u002Fec2\u002Fm5n.xlarge\"\u003Em5n.xlarge\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Finstances.vantage.sh\u002Faws\u002Fec2\u002Fm7i.xlarge\"\u003Em7i.xlarge\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"results\"\u003EResults\u003C\u002Fh2\u003E\n\u003Cp\u003EAcross all CPUs except the m7i.large, DeepSparse outperforms OpenVINO by an average of 12% in terms of throughput. On m7i.xlarge, OpenVINO sees a 28% jump in throughput compared to its 2nd best performing AWS instance (m6a.xlarge). On a per model basis, EfficientNet-b0, EfficientNet-b2, and MobileViT did not see significant speed-up when comparing the baseline model and pruned models. These models were ones that I had pruned, so it could be down to the sparisification recipe as well as the sparsity target not hitting an optimization threshold for any of the inference engine. In terms of performance across AWS instances, the increase in throughput was only seen on the higher-cost instances, which typically have stronger CPUs.\u003C\u002Fp\u003E\n\u003Cp\u003EAs an aside, the compressive affects of pruning can be seen in Table 2. I use gzip with the compression ratio set to 9 (highest available compression ratio) on all base and pruned models. Excluding ResNet50, which has a higher reduction ratio due to the INT8 datatype affecting filesize, the average reduction ratio is 2.99.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003ENetwork Name\u003C\u002Fth\u003E\n\u003Cth\u003EOriginal Size (MB)\u003C\u002Fth\u003E\n\u003Cth\u003EOptimized Size (MB)\u003C\u002Fth\u003E\n\u003Cth\u003EReduction Ratio\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003EMobileViT-xs\u003C\u002Ftd\u003E\n\u003Ctd\u003E8.62\u003C\u002Ftd\u003E\n\u003Ctd\u003E4.24\u003C\u002Ftd\u003E\n\u003Ctd\u003E2.03x\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EEfficientNet-B0\u003C\u002Ftd\u003E\n\u003Ctd\u003E19.59\u003C\u002Ftd\u003E\n\u003Ctd\u003E8.60\u003C\u002Ftd\u003E\n\u003Ctd\u003E2.28x\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EEfficientNet-B2\u003C\u002Ftd\u003E\n\u003Ctd\u003E33.76\u003C\u002Ftd\u003E\n\u003Ctd\u003E17.82\u003C\u002Ftd\u003E\n\u003Ctd\u003E1.89x\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EResNet50 \u003Cb style=\"color: red;\"\u003E*\u003C\u002Fb\u003E\u003C\u002Ftd\u003E\n\u003Ctd\u003E95.16\u003C\u002Ftd\u003E\n\u003Ctd\u003E10.48\u003C\u002Ftd\u003E\n\u003Ctd\u003E9.08x\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EMobileNetv1\u003C\u002Ftd\u003E\n\u003Ctd\u003E16.26\u003C\u002Ftd\u003E\n\u003Ctd\u003E4.70\u003C\u002Ftd\u003E\n\u003Ctd\u003E3.45x\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EVGG19\u003C\u002Ftd\u003E\n\u003Ctd\u003E533.10\u003C\u002Ftd\u003E\n\u003Ctd\u003E110.34\u003C\u002Ftd\u003E\n\u003Ctd\u003E4.83x\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EInceptionV3\u003C\u002Ftd\u003E\n\u003Ctd\u003E88.63\u003C\u002Ftd\u003E\n\u003Ctd\u003E25.37\u003C\u002Ftd\u003E\n\u003Ctd\u003E3.49x\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cfigcaption\u003ETable 2. Table shows the different sparsity levels and their affect on file size after gzip compression with the compression ratio set to 9 (the highest).\u003C\u002Ffigcaption\u003E\n\u003Cfigcaption\u003E\u003Cb style=\"color: red;\"\u003E*\u003C\u002Fb\u003E - INT8 quantization was also applied. \u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\n\u003Cp\u003E\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Foverview_graph_intel_xeon_e5-2666_v3_cpu.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Foverview_graph_amd_epyc_7r13_cpu.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Foverview_graph_intel_xeon_platinum_8175m_cpu.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Foverview_graph_intel_xeon_platinum_8175m_cpu_2.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Foverview_graph_intel_xeon_platinum_8259cl_cpu.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Foverview_graph_intel_xeon_platinum_8488c_cpu.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Cfigcaption\u003E\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch1 id=\"discussion\"\u003EDiscussion\u003C\u002Fh1\u003E\n\u003Cp\u003E\u003Cstrong\u003E1. Which inference engine was faster?\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EFrom the results, it&#39;s \u003Cstrong\u003Etypically\u003C\u002Fstrong\u003E DeepSparse. While there is a lot of naunce in terms of hardware and model architecutre, all but one AWS instance type consistently showed higher throughput for models on DeepSparse.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E2. What is the reason for the advantage?\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ESince both inference engines are closed source, I can&#39;t tell you. But I would not be surprised if certain kernels in DeepSparse&#39;s arsenal were more optimized for these kinds of networks.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E3. Why didn&#39;t the EfficientNet and MobileViT networks see any throughput improvements after pruning?\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EIn the &quot;Results&quot; section, I alluded to the sparsity level not being high enough for any engine to take advantage of. I went back and did a pruning session on all three networks at 85% and 99% sparsity. This pruning attempt only focused on sparsity and not the model performance:\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\n\u003Cimg src=\"post-res\u002Fpruning_experiment\u002Ffake_prune_throughput.svg\" style=\"width:75% !important\"\u002F\u003E\n\n\u003Cfigcaption\u003EN.B. CPU used was an AMD Ryzen 7 2700X\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cp\u003ESparsity had little effect on the throughput speed for these networks. EfficientNet-b0 was the only network that saw an increase in throughput, but it was on ONNXRuntime. I don&#39;t fully understand why.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E4. How come OpenVINO was faster on m7i.xlarge?\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ESimilar to the points made in #2, I can&#39;t provide a definitive answer, but I believe it comes down to the CPU. After retrieving some public specifications for each CPU, I saw that the Intel Xeon Platinum 8488C is the newest CPU with VNNI support and a large L2 &amp; L3 cache. It also had the highest maximum clock frequency amongst all the listed CPUs. These factors are likely the main reason why OpenVINO was faster.\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EInstance Type\u003C\u002Fth\u003E\n\u003Cth\u003ECPU Type\u003C\u002Fth\u003E\n\u003Cth\u003ECPU Family\u003C\u002Fth\u003E\n\u003Cth\u003EAVX2 Support\u003C\u002Fth\u003E\n\u003Cth\u003EAVX512 Support\u003C\u002Fth\u003E\n\u003Cth\u003EVNNI Support\u003C\u002Fth\u003E\n\u003Cth\u003EL1 Cache (Data)\u003C\u002Fth\u003E\n\u003Cth\u003EL1 Cache (Instruct)\u003C\u002Fth\u003E\n\u003Cth\u003EL2 Cache\u003C\u002Fth\u003E\n\u003Cth\u003EL3 Cache\u003C\u002Fth\u003E\n\u003Cth\u003EMaximum Frequency\u003C\u002Fth\u003E\n\u003Cth\u003ECores\u003C\u002Fth\u003E\n\u003Cth\u003ESources\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003Em7i.xlarge\u003C\u002Ftd\u003E\n\u003Ctd\u003EIntel Xeon Platinum 8488C\u003C\u002Ftd\u003E\n\u003Ctd\u003ESapphire Rapids-SP\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003E48KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E32KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E2MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E105MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E3.8 GHz\u003C\u002Ftd\u003E\n\u003Ctd\u003E56\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fwww.intel.com\u002Fcontent\u002Fwww\u002Fus\u002Fen\u002Fproducts\u002Fsku\u002F231730\u002Fintel-xeon-platinum-8480c-processor-105m-cache-2-00-GHz\u002Fspecifications.html\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Et3.xlarge\u003C\u002Ftd\u003E\n\u003Ctd\u003EIntel Xeon Platinum 8175M\u003C\u002Ftd\u003E\n\u003Ctd\u003ESkylake\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003ENo\u003C\u002Ftd\u003E\n\u003Ctd\u003E24KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E24KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E1MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E33MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E2.5 GHz\u003C\u002Ftd\u003E\n\u003Ctd\u003E24\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fwww.cpu-world.com\u002FCPUs\u002FXeon\u002FIntel-Xeon%208175M.html\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Em5.xlarge\u003C\u002Ftd\u003E\n\u003Ctd\u003EIntel Xeon Platinum 8175M\u003C\u002Ftd\u003E\n\u003Ctd\u003ESkylake\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003ENo\u003C\u002Ftd\u003E\n\u003Ctd\u003E24KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E24KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E1MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E33MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E2.5 GHz\u003C\u002Ftd\u003E\n\u003Ctd\u003E24\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fwww.cpu-world.com\u002FCPUs\u002FXeon\u002FIntel-Xeon%208175M.html\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Em5n.xlarge\u003C\u002Ftd\u003E\n\u003Ctd\u003EIntel Xeon Platinum 8259CL\u003C\u002Ftd\u003E\n\u003Ctd\u003ECascade Lake\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003E24KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E24KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E2MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E33MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E2.5 GHz\u003C\u002Ftd\u003E\n\u003Ctd\u003E32\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fwww.cpu-world.com\u002FCPUs\u002FXeon\u002FIntel-Xeon%208259CL.html\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Em6a.xlarge\u003C\u002Ftd\u003E\n\u003Ctd\u003EAMD EPYC 7R13\u003C\u002Ftd\u003E\n\u003Ctd\u003EZen3 (EPYC 7003 series)\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003ENo\u003C\u002Ftd\u003E\n\u003Ctd\u003ENo\u003C\u002Ftd\u003E\n\u003Ctd\u003E32KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E32KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E512KB(?)\u003C\u002Ftd\u003E\n\u003Ctd\u003E33MB(?)\u003C\u002Ftd\u003E\n\u003Ctd\u003E3.5 GHz\u003C\u002Ftd\u003E\n\u003Ctd\u003E48\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fldbcouncil.org\u002Fbenchmarks\u002Fsnb\u002FLDBC_SNB_BI_20230406_SF10000_tigergraph.pdf\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003Ec4.xlarge\u003C\u002Ftd\u003E\n\u003Ctd\u003EIntel Xeon E5-2666V3\u003C\u002Ftd\u003E\n\u003Ctd\u003EHaswell-EP (?)\u003C\u002Ftd\u003E\n\u003Ctd\u003EYes\u003C\u002Ftd\u003E\n\u003Ctd\u003ENo\u003C\u002Ftd\u003E\n\u003Ctd\u003ENo\u003C\u002Ftd\u003E\n\u003Ctd\u003E32KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E32KB\u003C\u002Ftd\u003E\n\u003Ctd\u003E1MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E45MB\u003C\u002Ftd\u003E\n\u003Ctd\u003E3.5 GHz\u003C\u002Ftd\u003E\n\u003Ctd\u003E10\u003C\u002Ftd\u003E\n\u003Ctd\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fwww.techpowerup.com\u002Fcpu-specs\u002Fxeon-e5-2666-v3.c2876\"\u003ELink\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Cp\u003E\u003Cstrong\u003E5. Does this mean I&#39;ll actually use DeepSparse?\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ESince entering the industry, my prespective has changed from using tools that will give me the best performance (speed, task performance, etc.) to tools that give me flexiblity. Currently, DeepSparse lacks the level of integration that OpenVINO has - particularly, an integration with ONNXRuntime and deployment stacks like Triton. DeepSparse does have its own server program, but that - again - limits your flexbility. Why have a deployment stack only geared towards CPU? Finally, based on OpenVINO&#39;s performance on the 8488C, we might see that 12% advantage dwindle with newer Intel CPUs.   \u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003E6. Anything else, Vijay?\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EI have a nagging feeling that there&#39;s some bias being introduced when utilizing SparseML for network pruning, since it is a tool developed by NeuralMagic. I guess a proper follow up would be to reimplement GMP with pure PyTorch and re-run all the experiments, but PyTorch&#39;s pruning API is much more complicated compared to SparseML.\u003C\u002Fp\u003E\n",created:"Sat, 11 Jan 2025 19:45:28 GMT",excerpt:"To support low-latency applications, there has been extensive work in the Deep Learning space to accelerate neural networks.\n",author:a,readingTime:"8 min read",mediaFilePath:"post-res\u002Fpruning_experiment\u002Fpruning_experiment_thumb.mp4",tags:["machine learning"],art_credit:a}}}("Vijay Rajagopal"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');var s=document.createElement("script");try{new Function("if(0)import('')")();s.src="/client/client.4da8ec3a.js";s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main","/client/client.4da8ec3a.js")}document.head.appendChild(s)</script> 