<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=logo.png rel=icon type=image/png> <script> MathJax = {
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				processEscapes: true
			},
			svg: {
				fontCache: 'global'
			}
		}; </script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async id=MathJax-script></script> <script src=https://kit.fontawesome.com/d1e7542e7f.js crossorigin=anonymous></script> <link href=client/main.677622768.css rel=stylesheet><link href=client/[slug].d2f2c26c.css rel=stylesheet><link href=client/client.4da8ec3a.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Revisiting EmojiCam</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class=svelte-wyfi55><ul class=svelte-wyfi55><li class=svelte-wyfi55><a href=blog class="svelte-wyfi55 btn-gen" aria-current=page rel=prefetch>Blog</a></li> <li class=svelte-wyfi55><a href=. class="svelte-wyfi55 btn-gen">Home</a></li> </ul></nav> <main class=svelte-1xjjggr> <div class="content svelte-1d9fknd"><h1>Revisiting EmojiCam</h1> <hr> <p>In this blog post, I revisit my EmojiCam project and try to rectify some of the major pitfalls the original algorithm had. This rectified version of EmojiCam includes multiscale emojis and better emoji color calibration.</p> <h2 id=table-of-contents>Table of Contents</h2> <ul> <li><a href=javascript:; onclick="document.location.hash='old-processing-pipeline';">Old Processing Pipeline</a></li> <li><a href=javascript:; onclick="document.location.hash='issues';">Issues</a></li> <li><a href=javascript:; onclick="document.location.hash='rectify';">Rectifications</a></li> <li><a href=javascript:; onclick="document.location.hash='results';">Results</a></li> </ul> <h2 id=old-processing-pipeline>Old Processing Pipeline</h2> <p>The original processing pipeline for EmojiCam consisted of an algorithm that calibrated all emojis to a given color in 8-bit RGB space. This algorithm took the non-transparent areas of a given emoji and calculated the most significant colors.</p> <p>The significance of a color was computed by doing supervised kMeans clustering with a k value of 3. Then, an RGB matrix with a shape of 256 x 256 x 256 is created and indexed with each of the significant colors being indices into the matrix. Each element in the matrix is an index representing an emoji.</p> <p>This matrix is useful for determining the emoji to use in a given area. It is also very fast since the matrix is a giant look up table.</p> <p>After this step, EmojiCam creates a blank canvas that will have the emojis applied onto. During the main execution, a given image or video frame are pixelated, where width and height are divided by 24. This means that each emoji is going to be 24x24 pixels in size. Then, we look at each pixel value in the pixelated image, access the emoji index from the RGB matrix, and reproject the location of the pixel in the pixelated image back to the full-sized image. After the location is calculated, the emoji data is applied onto the canvas.</p> <p>To improve the aesthetic, I apply a weighted overlay of the pixelated image (resized to original image size) on to the canvas. I call this alpha-blending (very fancy). You can check the previous post to see how impactful alpha-blending was to the ultimate output. The overall process is also visualized in Figure 1.</p> <figure> <img , alt="" src=post-res/emojicamv2/emojicamprocesspipeline.png> <figcaption><b>Fig. 1</b> - Overall diagram of the old EmojiCam processing pipeline. The emojis are processed with kMeans to calculate predominant colors and store them into the RGB matrix. Afterwards, we can cache the matrix and use it for a O(1) lookup and apply an emoji onto a certain area of the image.</figcaption> </figure> <h2 id=issues>Issues</h2> <p>With the old processing pipeline from EmojiCam laid out, this section will discuss the pitfalls of this method.</p> <p>The first notable issue is the poor color calibration between emojis to a given color in the source image. The old method used the full 256, 8-bit RGB color space. This means there are 16,777,216 colors to map all the Twitter Emojis to. This results in a <strong>huge</strong> matrix file (~1 GB) and a very sparse matrix as well.</p> <p>Additionally, using the kMeans algorithm caused unexpected results since initial location of the mean points are randomized and group colors together in an improper way. The clustering algorithm also fails to consider the consolidation of minor color swatches in complex emojis into a cluster that is a misrepresentation of the emoji's true color distribution.</p> <p>An example of this calibration error is the video below. The emojis are assigned to other colors in a nonsensical nature. In turn, the output looks really bad. The only thing holding EmojiCam together aesthetically is the alpha-blending.</p> <video autoplay loop muted> <source src=post-res/emojicamv2/test_out.mp4 type=video/mp4> Your browser does not support the video tag. </video> <h2 id=rectifications>Rectifications</h2> <h3 id=new-calibration>New Calibration</h3> <p>So, how do we fix/improve the issues from the previous section? For the RGB matrix issue, I downsample the possible number of colors from 256 to 128.</p> <p>I approach emoji calibration from a simpler perspective. Firstly, I will not use all the available emojis since some of them are incredibly complex in terms of color distribution and canvas composition. In order to narrow the number of useful emojis, I removed KMeans and simply calculated the number of unique colors (excluding the transparent areas).</p> <p>For a given emoji, I filter out the unique values with a percentile threshold. Additionally, I also filter emojis with a transparent ratio that is less than 30%. Finally, I classify emojis as "simple" if the most frequent unique color value has a percentage over 85%. All of this filter ensured that the emojis used were simple and have enough color expression to match almost all typical color swatches.</p> <h3 id=multiscale-emojis>Multiscale Emojis</h3> <p>Another improvement I implement is the ability for emojis to be different sizes. This can enable the output to look more artistic rather than just a slightly more interesting version of standard pixelization. But how do we algorithmically implement multiscale emojis and how do we decide to use which emoji scale?</p> <p>In terms of multiscale, I just work with two scales - 24x24 and 48x48. A third 96x96 scale emoji was tried but the results did not look aesthetically pleasing. In terms of where to put the 48x48 scaled emojis, I utilize Discrete Cosine Transform (DCT). DCT generates information regarding an image's "frequency" and is typically used in image compression algorithms like JPEG and WebP. These algorithms take advantage of the fact that human eyes cannot distinguish high frequency components very well (example in the figure below). By removing high frequency components via quantization, the compression algorithms can reduce file size while preserving image quality.</p> <p>I use DCT in order to determine where the low frequency components were. Technically, I convert the RGB image into a YCrCb format, and I apply an 8x8 DCT across the Luminance (Y) channel only. Filtering out only low frequencies was done by taking only the top-left triangle of a given DCT block. Then I sum up the absolute value of each DCT block into a single scalar value in a separate DCT matrix. Then I determine the viable positions by calculating the 80th percentile of all DCT values.</p> <figure> <img , alt="" src=post-res/emojicamv2/example_forest.png> <img , alt="" src=post-res/emojicamv2/dct_high_freq_example.png> <img , alt="" src=post-res/emojicamv2/dct_low_freq_example.png> <figcaption><b>Fig. 2</b> - An example of high frequency and low frequency components generated by DCT. Each DCT block (in this case a 8x8 pixel block) contains a matrix that represent frequencies. <a href=https://www.researchgate.net/figure/DCT-blocks-Where-FL-Low-frequencies-FM-mid-frequencies-FH-High-frequencies_fig3_259763663>Here is a good diagram of how the frequencies are aligned in each block</a>.</figcaption> </figure> <figure> <img , alt="" src=post-res/emojicamv2/example_percentile_mask.png> <figcaption><b>Fig. 3</b> - Example of a position mask of DCT by computing a certain percentile. Using low frequency components means we can increase the scale of emojis only in less detailed parts of the image.</figcaption> </figure> <h2 id=results>Results</h2> <p>Did those rectifications do any good? Well, let's see the following comparison:</p> <figure> <img , , alt="" src=post-res/emojicamv2/out_forest.jpg style=margin-bottom:5px> <img , , alt="" src=post-res/emojicamv2/out_forest.jpg.png style=margin-bottom:5px> <img , alt="" src=post-res/emojicamv2/forest.jpg> <figcaption><b>Fig. 4</b> - An example of a woodland scene is used. The first image is the original method's output. The image below it is the new method's output. It is clear that the color and emoji selection is much better on the new method. Additionally, the multiscale method works fairly well by choosing relatively non-intrusive areas to upscale to 48x48.</figcaption> </figure> <figure> <video autoplay loop muted> <source src=post-res/emojicamv2/test_out-converted.mp4 type=video/mp4> Your browser does not support the video tag. </video> <video autoplay loop muted> <source src=post-res/emojicamv2/out_test.mp4 type=video/mp4> Your browser does not support the video tag. </video> <video autoplay loop muted> <source src=post-res/emojicamv2/test.mp4 type=video/mp4> Your browser does not support the video tag. </video> <figcaption><b>Fig. 5</b> - Another example applied to a video. We see very similar results to the image-based example. The newer method produces better results and utilizes the larger scaled emojis effectively.</figcaption> </figure> <p>So what is next? Unfortunately, the addition of DCT and multiscale emojis make this method slow. It runs less than 1 FPS on a mid-range CPU (AMD Ryzen 2700X). I want to optimize the DCT method I'm using and find a better way of rendering the final output. Also, I need to clean up the code. Maybe by 2024.</p> </div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Revisiting EmojiCam",description:"Earlier this year, I created an image filter that replaced blocks of an image with emojis. In this post, I revisit some of the weakness of that code.",slug:"emojicamv2",html:"\u003Cp\u003EIn this blog post, I revisit my EmojiCam project and try to rectify some of the major pitfalls the original algorithm had. This rectified version of EmojiCam includes multiscale emojis and better emoji color calibration.\u003C\u002Fp\u003E\n\u003Ch2 id=\"table-of-contents\"\u003ETable of Contents\u003C\u002Fh2\u003E\n\u003Cul\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='old-processing-pipeline';\" href=\"javascript:;\"\u003EOld Processing Pipeline\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='issues';\" href=\"javascript:;\"\u003EIssues\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='rectify';\" href=\"javascript:;\"\u003ERectifications\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003Cli\u003E\u003Ca onclick=\"document.location.hash='results';\" href=\"javascript:;\"\u003EResults\u003C\u002Fa\u003E\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"old-processing-pipeline\"\u003EOld Processing Pipeline\u003C\u002Fh2\u003E\n\u003Cp\u003EThe original processing pipeline for EmojiCam consisted of an algorithm that calibrated all emojis to a given color in 8-bit RGB space. This algorithm took the non-transparent areas of a given emoji and calculated the most significant colors.\u003C\u002Fp\u003E\n\u003Cp\u003EThe significance of a color was computed by doing supervised kMeans clustering with a k value of 3. Then, an RGB matrix with a shape of 256 x 256 x 256 is created and indexed with each of the significant colors being indices into the matrix. Each element in the matrix is an index representing an emoji.\u003C\u002Fp\u003E\n\u003Cp\u003EThis matrix is useful for determining the emoji to use in a given area. It is also very fast since the matrix is a giant look up table.\u003C\u002Fp\u003E\n\u003Cp\u003EAfter this step, EmojiCam creates a blank canvas that will have the emojis applied onto. During the main execution, a given image or video frame are pixelated, where width and height are divided by 24. This means that each emoji is going to be 24x24 pixels in size. Then, we look at each pixel value in the pixelated image, access the emoji index from the RGB matrix, and reproject the location of the pixel in the pixelated image back to the full-sized image. After the location is calculated, the emoji data is applied onto the canvas.\u003C\u002Fp\u003E\n\u003Cp\u003ETo improve the aesthetic, I apply a weighted overlay of the pixelated image (resized to original image size) on to the canvas. I call this alpha-blending (very fancy). You can check the previous post to see how impactful alpha-blending was to the ultimate output. The overall process is also visualized in Figure 1.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Femojicamv2\u002Femojicamprocesspipeline.png\", alt=\"\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 1\u003C\u002Fb\u003E - Overall diagram of the old EmojiCam processing pipeline. The emojis are processed with kMeans to calculate predominant colors and store them into the RGB matrix. Afterwards, we can cache the matrix and use it for a O(1) lookup and apply an emoji onto a certain area of the image.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch2 id=\"issues\"\u003EIssues\u003C\u002Fh2\u003E\n\u003Cp\u003EWith the old processing pipeline from EmojiCam laid out, this section will discuss the pitfalls of this method.\u003C\u002Fp\u003E\n\u003Cp\u003EThe first notable issue is the poor color calibration between emojis to a given color in the source image. The old method used the full 256, 8-bit RGB color space. This means there are 16,777,216 colors to map all the Twitter Emojis to. This results in a \u003Cstrong\u003Ehuge\u003C\u002Fstrong\u003E matrix file (~1 GB) and a very sparse matrix as well.\u003C\u002Fp\u003E\n\u003Cp\u003EAdditionally, using the kMeans algorithm caused unexpected results since initial location of the mean points are randomized and group colors together in an improper way. The clustering algorithm also fails to consider the consolidation of minor color swatches in complex emojis into a cluster that is a misrepresentation of the emoji&#39;s true color distribution.\u003C\u002Fp\u003E\n\u003Cp\u003EAn example of this calibration error is the video below. The emojis are assigned to other colors in a nonsensical nature. In turn, the output looks really bad. The only thing holding EmojiCam together aesthetically is the alpha-blending.\u003C\u002Fp\u003E\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Femojicamv2\u002Ftest_out.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\n\u003Ch2 id=\"rectifications\"\u003ERectifications\u003C\u002Fh2\u003E\n\u003Ch3 id=\"new-calibration\"\u003ENew Calibration\u003C\u002Fh3\u003E\n\u003Cp\u003ESo, how do we fix\u002Fimprove the issues from the previous section? For the RGB matrix issue, I downsample the possible number of colors from 256 to 128.\u003C\u002Fp\u003E\n\u003Cp\u003EI approach emoji calibration from a simpler perspective. Firstly, I will not use all the available emojis since some of them are incredibly complex in terms of color distribution and canvas composition. In order to narrow the number of useful emojis, I removed KMeans and simply calculated the number of unique colors (excluding the transparent areas).\u003C\u002Fp\u003E\n\u003Cp\u003EFor a given emoji, I filter out the unique values with a percentile threshold. Additionally, I also filter emojis with a transparent ratio that is less than 30%. Finally, I classify emojis as &quot;simple&quot; if the most frequent unique color value has a percentage over 85%. All of this filter ensured that the emojis used were simple and have enough color expression to match almost all typical color swatches.\u003C\u002Fp\u003E\n\u003Ch3 id=\"multiscale-emojis\"\u003EMultiscale Emojis\u003C\u002Fh3\u003E\n\u003Cp\u003EAnother improvement I implement is the ability for emojis to be different sizes. This can enable the output to look more artistic rather than just a slightly more interesting version of standard pixelization. But how do we algorithmically implement multiscale emojis and how do we decide to use which emoji scale?\u003C\u002Fp\u003E\n\u003Cp\u003EIn terms of multiscale, I just work with two scales - 24x24 and 48x48. A third 96x96 scale emoji was tried but the results did not look aesthetically pleasing. In terms of where to put the 48x48 scaled emojis, I utilize Discrete Cosine Transform (DCT). DCT generates information regarding an image&#39;s &quot;frequency&quot; and is typically used in image compression algorithms like JPEG and WebP. These algorithms take advantage of the fact that human eyes cannot distinguish high frequency components very well (example in the figure below). By removing high frequency components via quantization, the compression algorithms can reduce file size while preserving image quality.\u003C\u002Fp\u003E\n\u003Cp\u003EI use DCT in order to determine where the low frequency components were. Technically, I convert the RGB image into a YCrCb format, and I apply an 8x8 DCT across the Luminance (Y) channel only. Filtering out only low frequencies was done by taking only the top-left triangle of a given DCT block. Then I sum up the absolute value of each DCT block into a single scalar value in a separate DCT matrix. Then I determine the viable positions by calculating the 80th percentile of all DCT values.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Femojicamv2\u002Fexample_forest.png\", alt=\"\"\u002F\u003E\n\u003Cimg src=\"post-res\u002Femojicamv2\u002Fdct_high_freq_example.png\", alt=\"\"\u002F\u003E\n\u003Cimg src=\"post-res\u002Femojicamv2\u002Fdct_low_freq_example.png\", alt=\"\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 2\u003C\u002Fb\u003E - An example of high frequency and low frequency components generated by DCT. Each DCT block (in this case a 8x8 pixel block) contains a matrix that represent frequencies. \u003Ca href=\"https:\u002F\u002Fwww.researchgate.net\u002Ffigure\u002FDCT-blocks-Where-FL-Low-frequencies-FM-mid-frequencies-FH-High-frequencies_fig3_259763663\"\u003EHere is a good diagram of how the frequencies are aligned in each block\u003C\u002Fa\u003E.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Femojicamv2\u002Fexample_percentile_mask.png\", alt=\"\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 3\u003C\u002Fb\u003E - Example of a position mask of DCT by computing a certain percentile. Using low frequency components means we can increase the scale of emojis only in less detailed parts of the image.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Ch2 id=\"results\"\u003EResults\u003C\u002Fh2\u003E\n\u003Cp\u003EDid those rectifications do any good? Well, let&#39;s see the following comparison:\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\"post-res\u002Femojicamv2\u002Fout_forest.jpg\", alt=\"\", style=\"margin-bottom:5px\"\u002F\u003E\n\u003Cimg src=\"post-res\u002Femojicamv2\u002Fout_forest.jpg.png\", alt=\"\", style=\"margin-bottom:5px\"\u002F\u003E\n\u003Cimg src=\"post-res\u002Femojicamv2\u002Fforest.jpg\", alt=\"\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 4\u003C\u002Fb\u003E - An example of a woodland scene is used. The first image is the original method's output. The image below it is the new method's output. It is clear that the color and emoji selection is much better on the new method. Additionally, the multiscale method works fairly well by choosing relatively non-intrusive areas to upscale to 48x48.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cfigure\u003E\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Femojicamv2\u002Ftest_out-converted.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Femojicamv2\u002Fout_test.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\u003Cvideo autoplay muted loop\u003E\n  \u003Csource src=\"post-res\u002Femojicamv2\u002Ftest.mp4\" type=\"video\u002Fmp4\"\u003E\n  Your browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 5\u003C\u002Fb\u003E - Another example applied to a video. We see very similar results to the image-based example. The newer method produces better results and utilizes the larger scaled emojis effectively.\u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\u003Cp\u003ESo what is next? Unfortunately, the addition of DCT and multiscale emojis make this method slow. It runs less than 1 FPS on a mid-range CPU (AMD Ryzen 2700X). I want to optimize the DCT method I&#39;m using and find a better way of rendering the final output. Also, I need to clean up the code. Maybe by 2024.\u003C\u002Fp\u003E\n",created:"Tue, 20 Dec 2022 20:50:28 GMT",excerpt:"In this blog post, I revisit my EmojiCam project and try to rectify some of the major pitfalls the original algorithm had. This rectified version of EmojiCam includes multiscale emojis and better emoji color calibration.\n",author:"Vijay Rajagopal",readingTime:"7 min read",mediaFilePath:"post-res\u002Femojicamv2\u002Femojicamv2_thumb.png",tags:["image processing "," wip"],art_credit:"Stable Diffusion (Huggingface)"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');var s=document.createElement("script");try{new Function("if(0)import('')")();s.src="/client/client.4da8ec3a.js";s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main","/client/client.4da8ec3a.js")}document.head.appendChild(s)</script> 