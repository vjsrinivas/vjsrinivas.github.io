<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=logo.png rel=icon type=image/png> <script> MathJax = {
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				processEscapes: true
			},
			svg: {
				fontCache: 'global'
			}
		}; </script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async id=MathJax-script></script> <script src=https://kit.fontawesome.com/d1e7542e7f.js crossorigin=anonymous></script> <link href=client/main.677622768.css rel=stylesheet><link href=client/[slug].d2f2c26c.css rel=stylesheet><link href=client/client.4da8ec3a.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>SiamMask ONNX Export Journey</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class=svelte-wyfi55><ul class=svelte-wyfi55><li class=svelte-wyfi55><a href=blog class="svelte-wyfi55 btn-gen" aria-current=page rel=prefetch>Blog</a></li> <li class=svelte-wyfi55><a href=. class="svelte-wyfi55 btn-gen">Home</a></li> </ul></nav> <main class=svelte-1xjjggr> <div class="content svelte-1d9fknd"><h1>SiamMask ONNX Export Journey</h1> <hr> <p>During my masters thesis, I worked on different multiple object tracking (MOT) methods. There is another type of tracking method, single object tracking (SOT), that I always was interested in but never got the chance to try. One such SOT method is called SiamMask, and this blog post will go over how I exported this network to ONNX.</p> <h1 id=single-object-tracking-overview>Single Object Tracking Overview</h1> <p>Single Object Tracking (SOT) is a pretty straight-forward task. A defined ROI is given to the SOT method, and the method attempts to propogate the location of the object within the defined ROI for the next <strong>n</strong> frames. Typically, SOT ROIs are bounding boxes (similar to MOT), but SOT can be extended to other localization types, such as segmentation masks or poses. With the advances in Deep Learning, SOT methods have increasingly relied on deep neural networks in order to derive useful features and information from task-aligned frames.</p> <figure> <img , , alt="Example of different Single Object Tracking tasks - Taken from Do Different Tracking Tasks Require Different Appearance Models? by Wang et. al" src=./post-res/siammask_onnx_export/sot_example.png style=margin-right:auto;margin-left:auto;max-width:400px> <figcaption><b>Fig. 1</b> - An example of different Single Object Tracking sub-tasks. Taken from <a href=https://arxiv.org/pdf/2107.02156.pdf>here</a> </figcaption> </figure> <h1 id=siammask-paper-overview>SiamMask Paper Overview</h1> <p>SiamMask mainly follows in the footsteps of two different Siamese-based SOT methods: SiamFC and SiamRPN. SiamFC was one of the first real-time Siamese-based SOT methods and follows a very simple system for generating bounding boxes. A feature extraction network (ex: ResNet, MobileNet, etc.) takes in the user-defined RoI (typically called the "exemplar" input) and generates a feature map that can be used in the preceding frames (typically called "search" inputs). <strong>The exemplar input is also often smaller than the search inputs</strong> as to conserve speed and reduce features to their most important aspects. When SiamFC ingests a search input, it takes a cross-correlation between the search and the exemplar input. This cross-correlation generates a feature map that can be used to generate bounding boxes at each spatial location. <strong>The output of the cross-correlation operation is also referred to as the "Response of a Candidate Window" (RoW).</strong> SiamFC's main loss objective was logistic regression.</p> <figure> <img , , alt="Diagram of general network flow from original SiamFC paper" src=./post-res/siammask_onnx_export/siamfc_diagram.png style=margin-right:auto;margin-left:auto;max-width:700px> <figcaption><b>Fig. 2</b> - Network flow of SiamFC showing how the subject in the search input can be correlated in the final feature map. Taken from <a href=https://arxiv.org/pdf/1606.09549.pdf>here</a> </figcaption> </figure> <p>SiamRPN extends SiamFC by incorporating a traditional <strong>Regional Proposal Network (RPN) to help guide the RoW and generate more descriptive features.</strong> In implementation, the RPN actually generates two different RoWs for their respective prediction branches (bounding box regression prediction and classification prediction). Each channel within the bounding box-related RoW encodes RPN data (anchors and bounding box confidence score). This robustifies not only the bounding boxes but also the implicit similarity matching. SiamRPN trains with L1 smooth loss (bounding box) and cross-entropy loss (classification score).</p> <figure> <img , , alt="Diagram of general network flow from original SiamRPN paper" src=./post-res/siammask_onnx_export/siamrpn_diagram.png style=margin-right:auto;margin-left:auto;max-width:900px> <figcaption><b>Fig. 3</b> - Network flow of SiamRPN showing the RPN and two branch approach to assist with bounding box prediction. Taken from <a href=https://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf>here</a> </figcaption> </figure> <p>Finally, SiamMask considers utilizing <strong>segmentation as an additional method in increasing the descriptiveness of the RoW features</strong> but without a computationally expensive RPN. RoW generation occurs similar to SiamFC, but instead of cross correlation, SiamMask utilizes depth-wise (channel-wise) correlation. Like SiamRPN, prediction outputs are separated into their respective branches. SiamMask has "bounding box", "segmentation", and "score" branches with their own shallow sub-networks. SiamMask trains with a multi-objective loss function that combines SiamRPN's L1 smooth (bounding box branch) and cross-entropy loss (score branch) with binary logistic regression (segmentation branch). Each part of the multi-objective loss function is weighted by a scalar lambda. The paper mentions that they just guessed some good numbers, where the mask lambda was 32 and the other lambdas were 1. SiamMask also comes in a two-branch variant, where the bounding box branch is removed.</p> <figure> <img , , alt="Diagram showing the network flow of SiamMask (taken from original paper)" src=./post-res/siammask_onnx_export/siammask_diagram.jpeg style=margin-right:auto;margin-left:auto;max-width:700px> <figcaption><b>Fig. 4</b> - Network flow of SiamMask showing a three-branched approach with a focus on the segmentation output. Taken from <a href=https://arxiv.org/pdf/1812.05050.pdf>here</a> </figcaption> </figure> <h1 id=onnx-process>ONNX Process</h1> <mark> Here is the isolated code for exporting SiamMask as an ONNX model: <a href=https://github.com/vjsrinivas/siammask_onnx>Link</a> </mark> <p><strong>Please note that this only allows for the default VOT configuration with the Mask Refinement module.</strong> You can probably adapt this process to work for other versions of SiamMask.</p> <p>Here is the bullet point summary of my PyTorch to ONNX process:</p> <ul> <li>I utilize the PyTorch <code>torch.onnx.export</code> function but, because of the conditional nature of the network, have to recreate the PyTorch model with Torchscript<ul> <li>This means rewriting some of the code to remove most, if not all, Python primitives</li> <li>With a Torchscript model, we are "scripting" (transcompiling into C) rather than the traditional "tracing" (JIT-based) method</li> </ul> </li> <li>During the first frame (n = 0), there is an initialization function that takes in the downsampled input and processes it through the feature extractor<ul> <li>Some unnotable preprocessing specifically for the n=0 state (this is NOT included in the ONNX model and is assumed to be done outside of the model)</li> </ul> </li> <li>For proceeding frames (n > 0), we forward through the entirety of the network (feature extractor, segmentation branch, bounding box branch, and refinement module) and produce a mask.<ul> <li>Some additional post-processing steps to reproject the mask from the input (254x254) to the original image</li> </ul> </li> <li>Because of the conditional nature of the network, we need to take advantage of the <code>If</code> ONNX operator, which will allow us to switch between the n=0 and n>0 states within ONNX<ul> <li>To make this model more robust to other inference engines (ex: TensorRT), we need to ensure that the output and input shapes and datatypes are the same among both conditional branches</li> <li>The above point comes into conflict with the original PyTorch implementation, where the n=0 state has a 127x127 input</li> <li>I work around this by requiring both branches have an input of 254x254, but in n=0 pre-processing, I create a blank image and fill the upper quadrant with the 127x127 input. In the n=0 state, ONNX will trim that 254x254 image input into 127x127</li> </ul> </li> <li>The return package of the model is counter-intuitive, but here is a rundown of them:<ul> <li>Feature maps from the n=0 state<ul> <li>It needs to be used repeatedly in n>0 states to satisfy the identical return requirement</li> </ul> </li> <li>Deltas<ul> <li>Not used in n=0 but used in n>0 for propogating the ROI for each frame in n>0 states</li> </ul> </li> <li>Center position of bounding box<ul> <li>Used in calculation of every new propagation</li> </ul> </li> <li>Size of Bounding Box<ul> <li>Used in calculation of every new propagation</li> </ul> </li> <li>Mask<ul> <li>Self explainatory (a zero matrix for n=0 state)</li> </ul> </li> </ul> </li> </ul> <h2 id=result-examples>Result Examples</h2> <video autoplay controls loop muted> <source src=./post-res/siammask_onnx_export/car_example-converted.mp4 type=video/mp4> </video> <video autoplay controls loop muted> <source src=./post-res/siammask_onnx_export/tennis_example-converted.mp4 type=video/mp4> </video> </div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{title:"SiamMask ONNX Export Journey",description:"Blog post going over exporting one kind of SiamMask model into ONNX",slug:"siammask_onnx_export",html:"\u003Cp\u003EDuring my masters thesis, I worked on different multiple object tracking (MOT) methods. There is another type of tracking method, single object tracking (SOT), that I always was interested in but never got the chance to try. One such SOT method is called SiamMask, and this blog post will go over how I exported this network to ONNX.\u003C\u002Fp\u003E\n\u003Ch1 id=\"single-object-tracking-overview\"\u003ESingle Object Tracking Overview\u003C\u002Fh1\u003E\n\u003Cp\u003ESingle Object Tracking (SOT) is a pretty straight-forward task. A defined ROI is given to the SOT method, and the method attempts to propogate the location of the object within the defined ROI for the next \u003Cstrong\u003En\u003C\u002Fstrong\u003E frames. Typically, SOT ROIs are bounding boxes (similar to MOT), but SOT can be extended to other localization types, such as segmentation masks or poses. With the advances in Deep Learning, SOT methods have increasingly relied on deep neural networks in order to derive useful features and information from task-aligned frames.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fsiammask_onnx_export\u002Fsot_example.png\", alt=\"Example of different Single Object Tracking tasks - Taken from Do Different Tracking Tasks Require Different Appearance Models? by Wang et. al\", style=\"margin-right:auto; margin-left:auto; max-width:400px\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 1\u003C\u002Fb\u003E - An example of different Single Object Tracking sub-tasks. Taken from \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F2107.02156.pdf\"\u003Ehere\u003C\u002Fa\u003E \u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\n\u003Ch1 id=\"siammask-paper-overview\"\u003ESiamMask Paper Overview\u003C\u002Fh1\u003E\n\u003Cp\u003ESiamMask mainly follows in the footsteps of two different Siamese-based SOT methods: SiamFC and SiamRPN. SiamFC was one of the first real-time Siamese-based SOT methods and follows a very simple system for generating bounding boxes. A feature extraction network (ex: ResNet, MobileNet, etc.) takes in the user-defined RoI (typically called the &quot;exemplar&quot; input) and generates a feature map that can be used in the preceding frames (typically called &quot;search&quot; inputs). \u003Cstrong\u003EThe exemplar input is also often smaller than the search inputs\u003C\u002Fstrong\u003E as to conserve speed and reduce features to their most important aspects. When SiamFC ingests a search input, it takes a cross-correlation between the search and the exemplar input. This cross-correlation generates a feature map that can be used to generate bounding boxes at each spatial location. \u003Cstrong\u003EThe output of the cross-correlation operation is also referred to as the &quot;Response of a Candidate Window&quot; (RoW).\u003C\u002Fstrong\u003E SiamFC&#39;s main loss objective was logistic regression.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fsiammask_onnx_export\u002Fsiamfc_diagram.png\", alt=\"Diagram of general network flow from original SiamFC paper\", style=\"margin-right:auto; margin-left:auto; max-width:700px\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 2\u003C\u002Fb\u003E - Network flow of SiamFC showing how the subject in the search input can be correlated in the final feature map. Taken from \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1606.09549.pdf\"\u003Ehere\u003C\u002Fa\u003E \u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\n\u003Cp\u003ESiamRPN extends SiamFC by incorporating a traditional \u003Cstrong\u003ERegional Proposal Network (RPN) to help guide the RoW and generate more descriptive features.\u003C\u002Fstrong\u003E In implementation, the RPN actually generates two different RoWs for their respective prediction branches (bounding box regression prediction and classification prediction). Each channel within the bounding box-related RoW encodes RPN data (anchors and bounding box confidence score). This robustifies not only the bounding boxes but also the implicit similarity matching. SiamRPN trains with L1 smooth loss (bounding box) and cross-entropy loss (classification score).\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fsiammask_onnx_export\u002Fsiamrpn_diagram.png\", alt=\"Diagram of general network flow from original SiamRPN paper\", style=\"margin-right:auto; margin-left:auto; max-width:900px\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 3\u003C\u002Fb\u003E - Network flow of SiamRPN showing the RPN and two branch approach to assist with bounding box prediction. Taken from \u003Ca href=\"https:\u002F\u002Fopenaccess.thecvf.com\u002Fcontent_cvpr_2018\u002Fpapers\u002FLi_High_Performance_Visual_CVPR_2018_paper.pdf\"\u003Ehere\u003C\u002Fa\u003E \u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\n\u003Cp\u003EFinally, SiamMask considers utilizing \u003Cstrong\u003Esegmentation as an additional method in increasing the descriptiveness of the RoW features\u003C\u002Fstrong\u003E but without a computationally expensive RPN. RoW generation occurs similar to SiamFC, but instead of cross correlation, SiamMask utilizes depth-wise (channel-wise) correlation. Like SiamRPN, prediction outputs are separated into their respective branches. SiamMask has &quot;bounding box&quot;, &quot;segmentation&quot;, and &quot;score&quot; branches with their own shallow sub-networks. SiamMask trains with a multi-objective loss function that combines SiamRPN&#39;s L1 smooth (bounding box branch) and cross-entropy loss (score branch) with binary logistic regression (segmentation branch). Each part of the multi-objective loss function is weighted by a scalar lambda. The paper mentions that they just guessed some good numbers, where the mask lambda was 32 and the other lambdas were 1. SiamMask also comes in a two-branch variant, where the bounding box branch is removed.\u003C\u002Fp\u003E\n\u003Cfigure\u003E\n\u003Cimg src=\".\u002Fpost-res\u002Fsiammask_onnx_export\u002Fsiammask_diagram.jpeg\", alt=\"Diagram showing the network flow of SiamMask (taken from original paper)\", style=\"margin-right:auto; margin-left:auto; max-width:700px\"\u002F\u003E\n\u003Cfigcaption\u003E\u003Cb\u003EFig. 4\u003C\u002Fb\u003E - Network flow of SiamMask showing a three-branched approach with a focus on the segmentation output. Taken from \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1812.05050.pdf\"\u003Ehere\u003C\u002Fa\u003E \u003C\u002Ffigcaption\u003E\n\u003C\u002Ffigure\u003E\n\n\n\u003Ch1 id=\"onnx-process\"\u003EONNX Process\u003C\u002Fh1\u003E\n\u003Cmark\u003E\nHere is the isolated code for exporting SiamMask as an ONNX model: \u003Ca href=https:\u002F\u002Fgithub.com\u002Fvjsrinivas\u002Fsiammask_onnx\u003ELink\u003C\u002Fa\u003E\n\u003C\u002Fmark\u003E\n\n\u003Cp\u003E\u003Cstrong\u003EPlease note that this only allows for the default VOT configuration with the Mask Refinement module.\u003C\u002Fstrong\u003E You can probably adapt this process to work for other versions of SiamMask.\u003C\u002Fp\u003E\n\u003Cp\u003EHere is the bullet point summary of my PyTorch to ONNX process:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003EI utilize the PyTorch \u003Ccode\u003Etorch.onnx.export\u003C\u002Fcode\u003E function but, because of the conditional nature of the network, have to recreate the PyTorch model with Torchscript\u003Cul\u003E\n\u003Cli\u003EThis means rewriting some of the code to remove most, if not all, Python primitives\u003C\u002Fli\u003E\n\u003Cli\u003EWith a Torchscript model, we are &quot;scripting&quot; (transcompiling into C) rather than the traditional &quot;tracing&quot; (JIT-based) method\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EDuring the first frame (n = 0), there is an initialization function that takes in the downsampled input and processes it through the feature extractor\u003Cul\u003E\n\u003Cli\u003ESome unnotable preprocessing specifically for the n=0 state (this is NOT included in the ONNX model and is assumed to be done outside of the model)\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EFor proceeding frames (n &gt; 0), we forward through the entirety of the network (feature extractor, segmentation branch, bounding box branch, and refinement module) and produce a mask.\u003Cul\u003E\n\u003Cli\u003ESome additional post-processing steps to reproject the mask from the input (254x254) to the original image\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EBecause of the conditional nature of the network, we need to take advantage of the \u003Ccode\u003EIf\u003C\u002Fcode\u003E ONNX operator, which will allow us to switch between the n=0 and n&gt;0 states within ONNX\u003Cul\u003E\n\u003Cli\u003ETo make this model more robust to other inference engines (ex: TensorRT), we need to ensure that the output and input shapes and datatypes are the same among both conditional branches\u003C\u002Fli\u003E\n\u003Cli\u003EThe above point comes into conflict with the original PyTorch implementation, where the n=0 state has a 127x127 input\u003C\u002Fli\u003E\n\u003Cli\u003EI work around this by requiring both branches have an input of 254x254, but in n=0 pre-processing, I create a blank image and fill the upper quadrant with the 127x127 input. In the n=0 state, ONNX will trim that 254x254 image input into 127x127\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EThe return package of the model is counter-intuitive, but here is a rundown of them:\u003Cul\u003E\n\u003Cli\u003EFeature maps from the n=0 state\u003Cul\u003E\n\u003Cli\u003EIt needs to be used repeatedly in n&gt;0 states to satisfy the identical return requirement\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EDeltas\u003Cul\u003E\n\u003Cli\u003ENot used in n=0 but used in n&gt;0 for propogating the ROI for each frame in n&gt;0 states\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003ECenter position of bounding box\u003Cul\u003E\n\u003Cli\u003EUsed in calculation of every new propagation\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003ESize of Bounding Box\u003Cul\u003E\n\u003Cli\u003EUsed in calculation of every new propagation\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EMask\u003Cul\u003E\n\u003Cli\u003ESelf explainatory (a zero matrix for n=0 state)\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch2 id=\"result-examples\"\u003EResult Examples\u003C\u002Fh2\u003E\n\u003Cvideo controls muted loop autoplay\u003E\n  \u003Csource src=\".\u002Fpost-res\u002Fsiammask_onnx_export\u002Fcar_example-converted.mp4\" type=\"video\u002Fmp4\"\u003E\n\u003C\u002Fvideo\u003E\n\n\u003Cvideo controls muted loop autoplay\u003E\n  \u003Csource src=\".\u002Fpost-res\u002Fsiammask_onnx_export\u002Ftennis_example-converted.mp4\" type=\"video\u002Fmp4\"\u003E\n\u003C\u002Fvideo\u003E\n",created:"Mon, 11 Sep 2023 10:20:28 GMT",excerpt:"During my masters thesis, I worked on different multiple object tracking (MOT) methods. There is another type of tracking method, single object tracking (SOT), that I always was interested in but never got the chance to try. One such SOT method is called SiamMask, and this blog post will go over how I exported this network to ONNX.\n",author:a,readingTime:"6 min read",mediaFilePath:"post-res\u002Fsiammask_onnx_export\u002Fsiammask_onnx_export_thumb.mp4",tags:["machine learning"],art_credit:a}}}("Vijay Rajagopal"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');var s=document.createElement("script");try{new Function("if(0)import('')")();s.src="/client/client.4da8ec3a.js";s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main","/client/client.4da8ec3a.js")}document.head.appendChild(s)</script> 