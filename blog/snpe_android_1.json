{"title":"Journey with Qualcomm SNPE on Android","description":"Description goes here","slug":"snpe_android_1","html":"<p>For the past couple of weeks, I&#39;ve been working on a project that would mimic the behavior of Apple&#39;s &quot;CenterStage&quot;. As a part of that project, I wanted to explore how neural networks were deployed, accelerated, and utilized on smartphone devices.</p>\n<style>\n  .video_override {\n    height: 610px !important;\n  }\n</style>\n\n\n<video autoplay loop muted class=\"video_override\">\n  <source src=\"./post-res/snpe_android_1/demo1.mp4\" type=\"video/mp4\">\nYour browser does not support the video tag.\n</video>\n\n<p>More specifically, what was the best method that produced the fastest neural networks on an Android smartphone? How could you connect this neural network with a live data feed (such as images from the phone&#39;s camera)? What were the limitations of neural networks on smartphones?</p>\n<p>My Masters Thesis focused on deploying neural networks to low-powered, low-resource &quot;edge&quot; devices. The toolkits that exist for these edge devices allowed developers to optimize networks via layer optimization and quantization. A very similar environment exists for smartphones. Hardware-wise, these devices are almost exactly the same as the edge devices I&#39;ve worked with before, but because of their general target user, they have large limitations in terms of application access to said hardware.</p>\n<p>In terms of Android, neural network operations are abstracted and defined in terms of the <strong>NNAPI</strong>, which can do various I/O for data input as well as primitive operations. Many of the toolkits for neural network inferencing on Android smartphones utilize NNAPI methods, but only one of these toolkits has integration on a deeper hardware level - Qualcomm Snapdragon Neural Processing Engine (Qualcomm SNPE). At the time of this writing, Qualcomm produces the majority of smartphone hardware components (CPUs, GPUs, and DSPs), and Qualcomm SNPE offers optimizations aimed directly at a large swath of these components.</p>\n<p>But why not toolkits such as Tensorflow Lite or MNN? Apart from wide smartphone support and less abstracted access to neural network I/O and inference operations, I have no clear basis for choosing Qualcomm SNPE. <a target=\"_blank\" rel=\"nofollow\" href=\"https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk/learning-resources/ai-ml-android-neural-processing/benchmarking-neural-processing-sdk-tensorflow\">Qualcomm might have you believe that their optimization process can have neural networks running faster than any other toolkit</a>. But, <a target=\"_blank\" rel=\"nofollow\" href=\"https://dl.acm.org/doi/fullHtml/10.1145/3485447.3512148\">Zhang et. al</a> show that the &quot;fastest&quot; toolkit is very dependent on network architecture and hardware availability. In terms of hardware availability, most toolkits have access to the CPU and GPU, and Tensorflow Lite <em>just</em> implemented a runtime option with Qualcomm DSP. That said, the concepts and procedures I&#39;ve taken with Qualcomm SNPE would apply to almost any other mobile NN toolkit. </p>\n<h2 id=\"processing-a-neural-network\">Processing a Neural Network</h2>\n<h3 id=\"choosing-neural-network\">Choosing Neural Network</h3>\n<p>Before a neural network can run on an smartphone, it must be processed through your toolkit of choice. This toolkit will regenerate your network&#39;s layer structure, operations, and weights into its own proprietary file format (in SNPE&#39;s case, we generate <strong>dlc</strong> files). This file is then read and intrepreted by the smartphone code <strong>you</strong> write. You also need to write code for preprocessing the input into your network as well as the output from the network. For my first baby steps, I utilized the following off-the-shelf CNNs for image classification - ResNet18, MobileNetv2, and MobileNetv3. These models were all from PyTorch&#39;s torchvision library and were trained on the traditional 1000-class ImageNet dataset. </p>\n<p>Choosing the proper network (or designing a network for mobile deployment) can be tricky. Here are some network attributes I look for:</p>\n<ul>\n<li>Total network parameter size<ul>\n<li>Each parameter is a number and each number takes up space in memory</li>\n</ul>\n</li>\n<li>Total network operations<ul>\n<li>This can be measured in FLOPs or GFLOPs</li>\n<li>Usually correlated with parameter size</li>\n<li>Arguably more important than total network parameter size for real-time computation on edge or smartphone devices</li>\n</ul>\n</li>\n<li>Layer Type/Layer Operation<ul>\n<li>Many toolkits for edge/smartphone platforms have a limited set of supported operations</li>\n<li>Some operations (ex: deformable convolutions) can be computationally expensive and resource heavy</li>\n</ul>\n</li>\n<li>Complexity of network connections<ul>\n<li>Many SOTA or popular networks utilize things like skip-connections, residual connections, etc.</li>\n<li>This is typically not an issue in terms of computation or resources on our targeted hardware </li>\n<li>Architectures containing certain advanced connections can cause bad optimization (quantization or pruning)</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"snpe-network-integration\">SNPE Network Integration</h3>\n<p>Qualcomm SNPE has integration with many of the popular deep learning frameworks. You would think that because the models I&#39;m using were created and trained in PyTorch, I would use the SNPE PyTorch integration. But I found that there are many bugs and issues related to the PyTorch conversion process with the current SNPE build I am using (1.68.0.3932). Instead, I converted the model into <strong>ONNX</strong>, which is a open-source exchange standard for neural networks. This means that if your model is able to be exported as ONNX model, it can be exchanged into other frameworks, such as TensorFlow. So, the tricky part becomes properly exporting your ONNX model into the SNPE ONNX integration.</p>\n<p>For ResNet-based networks, ONNX exporting is flawless, and in turn the SNPE conversion is also completed with no issues. It was similar for MobileNetv2, but MobileNetv3 is where stuff got a little hairy. The main issue is related to the &quot;Layer Type/Layer Operation&quot; point in the list above. The neural network would export from ONNX without issue but would error out in SNPE with a &quot;layer not supported&quot; error. What part of the network was not supported by Qualcomm SNPE? It was the activation functions! One of the advancements that MobileNetv3 had was the integration of two new activation functions - Hard Sigmoid and Hard Swish. Both activations were derivatives of older, more primitive activations function that SNPE <strong>did</strong> support, so I reworked those activation functions into simpler operations in PyTorch. This simplified network was re-exported to ONNX and SNPE successfully.</p>\n<p><strong>Hard Sigmoid:</strong></p>\n<p>It utilizes ReLU6 (which is ReLU but the value is capped between [-6,6] ). </p>\n<pre class=\"language-\"><code class=\"language-\">hard_sigmoid(x) = ReLU6(x+3)/6</code></pre><p><strong>Hard Swish:</strong></p>\n<p>It is just Hard Sigmoid with an extra coefficient:</p>\n<pre class=\"language-\"><code class=\"language-\">hard_swish(x) = x(ReLU6(x+3)/6)</code></pre><p><a target=\"_blank\" rel=\"nofollow\" href=\"https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/mobilenets_v3.html\">Source for equivalents</a></p>\n<p><strong>SNPE Exporting Command:</strong></p>\n<pre class=\"language-bash\"><code class=\"language-bash\">./snpe-onnx-to-dlc --input_network [PATH_TO_ONNX_FILE] --out_node [OUTPUT_NAME] -o [NETWORK].dlc</code></pre><p><em>N.B. --out_node corresponds to the name of the layer that gives you the output(s). You can find the output(s) from an ONNX model by using Netron.  Multiple outputs are separated by a comma.</em></p>\n<h3 id=\"snpe-optimization\">SNPE Optimization</h3>\n<p>One of the main optimizations that SNPE offers is quantization. Quantization reduces a neural network&#39;s parameters (or weights) from a higher precision datatype to a lower precision datatype. Typically, neural networks are trained in FLOAT32 (sometimes FLOAT64). Quantization can step the parameters down to FLOAT16 and INT8. For FLOAT16, it is a very straight forward recasting with little to no loss in task performance. For INT8, you are constrained to a [-128,128] integer range, which means you need to approximate the neural network parameter ranges. This approximation is accomplished with various algorithms, but the majority of them use a calibrating dataset.</p>\n<p>Qualcomm SNPE offers this INT8 quantization, and it is probably the easiest quantization process I&#39;ve used. You need to determine the images you want to use for calibration, and preprocess those images into a matrix that is arranged in a way that SNPE can accept it. The format is a single vector of <code>W*H*3</code>, where <code>W</code> and <code>H</code> are the width and height of the neural network input. The order is also in <code>R_0, G_0, B_0, R_1, G_1, B_1,...R_n, G_n, B_n</code>, where the subscripts are the pixels in Cartesian coordinates. <strong>This is important later on when I&#39;m coding in Android Studio!</strong></p>\n<p>For preprocessing these images in Python, it is a breeze. You read in your images in OpenCV2 or PIL, preprocess it with the proper functions for that network, and save the output to a file with numpy&#39;s <code>tofile</code>. You also need a text file with the paths to those packaged images.</p>\n<p>For reference, this was my code:</p>\n<pre class=\"language-python\"><code class=\"language-python\">with open(os.path.join(OUTPATH, \"quant.txt\"), 'w') as quant_file:\n    # write ur custom output nodes here before for loop:\n    for _f in f:\n        _f2 = _f.replace(\"JPEG\", \"raw\")\n        _fi = os.path.join(IMAGENET_PATH, _f)\n        _img = cv2.imread(_fi) # read image (h,w,3)\n        _img = transform(_img) # transform is a preprocess function from torchvision.transforms.Compose\n        _img = _img.unsqueeze(axis=0).numpy()\n        _img.tofile(os.path.join(OUTPATH, _f2)) # write the numpy file to disk\n        quant_file.write(\"%s\\n\"%os.path.join(os.getcwd(), OUTPATH, _f2)) # write path to text file\n        </code></pre><p>The SNPE quantization to INT8 is the following command:</p>\n<pre class=\"language-bash\"><code class=\"language-bash\">snpe-dlc-quantize \\\n        --input_dlc [INPUT_DLC].dlc \\\n        --input_list [PATH TO TEXT FILE] \\\n        --output_dlc [OUTPUT_QUANT].dlc\\\n        --enable_htp \\ # htp (hexagon tensor processor) made for general NN acceleration\n        --enable_hta # hta (hexagon tensor acceclerator) is a type of convolution hardware accelerator\n</code></pre><h3 id=\"snpe-optimization-considerations\">SNPE Optimization Considerations</h3>\n<p>Typically, I would encourage INT8 quantization to almost all models, but this type of quantization can lead to catastrophic loss in task performance. Make sure you pick images that are representative of your training dataset, and consider the architecture of your network since some architecture lend themselves better for quantization than others. Another thing to consider, especially with smartphones, is the hardware support for INT8 operations</p>\n<p>Typically, CPUs have components that allow for computations in all the major datatypes, but the overall speed (regardless of network quantization) is typically below real-time speeds. Most modern smartphones have GPUs, which enable things like 1080p to 4k video playback or videogames.  Desktop GPUs are known to have different components that enable FLOAT32, FLOAT16, or INT8 computation, <strong>but it seems that Qualcomm&#39;s smartphone GPUs do not support INT8 computation.</strong> This means that if you run neural networks on a smartphone with a Qualcomm GPU, it will be recasted as a FLOAT32 network. From my understanding, Qualcomm&#39;s DSP, HTP, and HTA components <strong>only</strong> support INT8 computations, and you would probably see faster speeds on those hardware components. </p>\n<p>Unfortunately, DSPs and Tensor-based Hardware are <strong>very</strong> new, and only high-end chipsets have it. My current smartphone does not support DSPs, so my networks are using FLOAT32 and run on the GPU.</p>\n<h2 id=\"android-project-setup\">Android Project Setup</h2>\n<p>With my network converted to the DLC format, I can start integrating it into my android application. The application will be a simple camera passthrough that will display the most confident classification. I will use the Camera2 API to access the back-facing camera&#39;s preview data, which will feed into the inference function of the neural network. The Camera2 preview data can be interpreted as a Bitmap, and that Bitmap can be converted and preprocessed into a FloatArray, which can be read by SNPE&#39;s Android APK. </p>\n<p>Disclaimer: This is the first time I&#39;ve used Android Studio. This is the first time I&#39;ve done Android App development. This is the first time I&#39;ve touched Kotlin, so many parts of this application might be unoptimized.</p>\n<h3 id=\"camera-access\">Camera Access</h3>\n<p>Camera access can be tricky in Android with three main types of APIs: Camera, Camera2, and CameraX. Camera2 provides the lowest level access to things like Android image-processing pipelines. It is also one of the most verbose APIs for camera access. I utilized the &quot;Camera2Extensions&quot; project from <a target=\"_blank\" rel=\"nofollow\" href=\"https://github.com/android/camera-samples\">camera_samples</a> to help me get setup. The most important part for me is the <code>onSurfaceTextureUpdated</code> callback that provides an updated <code>SurfaceTexture</code>. To get a frame that we can actually use, I call the <code>getBitmap</code> function that is in the <code>TextureView</code> object. This <code>TextureView</code> object is what the <code>CameraManager</code> (from Camera2) will write to in order to display what the camera sees.</p>\n<h3 id=\"neural-network\">Neural Network</h3>\n<p>Before working with our SNPE neural network, we need to load the provided SNPE APKs located in the &quot;android&quot; folder within your SNPE installation folder (example: <code>/home/snpe-1.68.0.3932/android</code>). Once SNPE is sucessfully loaded in Android Studio, we can load our DLC file and build our neural network on our smartphone:</p>\n<pre class=\"language-kotlin\"><code class=\"language-kotlin\">// Load DLC file that I've put in the raw resource folder\nval modelStream = resources.openRawResource(fileModelPath) //fileModelPath = R.resource.raw.mobilenetv3\nval builder = SNPE.NeuralNetworkBuilder(context) // context is the MainActivity application (can be grabbed in various ways)\nbuilder.setRuntimeOrder(NeuralNetwork.Runtime.GPU) // define which runtime you'll be using\nbuilder.setModel(modelStream, modelStream.available())\nnetwork = builder.build(); // Build!\n\n// Defining an input tensor that will get rewritten when a new camera frame is ready\nval inputShape = network.inputTensorsShapes[\"input0\"]\ninputTensor = network.createFloatTensor(inputShape!![0], inputShape[1], inputShape[2], inputShape[3]);</code></pre><h3 id=\"preprocessing-method\">Preprocessing Method</h3>\n<p>The neural network&#39;s input format is going to be an input <code>Tensor</code>, which is nested into a <code>Map&lt;String, FloatTensor&gt;</code> object. This map object has a key, which is the input node of the network, and a value, which is the input tensor. You can write the contents of a native Kotlin <code>Array</code> object to a SNPE <code>Tensor</code> object by using the <code>write</code> function. Similarly, you can read contents of a SNPE <code>Tensor</code> object to a <code>FloatArray</code> by using the <code>Tensor.read</code> function.</p>\n<p>So the main task becomes: how do we take the Bitmap object, preprocess it with things such as image resizing and standardization in real-time, and convert that preprocessed image into a <code>FloatArray</code> with the same RGB format as mentioned in the &quot;SNPE Optimization&quot; section. All in real-time.</p>\n<p>I quickly found that the typical route of generating a FloatArray of pixels with Bitmap&#39;s <code>getPixel</code> function would mean I would have to manually split channels apart, standardized each pixel one at a time, and reorganize them into the proper format. All of this with Kotlin is <strong>extremely</strong> slow. So, the next best thing is to do this with OpenCV2 and utilize JNI C++ (Java Native Interface). </p>\n<p>With OpenCV2 on Android, I could convert the Bitmap to a <code>Mat</code> object in Kotlin and then feed it into my custom preprocessing C++ function.</p>\n<p>Here is the code for that:</p>\n<pre class=\"language-kotlin\"><code class=\"language-kotlin\">val _bitMap2: Bitmap = Bitmap.createScaledBitmap(_bitMap, 224, 224, true); // _bitMap is the bitmap created from the TextureView object\nUtils.bitmapToMat(_bitMap2, bitMat) // bitMat is already predefined; we are filling in data</code></pre><p>Here is the C++ preprocessing code:</p>\n<pre class=\"language-c++\"><code class=\"language-c++\">extern \"C\"\nJNIEXPORT jfloatArray JNICALL\nJava_com_example_neuralexample2_fragments_CameraFragment_MatNormalization(JNIEnv *env, jobject thiz, jlong matAddr) {\n    float mMean[3] = {0.485, 0.456, 0.406};\n    float mStd[3] = {0.229, 0.224, 0.225}; \n\n    cv::Mat *mainMat  = (cv::Mat*)matAddr;\n    int rows = mainMat->rows;\n    int cols = mainMat->cols;\n    cv::Mat floatImg;\n    mainMat->convertTo(floatImg, CV_32FC1);\n    std::vector<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span><span class=\"token namespace\">cv:</span>:Mat</span><span class=\"token punctuation\">></span></span> floatChannels(4);\n    cv::split(floatImg, floatChannels);\n    floatChannels.pop_back();\n\n    for(int i=0; i&lt;3; i++) {\n        int j, k;\n        float *p;\n\n        for (j = 0; j &lt; rows; ++j) {\n            p = floatChannels[i].ptr<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>float</span><span class=\"token punctuation\">></span></span>(j);\n            for (k = 0; k &lt; cols; ++k) {\n                p[k] = ((p[k]/255.0)-mMean[i])/(mStd[i]);\n            }\n        }\n    }\n\n    cv::merge(floatChannels, floatImg);\n    floatImg = floatImg.reshape(1,1); // This puts it in [R0, G0, B0, R1, G1, B1, ..., Rn, Gn, Bn]\n    float* FmatData = (float*)floatImg.data;\n\n    int rgbLength = rows*cols*3;\n    jfloatArray _jout = env->NewFloatArray(rgbLength);\n \n    env->SetFloatArrayRegion(_jout, 0, rgbLength, FmatData);\n    return _jout; // We are returning a FloatArray\n}</code></pre><p>The whole preprocessing function:</p>\n<pre class=\"language-kotlin\"><code class=\"language-kotlin\">val _bitMap2: Bitmap = Bitmap.createScaledBitmap(_bitMap, 224, 224, true);\nUtils.bitmapToMat(_bitMap2, bitMat)\nvar test: FloatArray = MatNormalization(bitMat.nativeObjAddr)\nUtils.matToBitmap(bitMat, _bitMap2)</code></pre><h3 id=\"inference-method\">Inference Method</h3>\n<p>To infer with the neural network, just package the <code>tensor</code> object into the aforementioned map and extract the output tensor. With the output tensor, post-processing can be applied and the output tensor can be freed.</p>\n<pre class=\"language-kotlin\"><code class=\"language-kotlin\">// inputTensor is a class member so we don't recreate that tensor over and over again\n// inputMat is the preprocessed FloatArray\ninputTensor.write(inputMat, 0, inputMat.size);\nval inputsMap = mapOf<span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>String,</span> <span class=\"token attr-name\">FloatTensor</span><span class=\"token punctuation\">></span></span>(\"input0\" to inputTensor) // input Map object\n/////////////////////////////////////////////\noutputsMap = network.execute(inputsMap); // outputsMap is a lateinit class member\n/////////////////////////////////////////////\n\n// predictionProbs is a FloatArray that is already defined as a class member\noutputsMap[\"output0\"]?.read(predictionProbs,0,1000,0); // Write output tensor result's to float array\n\n// Post-processing:\nval probs = softmax(predictionProbs, predictionProbs.size);\nval idx = probs.withIndex().maxByOrNull { it.value }?.index;\nval displayOut = \"Classified: %s (%.2f)\".format(IMAGENET_CLASSES[idx!!], probs[idx!!])\n\n// Clean up output tensor to avoid memory leaks\nfor (outtensor in outputsMap) {\n    outtensor.value.release()\n}</code></pre><h3 id=\"post-processing-method\">Post-Processing Method</h3>\n<p>For my classification network, post-processing is very simple - apply softmax on the logits. It&#39;s a pretty easy math equation, so I just implemented it in Kotlin directly:</p>\n<pre class=\"language-kotlin\"><code class=\"language-kotlin\">fun softmax(probs:FloatArray) {\n    val expVector = probs.map { exp(it) }\n    val expSum = expVector.sum()\n    for (i in 0 until expVector.size) {\n        probs[i] = expVector[i] / expSum\n    }\n}</code></pre><p>Then I apply an <code>argmax</code> to get the most confident class, and we&#39;re done! The output is put into a nice string with a template of: <code>CLASS_NAME - CLASS_CONFIDENCE%</code>. I have a <code>TextView</code> object that is over the <code>TextureView</code> object, and I send a write update to the <code>TextView</code> object once I get my prediction. This enables the user to see the predictions themselves.</p>\n<h2 id=\"performance\">Performance</h2>\n<p>Neural network performance and application was measured on a Samsung A71 5G UW, which has the following compute specs:</p>\n<ul>\n<li>CPU: Octa-core (1x2.4 GHz Kryo 475 Prime &amp; 1x2.2 GHz Kryo 475 Gold &amp; 6x1.8 GHz Kryo 475 Silver)</li>\n<li>GPU: Qualcomm Adreno 620</li>\n<li>DPS/HTA/HTP: None</li>\n</ul>\n<h3 id=\"neural-network-performance\">Neural Network Performance</h3>\n<p>I measured the inference speed of all the aforementioned neural networks and saw that the quantized models do not improve the speed. This is because, as mentioned before, the Adreno 620 does not support INT8 inferencing so the quantized models are upsampled to FLOAT32. Additionally, FLOAT16 mode is also <strong>not</strong> supported on the Adreno 620.</p>\n<p><img src=\"./post-res/snpe_android_1/performance_infer.png\" alt=\"\"></p>\n<h3 id=\"end-to-end-application-performance\">End-to-End Application Performance</h3>\n<p>I also measured the FPS of the entire application from camera capture to classifier output for 30 seconds. The average end-to-end time for the application was ~53 FPS. With some sporadic drops in the 30-40 FPS range.</p>\n<p><img src=\"./post-res/snpe_android_1/performance.png\" alt=\"\"></p>\n<table>\n<thead>\n<tr>\n<th>Statistical Type</th>\n<th>FPS</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>5th percentile (worst)</td>\n<td>43.48</td>\n</tr>\n<tr>\n<td>99th percentile (best)</td>\n<td>66.67</td>\n</tr>\n<tr>\n<td>Average</td>\n<td>53.49</td>\n</tr>\n</tbody></table>\n<h3 id=\"conclusions\">Conclusions</h3>\n<p>Throughout my journey with Qualcomm SNPE, I realized that the difficulties with this toolkit are essentially the same as the ones I faced when working through TensorRT for NVIDIA Jetsons, Alibaba&#39;s MNN for the Raspberry Pi 4, or Tensorflow Lite for TinyML. One conceptual thing I&#39;ve noticed is that these toolkits are going through the same feature flattening that major ML frameworks have and are still going through. They are all converging into a similar model conversion pipeline, with a growing set of core supported layer operations, and reliable methods for things like quantization. Many of these toolkits might boast speed-ups  over each other with benchmarks of certain networks, but, as stated before, they fail to capture the fact that network speeds can be erratic.</p>\n<p>In terms of development, I learned a lot about Android development. I have experience in traditional media  pipelines, such as GStreamer or FFMPEG, so the Camera2 pipeline was pretty straight-forward for me. The callback function presented a very easy way to capture the preview data, but I never figured out how to edit an image and render it on the same <code>TextureView</code>. Apart from that, getting Qualcomm SNPE working on Android was also fairly easy since the APK integration worked flawlessly. Finally, accelerating various portions of my application, such as the preprocessing step, was incredibly gratifying because I finally got to use C++. I&#39;ve used OpenCV2 in Python and C++, so this native code on Android made me feel at home. </p>\n<p>This probably isn&#39;t the end of my exploration of SNPE or mobile app development in general. I think I&#39;ll keep exploring other types of neural networks - such as object detectors.</p>\n","created":"Wed, 11 Jan 2023 19:45:28 GMT","excerpt":"For the past couple of weeks, I&#39;ve been working on a project that would mimic the behavior of Apple&#39;s &quot;CenterStage&quot;. As a part of that project, I wanted to explore how neural networks were deployed, accelerated, and utilized on smartphone devices.\n","author":"Vijay Rajagopal","readingTime":"16 min read","mediaFilePath":"post-res/snpe_android_1/snpe_android_1_thumb.png","tags":["machine learning"],"art_credit":"Vijay Rajagopal"}