{"title":"DeepSparse vs OpenVINO Benchmarking","description":"Benchmarking of DeepSparse against OpenVINO on sparse classification networks","slug":"pruning_experiment","html":"<p>To support low-latency applications, there has been extensive work in the Deep Learning space to accelerate neural networks.</p>\n<p>In general, these works break down into 3 camps:</p>\n<ul>\n<li><strong>Hardware-Assisted Acceleration:</strong> Hardware that targets the acceleration of common neural network operations; think TensorCores, TPUs, and NPUs.</li>\n<li><strong>Architecture-based Optimizations:</strong> Neural network architectures that focus on reduced operations or extracting more features from less parameters. Think architectures such as MobileNet and EfficientNet.</li>\n<li><strong>Runtime Optimizations:</strong> Any inference-time or inference-level optimization, such as layer fusion, quantization (PTQ or QAT), and pruning</li>\n</ul>\n<p>This post will benchmark an aspect of that last camp. Previous research has shown that sparse weight matrices introduced by unstructured pruning can be used to accelerate network inferencing by <a target=\"_blank\" rel=\"nofollow\" href=\"https://neuralmagic.com/blog/how-neural-magics-deep-sparse-technology-works/\">packing more data into CPU cache</a> and taking advantage of GEMM instructions (ex: AVX2, AVX512-VNNI, and etc.). The main comparison is <strong>not</strong> between different networks or pruning techniques. Rather, we are comparing OpenVINO and DeepSparse - two popular CPU-based inference engines with support for pruned networks.</p>\n<p><strong>TLDR:</strong> DeepSparse outperformed OpenVINO in throughput (img/s) by an average of 12%. It also excelled in all the lower-end compute scenarios tested and with most network architectures. However, on the m7i.xlarge instance (Intel Platinum 8488C), OpenVINO outperformed DeepSparse by an average of 17%. Does this mean I will use DeepSparse? Probably not. </p>\n<figure>\n\n<p><img src=\"post-res/pruning_experiment/overview_line_graph.png\" alt=\"\"></p>\n<figcaption></figcaption>\n</figure>\n\n\n<h2 id=\"methodology\">Methodology</h2>\n<p>I collected common “real-time” convolution-based classification networks and a transformer-based network called MobileViT in their base and pruned versions. Of the pruned models, MobileViT-xs, EfficientNet-B0, and EfficientNet-B2 were pruned by me, and the rest were collected from NeuralMagic&#39;s model zoo. All models were pruned using SparseML with the default Gradient Magnitude Pruning (GMP) method applied to convolutional and linear layers. Since hyperparameters for pruning are architecture-specific, there is no strict paradigm followed for the models I pruned. The top 10 layers of those models were analyzed for sensitivity by randomly masking the weights and calculating the validation accuracy on Imagenet-1k.</p>\n<p>Sparsity values for each layer were subjectively chosen based on the sensitivity analysis. Following NeuralMagic&#39;s recommendation, the pruning process goes through a stabilization, pruning, and finetuning period with an initial learning rate of 0.01. I applied a gamma-based learning rate scheduler and reduced the LR during different parts of the pruning and finetuning stages. The pruning process I implemented requires additional work, as the pruned networks show notable decreases in accuracy on Imagenet-1k, but they serve as reasonable models for benchmarking.</p>\n<figure>\n\n<img src=\"post-res/pruning_experiment/efficientnet_b0_sensitivity_graph_labeled.png\" style=\"width: 75% !important\"/>\n\n<figcaption></figcaption>\n</figure>\n\n<figure>\n\n<img src=\"post-res/pruning_experiment/efficientnet_b2_sensitivity_graph_labeled.png\" style=\"width: 75% !important\"/>\n\n<figcaption></figcaption>\n</figure>\n\n<figure>\n\n<img src=\"post-res/pruning_experiment/mobilenet_xs_sensitivity_graph_labeled.png\" style=\"width: 75% !important\"/>\n\n<figcaption></figcaption>\n</figure>\n\n<figure>\n\n<table>\n<thead>\n<tr>\n<th>Network Name</th>\n<th>Sparsity</th>\n<th>ImageNet-1k Validation Accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MobileViT-xs</td>\n<td>54%</td>\n<td>65.09% <b style=\"color: red;\">(-9.52%)</b></td>\n</tr>\n<tr>\n<td>EfficientNet-B0</td>\n<td>61%</td>\n<td>71.75% <b style=\"color: red;\">(-5.15%)</b></td>\n</tr>\n<tr>\n<td>EfficientNet-B2</td>\n<td>62%</td>\n<td>77.37% <b style=\"color: red;\">(-1.83%)</b></td>\n</tr>\n</tbody></table>\n<figcaption>Table 1. Table shows all models that were pruned by me, their sparsity, the ImageNet-1k validation accuracy, and the baseline delta.</figcaption>\n</figure>\n\n<p>For each model, we export an ONNX version and run dummy inputs at the models&#39; respective input size on ONNXRuntime, OpenVINO, and DeepStream (on the provided benchmarking tool as well as a separate Python that uses the API) inference engines. While testing, I saw that both DeepSparse methods yielded essentially the same results, but both were kept for the sake of completeness. Timing is measured with a simple <code>time.time()</code> before and after each inference call. Each inference engine was ran for 60 seconds (excluding the DeepSparse benchmark tool) and repeated 5 times to account for any noise. These runs were conducted on the following AWS EC2 instances:</p>\n<ul>\n<li><a target=\"_blank\" rel=\"nofollow\" href=\"https://instances.vantage.sh/aws/ec2/t3.xlarge\">t3.xlarge</a></li>\n<li><a target=\"_blank\" rel=\"nofollow\" href=\"https://instances.vantage.sh/aws/ec2/c4.xlarge\">c4.xlarge</a></li>\n<li><a target=\"_blank\" rel=\"nofollow\" href=\"https://instances.vantage.sh/aws/ec2/m6a.xlarge\">m6a.xlarge</a></li>\n<li><a target=\"_blank\" rel=\"nofollow\" href=\"https://instances.vantage.sh/aws/ec2/m5.xlarge\">m5.xlarge</a></li>\n<li><a target=\"_blank\" rel=\"nofollow\" href=\"https://instances.vantage.sh/aws/ec2/m5n.xlarge\">m5n.xlarge</a></li>\n<li><a target=\"_blank\" rel=\"nofollow\" href=\"https://instances.vantage.sh/aws/ec2/m7i.xlarge\">m7i.xlarge</a></li>\n</ul>\n<h2 id=\"results\">Results</h2>\n<p>Across all CPUs except the m7i.large, DeepSparse outperforms OpenVINO by an average of 12% in terms of throughput. On m7i.xlarge, OpenVINO sees a 28% jump in throughput compared to its 2nd best performing AWS instance (m6a.xlarge). On a per model basis, EfficientNet-b0, EfficientNet-b2, and MobileViT did not see significant speed-up when comparing the baseline model and pruned models. These models were ones that I had pruned, so it could be down to the sparisification recipe as well as the sparsity target not hitting an optimization threshold for any of the inference engine. In terms of performance across AWS instances, the increase in throughput was only seen on the higher-cost instances, which typically have stronger CPUs.</p>\n<p>As an aside, the compressive affects of pruning can be seen in Table 2. I use gzip with the compression ratio set to 9 (highest available compression ratio) on all base and pruned models. Excluding ResNet50, which has a higher reduction ratio due to the INT8 datatype affecting filesize, the average reduction ratio is 2.99.</p>\n<figure>\n\n<table>\n<thead>\n<tr>\n<th>Network Name</th>\n<th>Original Size (MB)</th>\n<th>Optimized Size (MB)</th>\n<th>Reduction Ratio</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>MobileViT-xs</td>\n<td>8.62</td>\n<td>4.24</td>\n<td>2.03x</td>\n</tr>\n<tr>\n<td>EfficientNet-B0</td>\n<td>19.59</td>\n<td>8.60</td>\n<td>2.28x</td>\n</tr>\n<tr>\n<td>EfficientNet-B2</td>\n<td>33.76</td>\n<td>17.82</td>\n<td>1.89x</td>\n</tr>\n<tr>\n<td>ResNet50 <b style=\"color: red;\">*</b></td>\n<td>95.16</td>\n<td>10.48</td>\n<td>9.08x</td>\n</tr>\n<tr>\n<td>MobileNetv1</td>\n<td>16.26</td>\n<td>4.70</td>\n<td>3.45x</td>\n</tr>\n<tr>\n<td>VGG19</td>\n<td>533.10</td>\n<td>110.34</td>\n<td>4.83x</td>\n</tr>\n<tr>\n<td>InceptionV3</td>\n<td>88.63</td>\n<td>25.37</td>\n<td>3.49x</td>\n</tr>\n</tbody></table>\n<figcaption>Table 2. Table shows the different sparsity levels and their affect on file size after gzip compression with the compression ratio set to 9 (the highest).</figcaption>\n<figcaption><b style=\"color: red;\">*</b> - INT8 quantization was also applied. </figcaption>\n</figure>\n\n<figure>\n\n<p><img src=\"post-res/pruning_experiment/overview_graph_intel_xeon_e5-2666_v3_cpu.png\" alt=\"\"></p>\n<p><img src=\"post-res/pruning_experiment/overview_graph_amd_epyc_7r13_cpu.png\" alt=\"\"></p>\n<p><img src=\"post-res/pruning_experiment/overview_graph_intel_xeon_platinum_8175m_cpu.png\" alt=\"\"></p>\n<p><img src=\"post-res/pruning_experiment/overview_graph_intel_xeon_platinum_8175m_cpu_2.png\" alt=\"\"></p>\n<p><img src=\"post-res/pruning_experiment/overview_graph_intel_xeon_platinum_8259cl_cpu.png\" alt=\"\"></p>\n<p><img src=\"post-res/pruning_experiment/overview_graph_intel_xeon_platinum_8488c_cpu.png\" alt=\"\"></p>\n<figcaption></figcaption>\n</figure>\n\n<h1 id=\"discussion\">Discussion</h1>\n<p><strong>1. Which inference engine was faster?</strong></p>\n<p>From the results, it&#39;s <strong>typically</strong> DeepSparse. While there is a lot of naunce in terms of hardware and model architecutre, all but one AWS instance type consistently showed higher throughput for models on DeepSparse.</p>\n<p><strong>2. What is the reason for the advantage?</strong></p>\n<p>Since both inference engines are closed source, I can&#39;t tell you. But I would not be surprised if certain kernels in DeepSparse&#39;s arsenal were more optimized for these kinds of networks.</p>\n<p><strong>3. Why didn&#39;t the EfficientNet and MobileViT networks see any throughput improvements after pruning?</strong></p>\n<p>In the &quot;Results&quot; section, I alluded to the sparsity level not being high enough for any engine to take advantage of. I went back and did a pruning session on all three networks at 85% and 99% sparsity. This pruning attempt only focused on sparsity and not the model performance:</p>\n<figure>\n\n<img src=\"post-res/pruning_experiment/fake_prune_throughput.svg\" style=\"width:75% !important\"/>\n\n<figcaption>N.B. CPU used was an AMD Ryzen 7 2700X</figcaption>\n</figure>\n\n<p>Sparsity had little effect on the throughput speed for these networks. EfficientNet-b0 was the only network that saw an increase in throughput, but it was on ONNXRuntime. I don&#39;t fully understand why.</p>\n<p><strong>4. How come OpenVINO was faster on m7i.xlarge?</strong></p>\n<p>Similar to the points made in #2, I can&#39;t provide a definitive answer, but I believe it comes down to the CPU. After retrieving some public specifications for each CPU, I saw that the Intel Xeon Platinum 8488C is the newest CPU with VNNI support and a large L2 &amp; L3 cache. It also had the highest maximum clock frequency amongst all the listed CPUs. These factors are likely the main reason why OpenVINO was faster.</p>\n<table>\n<thead>\n<tr>\n<th>Instance Type</th>\n<th>CPU Type</th>\n<th>CPU Family</th>\n<th>AVX2 Support</th>\n<th>AVX512 Support</th>\n<th>VNNI Support</th>\n<th>L1 Cache (Data)</th>\n<th>L1 Cache (Instruct)</th>\n<th>L2 Cache</th>\n<th>L3 Cache</th>\n<th>Maximum Frequency</th>\n<th>Cores</th>\n<th>Sources</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>m7i.xlarge</td>\n<td>Intel Xeon Platinum 8488C</td>\n<td>Sapphire Rapids-SP</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>48KB</td>\n<td>32KB</td>\n<td>2MB</td>\n<td>105MB</td>\n<td>3.8 GHz</td>\n<td>56</td>\n<td><a target=\"_blank\" rel=\"nofollow\" href=\"https://www.intel.com/content/www/us/en/products/sku/231730/intel-xeon-platinum-8480c-processor-105m-cache-2-00-GHz/specifications.html\">Link</a></td>\n</tr>\n<tr>\n<td>t3.xlarge</td>\n<td>Intel Xeon Platinum 8175M</td>\n<td>Skylake</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n<td>24KB</td>\n<td>24KB</td>\n<td>1MB</td>\n<td>33MB</td>\n<td>2.5 GHz</td>\n<td>24</td>\n<td><a target=\"_blank\" rel=\"nofollow\" href=\"https://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208175M.html\">Link</a></td>\n</tr>\n<tr>\n<td>m5.xlarge</td>\n<td>Intel Xeon Platinum 8175M</td>\n<td>Skylake</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>No</td>\n<td>24KB</td>\n<td>24KB</td>\n<td>1MB</td>\n<td>33MB</td>\n<td>2.5 GHz</td>\n<td>24</td>\n<td><a target=\"_blank\" rel=\"nofollow\" href=\"https://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208175M.html\">Link</a></td>\n</tr>\n<tr>\n<td>m5n.xlarge</td>\n<td>Intel Xeon Platinum 8259CL</td>\n<td>Cascade Lake</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>Yes</td>\n<td>24KB</td>\n<td>24KB</td>\n<td>2MB</td>\n<td>33MB</td>\n<td>2.5 GHz</td>\n<td>32</td>\n<td><a target=\"_blank\" rel=\"nofollow\" href=\"https://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%208259CL.html\">Link</a></td>\n</tr>\n<tr>\n<td>m6a.xlarge</td>\n<td>AMD EPYC 7R13</td>\n<td>Zen3 (EPYC 7003 series)</td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n<td>32KB</td>\n<td>32KB</td>\n<td>512KB(?)</td>\n<td>33MB(?)</td>\n<td>3.5 GHz</td>\n<td>48</td>\n<td><a target=\"_blank\" rel=\"nofollow\" href=\"https://ldbcouncil.org/benchmarks/snb/LDBC_SNB_BI_20230406_SF10000_tigergraph.pdf\">Link</a></td>\n</tr>\n<tr>\n<td>c4.xlarge</td>\n<td>Intel Xeon E5-2666V3</td>\n<td>Haswell-EP (?)</td>\n<td>Yes</td>\n<td>No</td>\n<td>No</td>\n<td>32KB</td>\n<td>32KB</td>\n<td>1MB</td>\n<td>45MB</td>\n<td>3.5 GHz</td>\n<td>10</td>\n<td><a target=\"_blank\" rel=\"nofollow\" href=\"https://www.techpowerup.com/cpu-specs/xeon-e5-2666-v3.c2876\">Link</a></td>\n</tr>\n</tbody></table>\n<p><strong>5. Does this mean I&#39;ll actually use DeepSparse?</strong></p>\n<p>Since entering the industry, my prespective has changed from using tools that will give me the best performance (speed, task performance, etc.) to tools that give me flexiblity. Currently, DeepSparse lacks the level of integration that OpenVINO has - particularly, an integration with ONNXRuntime and deployment stacks like Triton. DeepSparse does have its own server program, but that - again - limits your flexbility. Why have a deployment stack only geared towards CPU? Finally, based on OpenVINO&#39;s performance on the 8488C, we might see that 12% advantage dwindle with newer Intel CPUs.   </p>\n<p><strong>6. Anything else, Vijay?</strong></p>\n<p>I have a nagging feeling that there&#39;s some bias being introduced when utilizing SparseML for network pruning, since it is a tool developed by NeuralMagic. I guess a proper follow up would be to reimplement GMP with pure PyTorch and re-run all the experiments, but PyTorch&#39;s pruning API is much more complicated compared to SparseML.</p>\n","created":"Sat, 11 Jan 2025 19:45:28 GMT","excerpt":"To support low-latency applications, there has been extensive work in the Deep Learning space to accelerate neural networks.\n","author":"Vijay Rajagopal","readingTime":"8 min read","mediaFilePath":"post-res/pruning_experiment/pruning_experiment_thumb.mp4","tags":["machine learning"],"art_credit":"Vijay Rajagopal"}