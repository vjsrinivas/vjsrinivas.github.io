<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=logo.png rel=icon type=image/png> <script> MathJax = {
			tex: {
				inlineMath: [['$', '$'], ['\\(', '\\)']],
				processEscapes: true
			},
			svg: {
				fontCache: 'global'
			}
		}; </script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?version=4.8.0&features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async id=MathJax-script></script> <script src=https://kit.fontawesome.com/d1e7542e7f.js crossorigin=anonymous></script> <link href=client/main.677622768.css rel=stylesheet><link href=client/[slug].d2f2c26c.css rel=stylesheet><link href=client/client.4da8ec3a.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Journey with Qualcomm SNPE on Android</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class=svelte-wyfi55><ul class=svelte-wyfi55><li class=svelte-wyfi55><a href=blog rel=prefetch aria-current=page class="svelte-wyfi55 btn-gen">Blog</a></li> <li class=svelte-wyfi55><a href=. class="svelte-wyfi55 btn-gen">Home</a></li> </ul></nav> <main class=svelte-1xjjggr> <div class="content svelte-1d9fknd"><h1>Journey with Qualcomm SNPE on Android</h1> <hr> <p>For the past couple of weeks, I've been working on a project that would mimic the behavior of Apple's "CenterStage". As a part of that project, I wanted to explore how neural networks were deployed, accelerated, and utilized on smartphone devices.</p> <style>.video_override{height:610px!important}</style> <video autoplay class=video_override loop muted> <source src=./post-res/snpe_android_1/demo1.mp4 type=video/mp4> Your browser does not support the video tag. </video> <p>More specifically, what was the best method that produced the fastest neural networks on an Android smartphone? How could you connect this neural network with a live data feed (such as images from the phone's camera)? What were the limitations of neural networks on smartphones?</p> <p>My Masters Thesis focused on deploying neural networks to low-powered, low-resource "edge" devices. The toolkits that exist for these edge devices allowed developers to optimize networks via layer optimization and quantization. A very similar environment exists for smartphones. Hardware-wise, these devices are almost exactly the same as the edge devices I've worked with before, but because of their general target user, they have large limitations in terms of application access to said hardware.</p> <p>In terms of Android, neural network operations are abstracted and defined in terms of the <strong>NNAPI</strong>, which can do various I/O for data input as well as primitive operations. Many of the toolkits for neural network inferencing on Android smartphones utilize NNAPI methods, but only one of these toolkits has integration on a deeper hardware level - Qualcomm Snapdragon Neural Processing Engine (Qualcomm SNPE). At the time of this writing, Qualcomm produces the majority of smartphone hardware components (CPUs, GPUs, and DSPs), and Qualcomm SNPE offers optimizations aimed directly at a large swath of these components.</p> <p>But why not toolkits such as Tensorflow Lite or MNN? Apart from wide smartphone support and less abstracted access to neural network I/O and inference operations, I have no clear basis for choosing Qualcomm SNPE. <a href=https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk/learning-resources/ai-ml-android-neural-processing/benchmarking-neural-processing-sdk-tensorflow rel=nofollow target=_blank>Qualcomm might have you believe that their optimization process can have neural networks running faster than any other toolkit</a>. But, <a href=https://dl.acm.org/doi/fullHtml/10.1145/3485447.3512148 rel=nofollow target=_blank>Zhang et. al</a> show that the "fastest" toolkit is very dependent on network architecture and hardware availability. In terms of hardware availability, most toolkits have access to the CPU and GPU, and Tensorflow Lite <em>just</em> implemented a runtime option with Qualcomm DSP. That said, the concepts and procedures I've taken with Qualcomm SNPE would apply to almost any other mobile NN toolkit. </p> <h2 id=processing-a-neural-network>Processing a Neural Network</h2> <h3 id=choosing-neural-network>Choosing Neural Network</h3> <p>Before a neural network can run on an smartphone, it must be processed through your toolkit of choice. This toolkit will regenerate your network's layer structure, operations, and weights into its own proprietary file format (in SNPE's case, we generate <strong>dlc</strong> files). This file is then read and intrepreted by the smartphone code <strong>you</strong> write. You also need to write code for preprocessing the input into your network as well as the output from the network. For my first baby steps, I utilized the following off-the-shelf CNNs for image classification - ResNet18, MobileNetv2, and MobileNetv3. These models were all from PyTorch's torchvision library and were trained on the traditional 1000-class ImageNet dataset. </p> <p>Choosing the proper network (or designing a network for mobile deployment) can be tricky. Here are some network attributes I look for:</p> <ul> <li>Total network parameter size<ul> <li>Each parameter is a number and each number takes up space in memory</li> </ul> </li> <li>Total network operations<ul> <li>This can be measured in FLOPs or GFLOPs</li> <li>Usually correlated with parameter size</li> <li>Arguably more important than total network parameter size for real-time computation on edge or smartphone devices</li> </ul> </li> <li>Layer Type/Layer Operation<ul> <li>Many toolkits for edge/smartphone platforms have a limited set of supported operations</li> <li>Some operations (ex: deformable convolutions) can be computationally expensive and resource heavy</li> </ul> </li> <li>Complexity of network connections<ul> <li>Many SOTA or popular networks utilize things like skip-connections, residual connections, etc.</li> <li>This is typically not an issue in terms of computation or resources on our targeted hardware </li> <li>Architectures containing certain advanced connections can cause bad optimization (quantization or pruning)</li> </ul> </li> </ul> <h3 id=snpe-network-integration>SNPE Network Integration</h3> <p>Qualcomm SNPE has integration with many of the popular deep learning frameworks. You would think that because the models I'm using were created and trained in PyTorch, I would use the SNPE PyTorch integration. But I found that there are many bugs and issues related to the PyTorch conversion process with the current SNPE build I am using (1.68.0.3932). Instead, I converted the model into <strong>ONNX</strong>, which is a open-source exchange standard for neural networks. This means that if your model is able to be exported as ONNX model, it can be exchanged into other frameworks, such as TensorFlow. So, the tricky part becomes properly exporting your ONNX model into the SNPE ONNX integration.</p> <p>For ResNet-based networks, ONNX exporting is flawless, and in turn the SNPE conversion is also completed with no issues. It was similar for MobileNetv2, but MobileNetv3 is where stuff got a little hairy. The main issue is related to the "Layer Type/Layer Operation" point in the list above. The neural network would export from ONNX without issue but would error out in SNPE with a "layer not supported" error. What part of the network was not supported by Qualcomm SNPE? It was the activation functions! One of the advancements that MobileNetv3 had was the integration of two new activation functions - Hard Sigmoid and Hard Swish. Both activations were derivatives of older, more primitive activations function that SNPE <strong>did</strong> support, so I reworked those activation functions into simpler operations in PyTorch. This simplified network was re-exported to ONNX and SNPE successfully.</p> <p><strong>Hard Sigmoid:</strong></p> <p>It utilizes ReLU6 (which is ReLU but the value is capped between [-6,6] ). </p> <pre class=language-><code class=language->hard_sigmoid(x) = ReLU6(x+3)/6</code></pre><p><strong>Hard Swish:</strong></p> <p>It is just Hard Sigmoid with an extra coefficient:</p> <pre class=language-><code class=language->hard_swish(x) = x(ReLU6(x+3)/6)</code></pre><p><a href=https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/mobilenets_v3.html rel=nofollow target=_blank>Source for equivalents</a></p> <p><strong>SNPE Exporting Command:</strong></p> <pre class=language-bash><code class=language-bash>./snpe-onnx-to-dlc --input_network [PATH_TO_ONNX_FILE] --out_node [OUTPUT_NAME] -o [NETWORK].dlc</code></pre><p><em>N.B. --out_node corresponds to the name of the layer that gives you the output(s). You can find the output(s) from an ONNX model by using Netron. Multiple outputs are separated by a comma.</em></p> <h3 id=snpe-optimization>SNPE Optimization</h3> <p>One of the main optimizations that SNPE offers is quantization. Quantization reduces a neural network's parameters (or weights) from a higher precision datatype to a lower precision datatype. Typically, neural networks are trained in FLOAT32 (sometimes FLOAT64). Quantization can step the parameters down to FLOAT16 and INT8. For FLOAT16, it is a very straight forward recasting with little to no loss in task performance. For INT8, you are constrained to a [-128,128] integer range, which means you need to approximate the neural network parameter ranges. This approximation is accomplished with various algorithms, but the majority of them use a calibrating dataset.</p> <p>Qualcomm SNPE offers this INT8 quantization, and it is probably the easiest quantization process I've used. You need to determine the images you want to use for calibration, and preprocess those images into a matrix that is arranged in a way that SNPE can accept it. The format is a single vector of <code>W*H*3</code>, where <code>W</code> and <code>H</code> are the width and height of the neural network input. The order is also in <code>R_0, G_0, B_0, R_1, G_1, B_1,...R_n, G_n, B_n</code>, where the subscripts are the pixels in Cartesian coordinates. <strong>This is important later on when I'm coding in Android Studio!</strong></p> <p>For preprocessing these images in Python, it is a breeze. You read in your images in OpenCV2 or PIL, preprocess it with the proper functions for that network, and save the output to a file with numpy's <code>tofile</code>. You also need a text file with the paths to those packaged images.</p> <p>For reference, this was my code:</p> <pre class=language-python><code class=language-python>with open(os.path.join(OUTPATH, "quant.txt"), 'w') as quant_file:
    # write ur custom output nodes here before for loop:
    for _f in f:
        _f2 = _f.replace("JPEG", "raw")
        _fi = os.path.join(IMAGENET_PATH, _f)
        _img = cv2.imread(_fi) # read image (h,w,3)
        _img = transform(_img) # transform is a preprocess function from torchvision.transforms.Compose
        _img = _img.unsqueeze(axis=0).numpy()
        _img.tofile(os.path.join(OUTPATH, _f2)) # write the numpy file to disk
        quant_file.write("%s\n"%os.path.join(os.getcwd(), OUTPATH, _f2)) # write path to text file
        </code></pre><p>The SNPE quantization to INT8 is the following command:</p> <pre class=language-bash><code class=language-bash>snpe-dlc-quantize \
        --input_dlc [INPUT_DLC].dlc \
        --input_list [PATH TO TEXT FILE] \
        --output_dlc [OUTPUT_QUANT].dlc\
        --enable_htp \ # htp (hexagon tensor processor) made for general NN acceleration
        --enable_hta # hta (hexagon tensor acceclerator) is a type of convolution hardware accelerator
</code></pre><h3 id=snpe-optimization-considerations>SNPE Optimization Considerations</h3> <p>Typically, I would encourage INT8 quantization to almost all models, but this type of quantization can lead to catastrophic loss in task performance. Make sure you pick images that are representative of your training dataset, and consider the architecture of your network since some architecture lend themselves better for quantization than others. Another thing to consider, especially with smartphones, is the hardware support for INT8 operations</p> <p>Typically, CPUs have components that allow for computations in all the major datatypes, but the overall speed (regardless of network quantization) is typically below real-time speeds. Most modern smartphones have GPUs, which enable things like 1080p to 4k video playback or videogames. Desktop GPUs are known to have different components that enable FLOAT32, FLOAT16, or INT8 computation, <strong>but it seems that Qualcomm's smartphone GPUs do not support INT8 computation.</strong> This means that if you run neural networks on a smartphone with a Qualcomm GPU, it will be recasted as a FLOAT32 network. From my understanding, Qualcomm's DSP, HTP, and HTA components <strong>only</strong> support INT8 computations, and you would probably see faster speeds on those hardware components. </p> <p>Unfortunately, DSPs and Tensor-based Hardware are <strong>very</strong> new, and only high-end chipsets have it. My current smartphone does not support DSPs, so my networks are using FLOAT32 and run on the GPU.</p> <h2 id=android-project-setup>Android Project Setup</h2> <p>With my network converted to the DLC format, I can start integrating it into my android application. The application will be a simple camera passthrough that will display the most confident classification. I will use the Camera2 API to access the back-facing camera's preview data, which will feed into the inference function of the neural network. The Camera2 preview data can be interpreted as a Bitmap, and that Bitmap can be converted and preprocessed into a FloatArray, which can be read by SNPE's Android APK. </p> <p>Disclaimer: This is the first time I've used Android Studio. This is the first time I've done Android App development. This is the first time I've touched Kotlin, so many parts of this application might be unoptimized.</p> <h3 id=camera-access>Camera Access</h3> <p>Camera access can be tricky in Android with three main types of APIs: Camera, Camera2, and CameraX. Camera2 provides the lowest level access to things like Android image-processing pipelines. It is also one of the most verbose APIs for camera access. I utilized the "Camera2Extensions" project from <a href=https://github.com/android/camera-samples rel=nofollow target=_blank>camera_samples</a> to help me get setup. The most important part for me is the <code>onSurfaceTextureUpdated</code> callback that provides an updated <code>SurfaceTexture</code>. To get a frame that we can actually use, I call the <code>getBitmap</code> function that is in the <code>TextureView</code> object. This <code>TextureView</code> object is what the <code>CameraManager</code> (from Camera2) will write to in order to display what the camera sees.</p> <h3 id=neural-network>Neural Network</h3> <p>Before working with our SNPE neural network, we need to load the provided SNPE APKs located in the "android" folder within your SNPE installation folder (example: <code>/home/snpe-1.68.0.3932/android</code>). Once SNPE is sucessfully loaded in Android Studio, we can load our DLC file and build our neural network on our smartphone:</p> <pre class=language-kotlin><code class=language-kotlin>// Load DLC file that I've put in the raw resource folder
val modelStream = resources.openRawResource(fileModelPath) //fileModelPath = R.resource.raw.mobilenetv3
val builder = SNPE.NeuralNetworkBuilder(context) // context is the MainActivity application (can be grabbed in various ways)
builder.setRuntimeOrder(NeuralNetwork.Runtime.GPU) // define which runtime you'll be using
builder.setModel(modelStream, modelStream.available())
network = builder.build(); // Build!

// Defining an input tensor that will get rewritten when a new camera frame is ready
val inputShape = network.inputTensorsShapes["input0"]
inputTensor = network.createFloatTensor(inputShape!![0], inputShape[1], inputShape[2], inputShape[3]);</code></pre><h3 id=preprocessing-method>Preprocessing Method</h3> <p>The neural network's input format is going to be an input <code>Tensor</code>, which is nested into a <code>Map&lt;String, FloatTensor></code> object. This map object has a key, which is the input node of the network, and a value, which is the input tensor. You can write the contents of a native Kotlin <code>Array</code> object to a SNPE <code>Tensor</code> object by using the <code>write</code> function. Similarly, you can read contents of a SNPE <code>Tensor</code> object to a <code>FloatArray</code> by using the <code>Tensor.read</code> function.</p> <p>So the main task becomes: how do we take the Bitmap object, preprocess it with things such as image resizing and standardization in real-time, and convert that preprocessed image into a <code>FloatArray</code> with the same RGB format as mentioned in the "SNPE Optimization" section. All in real-time.</p> <p>I quickly found that the typical route of generating a FloatArray of pixels with Bitmap's <code>getPixel</code> function would mean I would have to manually split channels apart, standardized each pixel one at a time, and reorganize them into the proper format. All of this with Kotlin is <strong>extremely</strong> slow. So, the next best thing is to do this with OpenCV2 and utilize JNI C++ (Java Native Interface). </p> <p>With OpenCV2 on Android, I could convert the Bitmap to a <code>Mat</code> object in Kotlin and then feed it into my custom preprocessing C++ function.</p> <p>Here is the code for that:</p> <pre class=language-kotlin><code class=language-kotlin>val _bitMap2: Bitmap = Bitmap.createScaledBitmap(_bitMap, 224, 224, true); // _bitMap is the bitmap created from the TextureView object
Utils.bitmapToMat(_bitMap2, bitMat) // bitMat is already predefined; we are filling in data</code></pre><p>Here is the C++ preprocessing code:</p> <pre class=language-c++><code class=language-c++>extern "C"
JNIEXPORT jfloatArray JNICALL
Java_com_example_neuralexample2_fragments_CameraFragment_MatNormalization(JNIEnv *env, jobject thiz, jlong matAddr) {
    float mMean[3] = {0.485, 0.456, 0.406};
    float mStd[3] = {0.229, 0.224, 0.225}; 

    cv::Mat *mainMat  = (cv::Mat*)matAddr;
    int rows = mainMat->rows;
    int cols = mainMat->cols;
    cv::Mat floatImg;
    mainMat->convertTo(floatImg, CV_32FC1);
    std::vector<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span><span class="token namespace">cv:</span>:Mat</span><span class="token punctuation">></span></span> floatChannels(4);
    cv::split(floatImg, floatChannels);
    floatChannels.pop_back();

    for(int i=0; i&lt;3; i++) {
        int j, k;
        float *p;

        for (j = 0; j &lt; rows; ++j) {
            p = floatChannels[i].ptr<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>float</span><span class="token punctuation">></span></span>(j);
            for (k = 0; k &lt; cols; ++k) {
                p[k] = ((p[k]/255.0)-mMean[i])/(mStd[i]);
            }
        }
    }

    cv::merge(floatChannels, floatImg);
    floatImg = floatImg.reshape(1,1); // This puts it in [R0, G0, B0, R1, G1, B1, ..., Rn, Gn, Bn]
    float* FmatData = (float*)floatImg.data;

    int rgbLength = rows*cols*3;
    jfloatArray _jout = env->NewFloatArray(rgbLength);
 
    env->SetFloatArrayRegion(_jout, 0, rgbLength, FmatData);
    return _jout; // We are returning a FloatArray
}</code></pre><p>The whole preprocessing function:</p> <pre class=language-kotlin><code class=language-kotlin>val _bitMap2: Bitmap = Bitmap.createScaledBitmap(_bitMap, 224, 224, true);
Utils.bitmapToMat(_bitMap2, bitMat)
var test: FloatArray = MatNormalization(bitMat.nativeObjAddr)
Utils.matToBitmap(bitMat, _bitMap2)</code></pre><h3 id=inference-method>Inference Method</h3> <p>To infer with the neural network, just package the <code>tensor</code> object into the aforementioned map and extract the output tensor. With the output tensor, post-processing can be applied and the output tensor can be freed.</p> <pre class=language-kotlin><code class=language-kotlin>// inputTensor is a class member so we don't recreate that tensor over and over again
// inputMat is the preprocessed FloatArray
inputTensor.write(inputMat, 0, inputMat.size);
val inputsMap = mapOf<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>String,</span> <span class="token attr-name">FloatTensor</span><span class="token punctuation">></span></span>("input0" to inputTensor) // input Map object
/////////////////////////////////////////////
outputsMap = network.execute(inputsMap); // outputsMap is a lateinit class member
/////////////////////////////////////////////

// predictionProbs is a FloatArray that is already defined as a class member
outputsMap["output0"]?.read(predictionProbs,0,1000,0); // Write output tensor result's to float array

// Post-processing:
val probs = softmax(predictionProbs, predictionProbs.size);
val idx = probs.withIndex().maxByOrNull { it.value }?.index;
val displayOut = "Classified: %s (%.2f)".format(IMAGENET_CLASSES[idx!!], probs[idx!!])

// Clean up output tensor to avoid memory leaks
for (outtensor in outputsMap) {
    outtensor.value.release()
}</code></pre><h3 id=post-processing-method>Post-Processing Method</h3> <p>For my classification network, post-processing is very simple - apply softmax on the logits. It's a pretty easy math equation, so I just implemented it in Kotlin directly:</p> <pre class=language-kotlin><code class=language-kotlin>fun softmax(probs:FloatArray) {
    val expVector = probs.map { exp(it) }
    val expSum = expVector.sum()
    for (i in 0 until expVector.size) {
        probs[i] = expVector[i] / expSum
    }
}</code></pre><p>Then I apply an <code>argmax</code> to get the most confident class, and we're done! The output is put into a nice string with a template of: <code>CLASS_NAME - CLASS_CONFIDENCE%</code>. I have a <code>TextView</code> object that is over the <code>TextureView</code> object, and I send a write update to the <code>TextView</code> object once I get my prediction. This enables the user to see the predictions themselves.</p> <h2 id=performance>Performance</h2> <p>Neural network performance and application was measured on a Samsung A71 5G UW, which has the following compute specs:</p> <ul> <li>CPU: Octa-core (1x2.4 GHz Kryo 475 Prime & 1x2.2 GHz Kryo 475 Gold & 6x1.8 GHz Kryo 475 Silver)</li> <li>GPU: Qualcomm Adreno 620</li> <li>DPS/HTA/HTP: None</li> </ul> <h3 id=neural-network-performance>Neural Network Performance</h3> <p>I measured the inference speed of all the aforementioned neural networks and saw that the quantized models do not improve the speed. This is because, as mentioned before, the Adreno 620 does not support INT8 inferencing so the quantized models are upsampled to FLOAT32. Additionally, FLOAT16 mode is also <strong>not</strong> supported on the Adreno 620.</p> <p><img alt="" src=./post-res/snpe_android_1/performance_infer.png></p> <h3 id=end-to-end-application-performance>End-to-End Application Performance</h3> <p>I also measured the FPS of the entire application from camera capture to classifier output for 30 seconds. The average end-to-end time for the application was ~53 FPS. With some sporadic drops in the 30-40 FPS range.</p> <p><img alt="" src=./post-res/snpe_android_1/performance.png></p> <table> <thead> <tr> <th>Statistical Type</th> <th>FPS</th> </tr> </thead> <tr> <td>5th percentile (worst)</td> <td>43.48</td> </tr> <tr> <td>99th percentile (best)</td> <td>66.67</td> </tr> <tr> <td>Average</td> <td>53.49</td> </tr> </table> <h3 id=conclusions>Conclusions</h3> <p>Throughout my journey with Qualcomm SNPE, I realized that the difficulties with this toolkit are essentially the same as the ones I faced when working through TensorRT for NVIDIA Jetsons, Alibaba's MNN for the Raspberry Pi 4, or Tensorflow Lite for TinyML. One conceptual thing I've noticed is that these toolkits are going through the same feature flattening that major ML frameworks have and are still going through. They are all converging into a similar model conversion pipeline, with a growing set of core supported layer operations, and reliable methods for things like quantization. Many of these toolkits might boast speed-ups over each other with benchmarks of certain networks, but, as stated before, they fail to capture the fact that network speeds can be erratic.</p> <p>In terms of development, I learned a lot about Android development. I have experience in traditional media pipelines, such as GStreamer or FFMPEG, so the Camera2 pipeline was pretty straight-forward for me. The callback function presented a very easy way to capture the preview data, but I never figured out how to edit an image and render it on the same <code>TextureView</code>. Apart from that, getting Qualcomm SNPE working on Android was also fairly easy since the APK integration worked flawlessly. Finally, accelerating various portions of my application, such as the preprocessing step, was incredibly gratifying because I finally got to use C++. I've used OpenCV2 in Python and C++, so this native code on Android made me feel at home. </p> <p>This probably isn't the end of my exploration of SNPE or mobile app development in general. I think I'll keep exploring other types of neural networks - such as object detectors.</p> </div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,(function(a){return {post:{title:"Journey with Qualcomm SNPE on Android",description:"Description goes here",slug:"snpe_android_1",html:"\u003Cp\u003EFor the past couple of weeks, I&#39;ve been working on a project that would mimic the behavior of Apple&#39;s &quot;CenterStage&quot;. As a part of that project, I wanted to explore how neural networks were deployed, accelerated, and utilized on smartphone devices.\u003C\u002Fp\u003E\n\u003Cstyle\u003E\n  .video_override {\n    height: 610px !important;\n  }\n\u003C\u002Fstyle\u003E\n\n\n\u003Cvideo autoplay loop muted class=\"video_override\"\u003E\n  \u003Csource src=\".\u002Fpost-res\u002Fsnpe_android_1\u002Fdemo1.mp4\" type=\"video\u002Fmp4\"\u003E\nYour browser does not support the video tag.\n\u003C\u002Fvideo\u003E\n\n\u003Cp\u003EMore specifically, what was the best method that produced the fastest neural networks on an Android smartphone? How could you connect this neural network with a live data feed (such as images from the phone&#39;s camera)? What were the limitations of neural networks on smartphones?\u003C\u002Fp\u003E\n\u003Cp\u003EMy Masters Thesis focused on deploying neural networks to low-powered, low-resource &quot;edge&quot; devices. The toolkits that exist for these edge devices allowed developers to optimize networks via layer optimization and quantization. A very similar environment exists for smartphones. Hardware-wise, these devices are almost exactly the same as the edge devices I&#39;ve worked with before, but because of their general target user, they have large limitations in terms of application access to said hardware.\u003C\u002Fp\u003E\n\u003Cp\u003EIn terms of Android, neural network operations are abstracted and defined in terms of the \u003Cstrong\u003ENNAPI\u003C\u002Fstrong\u003E, which can do various I\u002FO for data input as well as primitive operations. Many of the toolkits for neural network inferencing on Android smartphones utilize NNAPI methods, but only one of these toolkits has integration on a deeper hardware level - Qualcomm Snapdragon Neural Processing Engine (Qualcomm SNPE). At the time of this writing, Qualcomm produces the majority of smartphone hardware components (CPUs, GPUs, and DSPs), and Qualcomm SNPE offers optimizations aimed directly at a large swath of these components.\u003C\u002Fp\u003E\n\u003Cp\u003EBut why not toolkits such as Tensorflow Lite or MNN? Apart from wide smartphone support and less abstracted access to neural network I\u002FO and inference operations, I have no clear basis for choosing Qualcomm SNPE. \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fdeveloper.qualcomm.com\u002Fsoftware\u002Fqualcomm-neural-processing-sdk\u002Flearning-resources\u002Fai-ml-android-neural-processing\u002Fbenchmarking-neural-processing-sdk-tensorflow\"\u003EQualcomm might have you believe that their optimization process can have neural networks running faster than any other toolkit\u003C\u002Fa\u003E. But, \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002FfullHtml\u002F10.1145\u002F3485447.3512148\"\u003EZhang et. al\u003C\u002Fa\u003E show that the &quot;fastest&quot; toolkit is very dependent on network architecture and hardware availability. In terms of hardware availability, most toolkits have access to the CPU and GPU, and Tensorflow Lite \u003Cem\u003Ejust\u003C\u002Fem\u003E implemented a runtime option with Qualcomm DSP. That said, the concepts and procedures I&#39;ve taken with Qualcomm SNPE would apply to almost any other mobile NN toolkit. \u003C\u002Fp\u003E\n\u003Ch2 id=\"processing-a-neural-network\"\u003EProcessing a Neural Network\u003C\u002Fh2\u003E\n\u003Ch3 id=\"choosing-neural-network\"\u003EChoosing Neural Network\u003C\u002Fh3\u003E\n\u003Cp\u003EBefore a neural network can run on an smartphone, it must be processed through your toolkit of choice. This toolkit will regenerate your network&#39;s layer structure, operations, and weights into its own proprietary file format (in SNPE&#39;s case, we generate \u003Cstrong\u003Edlc\u003C\u002Fstrong\u003E files). This file is then read and intrepreted by the smartphone code \u003Cstrong\u003Eyou\u003C\u002Fstrong\u003E write. You also need to write code for preprocessing the input into your network as well as the output from the network. For my first baby steps, I utilized the following off-the-shelf CNNs for image classification - ResNet18, MobileNetv2, and MobileNetv3. These models were all from PyTorch&#39;s torchvision library and were trained on the traditional 1000-class ImageNet dataset. \u003C\u002Fp\u003E\n\u003Cp\u003EChoosing the proper network (or designing a network for mobile deployment) can be tricky. Here are some network attributes I look for:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003ETotal network parameter size\u003Cul\u003E\n\u003Cli\u003EEach parameter is a number and each number takes up space in memory\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003ETotal network operations\u003Cul\u003E\n\u003Cli\u003EThis can be measured in FLOPs or GFLOPs\u003C\u002Fli\u003E\n\u003Cli\u003EUsually correlated with parameter size\u003C\u002Fli\u003E\n\u003Cli\u003EArguably more important than total network parameter size for real-time computation on edge or smartphone devices\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003ELayer Type\u002FLayer Operation\u003Cul\u003E\n\u003Cli\u003EMany toolkits for edge\u002Fsmartphone platforms have a limited set of supported operations\u003C\u002Fli\u003E\n\u003Cli\u003ESome operations (ex: deformable convolutions) can be computationally expensive and resource heavy\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003Cli\u003EComplexity of network connections\u003Cul\u003E\n\u003Cli\u003EMany SOTA or popular networks utilize things like skip-connections, residual connections, etc.\u003C\u002Fli\u003E\n\u003Cli\u003EThis is typically not an issue in terms of computation or resources on our targeted hardware \u003C\u002Fli\u003E\n\u003Cli\u003EArchitectures containing certain advanced connections can cause bad optimization (quantization or pruning)\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"snpe-network-integration\"\u003ESNPE Network Integration\u003C\u002Fh3\u003E\n\u003Cp\u003EQualcomm SNPE has integration with many of the popular deep learning frameworks. You would think that because the models I&#39;m using were created and trained in PyTorch, I would use the SNPE PyTorch integration. But I found that there are many bugs and issues related to the PyTorch conversion process with the current SNPE build I am using (1.68.0.3932). Instead, I converted the model into \u003Cstrong\u003EONNX\u003C\u002Fstrong\u003E, which is a open-source exchange standard for neural networks. This means that if your model is able to be exported as ONNX model, it can be exchanged into other frameworks, such as TensorFlow. So, the tricky part becomes properly exporting your ONNX model into the SNPE ONNX integration.\u003C\u002Fp\u003E\n\u003Cp\u003EFor ResNet-based networks, ONNX exporting is flawless, and in turn the SNPE conversion is also completed with no issues. It was similar for MobileNetv2, but MobileNetv3 is where stuff got a little hairy. The main issue is related to the &quot;Layer Type\u002FLayer Operation&quot; point in the list above. The neural network would export from ONNX without issue but would error out in SNPE with a &quot;layer not supported&quot; error. What part of the network was not supported by Qualcomm SNPE? It was the activation functions! One of the advancements that MobileNetv3 had was the integration of two new activation functions - Hard Sigmoid and Hard Swish. Both activations were derivatives of older, more primitive activations function that SNPE \u003Cstrong\u003Edid\u003C\u002Fstrong\u003E support, so I reworked those activation functions into simpler operations in PyTorch. This simplified network was re-exported to ONNX and SNPE successfully.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003EHard Sigmoid:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EIt utilizes ReLU6 (which is ReLU but the value is capped between [-6,6] ). \u003C\u002Fp\u003E\n\u003Cpre class=\"language-\"\u003E\u003Ccode class=\"language-\"\u003Ehard_sigmoid(x) = ReLU6(x+3)\u002F6\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cstrong\u003EHard Swish:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EIt is just Hard Sigmoid with an extra coefficient:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-\"\u003E\u003Ccode class=\"language-\"\u003Ehard_swish(x) = x(ReLU6(x+3)\u002F6)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fpatrick-llgc.github.io\u002FLearning-Deep-Learning\u002Fpaper_notes\u002Fmobilenets_v3.html\"\u003ESource for equivalents\u003C\u002Fa\u003E\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ESNPE Exporting Command:\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003E.\u002Fsnpe-onnx-to-dlc --input_network [PATH_TO_ONNX_FILE] --out_node [OUTPUT_NAME] -o [NETWORK].dlc\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003E\u003Cem\u003EN.B. --out_node corresponds to the name of the layer that gives you the output(s). You can find the output(s) from an ONNX model by using Netron.  Multiple outputs are separated by a comma.\u003C\u002Fem\u003E\u003C\u002Fp\u003E\n\u003Ch3 id=\"snpe-optimization\"\u003ESNPE Optimization\u003C\u002Fh3\u003E\n\u003Cp\u003EOne of the main optimizations that SNPE offers is quantization. Quantization reduces a neural network&#39;s parameters (or weights) from a higher precision datatype to a lower precision datatype. Typically, neural networks are trained in FLOAT32 (sometimes FLOAT64). Quantization can step the parameters down to FLOAT16 and INT8. For FLOAT16, it is a very straight forward recasting with little to no loss in task performance. For INT8, you are constrained to a [-128,128] integer range, which means you need to approximate the neural network parameter ranges. This approximation is accomplished with various algorithms, but the majority of them use a calibrating dataset.\u003C\u002Fp\u003E\n\u003Cp\u003EQualcomm SNPE offers this INT8 quantization, and it is probably the easiest quantization process I&#39;ve used. You need to determine the images you want to use for calibration, and preprocess those images into a matrix that is arranged in a way that SNPE can accept it. The format is a single vector of \u003Ccode\u003EW*H*3\u003C\u002Fcode\u003E, where \u003Ccode\u003EW\u003C\u002Fcode\u003E and \u003Ccode\u003EH\u003C\u002Fcode\u003E are the width and height of the neural network input. The order is also in \u003Ccode\u003ER_0, G_0, B_0, R_1, G_1, B_1,...R_n, G_n, B_n\u003C\u002Fcode\u003E, where the subscripts are the pixels in Cartesian coordinates. \u003Cstrong\u003EThis is important later on when I&#39;m coding in Android Studio!\u003C\u002Fstrong\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EFor preprocessing these images in Python, it is a breeze. You read in your images in OpenCV2 or PIL, preprocess it with the proper functions for that network, and save the output to a file with numpy&#39;s \u003Ccode\u003Etofile\u003C\u002Fcode\u003E. You also need a text file with the paths to those packaged images.\u003C\u002Fp\u003E\n\u003Cp\u003EFor reference, this was my code:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-python\"\u003E\u003Ccode class=\"language-python\"\u003Ewith open(os.path.join(OUTPATH, \"quant.txt\"), 'w') as quant_file:\n    # write ur custom output nodes here before for loop:\n    for _f in f:\n        _f2 = _f.replace(\"JPEG\", \"raw\")\n        _fi = os.path.join(IMAGENET_PATH, _f)\n        _img = cv2.imread(_fi) # read image (h,w,3)\n        _img = transform(_img) # transform is a preprocess function from torchvision.transforms.Compose\n        _img = _img.unsqueeze(axis=0).numpy()\n        _img.tofile(os.path.join(OUTPATH, _f2)) # write the numpy file to disk\n        quant_file.write(\"%s\\n\"%os.path.join(os.getcwd(), OUTPATH, _f2)) # write path to text file\n        \u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EThe SNPE quantization to INT8 is the following command:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-bash\"\u003E\u003Ccode class=\"language-bash\"\u003Esnpe-dlc-quantize \\\n        --input_dlc [INPUT_DLC].dlc \\\n        --input_list [PATH TO TEXT FILE] \\\n        --output_dlc [OUTPUT_QUANT].dlc\\\n        --enable_htp \\ # htp (hexagon tensor processor) made for general NN acceleration\n        --enable_hta # hta (hexagon tensor acceclerator) is a type of convolution hardware accelerator\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch3 id=\"snpe-optimization-considerations\"\u003ESNPE Optimization Considerations\u003C\u002Fh3\u003E\n\u003Cp\u003ETypically, I would encourage INT8 quantization to almost all models, but this type of quantization can lead to catastrophic loss in task performance. Make sure you pick images that are representative of your training dataset, and consider the architecture of your network since some architecture lend themselves better for quantization than others. Another thing to consider, especially with smartphones, is the hardware support for INT8 operations\u003C\u002Fp\u003E\n\u003Cp\u003ETypically, CPUs have components that allow for computations in all the major datatypes, but the overall speed (regardless of network quantization) is typically below real-time speeds. Most modern smartphones have GPUs, which enable things like 1080p to 4k video playback or videogames.  Desktop GPUs are known to have different components that enable FLOAT32, FLOAT16, or INT8 computation, \u003Cstrong\u003Ebut it seems that Qualcomm&#39;s smartphone GPUs do not support INT8 computation.\u003C\u002Fstrong\u003E This means that if you run neural networks on a smartphone with a Qualcomm GPU, it will be recasted as a FLOAT32 network. From my understanding, Qualcomm&#39;s DSP, HTP, and HTA components \u003Cstrong\u003Eonly\u003C\u002Fstrong\u003E support INT8 computations, and you would probably see faster speeds on those hardware components. \u003C\u002Fp\u003E\n\u003Cp\u003EUnfortunately, DSPs and Tensor-based Hardware are \u003Cstrong\u003Every\u003C\u002Fstrong\u003E new, and only high-end chipsets have it. My current smartphone does not support DSPs, so my networks are using FLOAT32 and run on the GPU.\u003C\u002Fp\u003E\n\u003Ch2 id=\"android-project-setup\"\u003EAndroid Project Setup\u003C\u002Fh2\u003E\n\u003Cp\u003EWith my network converted to the DLC format, I can start integrating it into my android application. The application will be a simple camera passthrough that will display the most confident classification. I will use the Camera2 API to access the back-facing camera&#39;s preview data, which will feed into the inference function of the neural network. The Camera2 preview data can be interpreted as a Bitmap, and that Bitmap can be converted and preprocessed into a FloatArray, which can be read by SNPE&#39;s Android APK. \u003C\u002Fp\u003E\n\u003Cp\u003EDisclaimer: This is the first time I&#39;ve used Android Studio. This is the first time I&#39;ve done Android App development. This is the first time I&#39;ve touched Kotlin, so many parts of this application might be unoptimized.\u003C\u002Fp\u003E\n\u003Ch3 id=\"camera-access\"\u003ECamera Access\u003C\u002Fh3\u003E\n\u003Cp\u003ECamera access can be tricky in Android with three main types of APIs: Camera, Camera2, and CameraX. Camera2 provides the lowest level access to things like Android image-processing pipelines. It is also one of the most verbose APIs for camera access. I utilized the &quot;Camera2Extensions&quot; project from \u003Ca target=\"_blank\" rel=\"nofollow\" href=\"https:\u002F\u002Fgithub.com\u002Fandroid\u002Fcamera-samples\"\u003Ecamera_samples\u003C\u002Fa\u003E to help me get setup. The most important part for me is the \u003Ccode\u003EonSurfaceTextureUpdated\u003C\u002Fcode\u003E callback that provides an updated \u003Ccode\u003ESurfaceTexture\u003C\u002Fcode\u003E. To get a frame that we can actually use, I call the \u003Ccode\u003EgetBitmap\u003C\u002Fcode\u003E function that is in the \u003Ccode\u003ETextureView\u003C\u002Fcode\u003E object. This \u003Ccode\u003ETextureView\u003C\u002Fcode\u003E object is what the \u003Ccode\u003ECameraManager\u003C\u002Fcode\u003E (from Camera2) will write to in order to display what the camera sees.\u003C\u002Fp\u003E\n\u003Ch3 id=\"neural-network\"\u003ENeural Network\u003C\u002Fh3\u003E\n\u003Cp\u003EBefore working with our SNPE neural network, we need to load the provided SNPE APKs located in the &quot;android&quot; folder within your SNPE installation folder (example: \u003Ccode\u003E\u002Fhome\u002Fsnpe-1.68.0.3932\u002Fandroid\u003C\u002Fcode\u003E). Once SNPE is sucessfully loaded in Android Studio, we can load our DLC file and build our neural network on our smartphone:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-kotlin\"\u003E\u003Ccode class=\"language-kotlin\"\u003E\u002F\u002F Load DLC file that I've put in the raw resource folder\nval modelStream = resources.openRawResource(fileModelPath) \u002F\u002FfileModelPath = R.resource.raw.mobilenetv3\nval builder = SNPE.NeuralNetworkBuilder(context) \u002F\u002F context is the MainActivity application (can be grabbed in various ways)\nbuilder.setRuntimeOrder(NeuralNetwork.Runtime.GPU) \u002F\u002F define which runtime you'll be using\nbuilder.setModel(modelStream, modelStream.available())\nnetwork = builder.build(); \u002F\u002F Build!\n\n\u002F\u002F Defining an input tensor that will get rewritten when a new camera frame is ready\nval inputShape = network.inputTensorsShapes[\"input0\"]\ninputTensor = network.createFloatTensor(inputShape!![0], inputShape[1], inputShape[2], inputShape[3]);\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch3 id=\"preprocessing-method\"\u003EPreprocessing Method\u003C\u002Fh3\u003E\n\u003Cp\u003EThe neural network&#39;s input format is going to be an input \u003Ccode\u003ETensor\u003C\u002Fcode\u003E, which is nested into a \u003Ccode\u003EMap&lt;String, FloatTensor&gt;\u003C\u002Fcode\u003E object. This map object has a key, which is the input node of the network, and a value, which is the input tensor. You can write the contents of a native Kotlin \u003Ccode\u003EArray\u003C\u002Fcode\u003E object to a SNPE \u003Ccode\u003ETensor\u003C\u002Fcode\u003E object by using the \u003Ccode\u003Ewrite\u003C\u002Fcode\u003E function. Similarly, you can read contents of a SNPE \u003Ccode\u003ETensor\u003C\u002Fcode\u003E object to a \u003Ccode\u003EFloatArray\u003C\u002Fcode\u003E by using the \u003Ccode\u003ETensor.read\u003C\u002Fcode\u003E function.\u003C\u002Fp\u003E\n\u003Cp\u003ESo the main task becomes: how do we take the Bitmap object, preprocess it with things such as image resizing and standardization in real-time, and convert that preprocessed image into a \u003Ccode\u003EFloatArray\u003C\u002Fcode\u003E with the same RGB format as mentioned in the &quot;SNPE Optimization&quot; section. All in real-time.\u003C\u002Fp\u003E\n\u003Cp\u003EI quickly found that the typical route of generating a FloatArray of pixels with Bitmap&#39;s \u003Ccode\u003EgetPixel\u003C\u002Fcode\u003E function would mean I would have to manually split channels apart, standardized each pixel one at a time, and reorganize them into the proper format. All of this with Kotlin is \u003Cstrong\u003Eextremely\u003C\u002Fstrong\u003E slow. So, the next best thing is to do this with OpenCV2 and utilize JNI C++ (Java Native Interface). \u003C\u002Fp\u003E\n\u003Cp\u003EWith OpenCV2 on Android, I could convert the Bitmap to a \u003Ccode\u003EMat\u003C\u002Fcode\u003E object in Kotlin and then feed it into my custom preprocessing C++ function.\u003C\u002Fp\u003E\n\u003Cp\u003EHere is the code for that:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-kotlin\"\u003E\u003Ccode class=\"language-kotlin\"\u003Eval _bitMap2: Bitmap = Bitmap.createScaledBitmap(_bitMap, 224, 224, true); \u002F\u002F _bitMap is the bitmap created from the TextureView object\nUtils.bitmapToMat(_bitMap2, bitMat) \u002F\u002F bitMat is already predefined; we are filling in data\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EHere is the C++ preprocessing code:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-c++\"\u003E\u003Ccode class=\"language-c++\"\u003Eextern \"C\"\nJNIEXPORT jfloatArray JNICALL\nJava_com_example_neuralexample2_fragments_CameraFragment_MatNormalization(JNIEnv *env, jobject thiz, jlong matAddr) {\n    float mMean[3] = {0.485, 0.456, 0.406};\n    float mStd[3] = {0.229, 0.224, 0.225}; \n\n    cv::Mat *mainMat  = (cv::Mat*)matAddr;\n    int rows = mainMat-\u003Erows;\n    int cols = mainMat-\u003Ecols;\n    cv::Mat floatImg;\n    mainMat-\u003EconvertTo(floatImg, CV_32FC1);\n    std::vector\u003Cspan class=\"token tag\"\u003E\u003Cspan class=\"token tag\"\u003E\u003Cspan class=\"token punctuation\"\u003E&lt;\u003C\u002Fspan\u003E\u003Cspan class=\"token namespace\"\u003Ecv:\u003C\u002Fspan\u003E:Mat\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E floatChannels(4);\n    cv::split(floatImg, floatChannels);\n    floatChannels.pop_back();\n\n    for(int i=0; i&lt;3; i++) {\n        int j, k;\n        float *p;\n\n        for (j = 0; j &lt; rows; ++j) {\n            p = floatChannels[i].ptr\u003Cspan class=\"token tag\"\u003E\u003Cspan class=\"token tag\"\u003E\u003Cspan class=\"token punctuation\"\u003E&lt;\u003C\u002Fspan\u003Efloat\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E(j);\n            for (k = 0; k &lt; cols; ++k) {\n                p[k] = ((p[k]\u002F255.0)-mMean[i])\u002F(mStd[i]);\n            }\n        }\n    }\n\n    cv::merge(floatChannels, floatImg);\n    floatImg = floatImg.reshape(1,1); \u002F\u002F This puts it in [R0, G0, B0, R1, G1, B1, ..., Rn, Gn, Bn]\n    float* FmatData = (float*)floatImg.data;\n\n    int rgbLength = rows*cols*3;\n    jfloatArray _jout = env-\u003ENewFloatArray(rgbLength);\n \n    env-\u003ESetFloatArrayRegion(_jout, 0, rgbLength, FmatData);\n    return _jout; \u002F\u002F We are returning a FloatArray\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EThe whole preprocessing function:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-kotlin\"\u003E\u003Ccode class=\"language-kotlin\"\u003Eval _bitMap2: Bitmap = Bitmap.createScaledBitmap(_bitMap, 224, 224, true);\nUtils.bitmapToMat(_bitMap2, bitMat)\nvar test: FloatArray = MatNormalization(bitMat.nativeObjAddr)\nUtils.matToBitmap(bitMat, _bitMap2)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch3 id=\"inference-method\"\u003EInference Method\u003C\u002Fh3\u003E\n\u003Cp\u003ETo infer with the neural network, just package the \u003Ccode\u003Etensor\u003C\u002Fcode\u003E object into the aforementioned map and extract the output tensor. With the output tensor, post-processing can be applied and the output tensor can be freed.\u003C\u002Fp\u003E\n\u003Cpre class=\"language-kotlin\"\u003E\u003Ccode class=\"language-kotlin\"\u003E\u002F\u002F inputTensor is a class member so we don't recreate that tensor over and over again\n\u002F\u002F inputMat is the preprocessed FloatArray\ninputTensor.write(inputMat, 0, inputMat.size);\nval inputsMap = mapOf\u003Cspan class=\"token tag\"\u003E\u003Cspan class=\"token tag\"\u003E\u003Cspan class=\"token punctuation\"\u003E&lt;\u003C\u002Fspan\u003EString,\u003C\u002Fspan\u003E \u003Cspan class=\"token attr-name\"\u003EFloatTensor\u003C\u002Fspan\u003E\u003Cspan class=\"token punctuation\"\u003E\u003E\u003C\u002Fspan\u003E\u003C\u002Fspan\u003E(\"input0\" to inputTensor) \u002F\u002F input Map object\n\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\noutputsMap = network.execute(inputsMap); \u002F\u002F outputsMap is a lateinit class member\n\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\u002F\n\n\u002F\u002F predictionProbs is a FloatArray that is already defined as a class member\noutputsMap[\"output0\"]?.read(predictionProbs,0,1000,0); \u002F\u002F Write output tensor result's to float array\n\n\u002F\u002F Post-processing:\nval probs = softmax(predictionProbs, predictionProbs.size);\nval idx = probs.withIndex().maxByOrNull { it.value }?.index;\nval displayOut = \"Classified: %s (%.2f)\".format(IMAGENET_CLASSES[idx!!], probs[idx!!])\n\n\u002F\u002F Clean up output tensor to avoid memory leaks\nfor (outtensor in outputsMap) {\n    outtensor.value.release()\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Ch3 id=\"post-processing-method\"\u003EPost-Processing Method\u003C\u002Fh3\u003E\n\u003Cp\u003EFor my classification network, post-processing is very simple - apply softmax on the logits. It&#39;s a pretty easy math equation, so I just implemented it in Kotlin directly:\u003C\u002Fp\u003E\n\u003Cpre class=\"language-kotlin\"\u003E\u003Ccode class=\"language-kotlin\"\u003Efun softmax(probs:FloatArray) {\n    val expVector = probs.map { exp(it) }\n    val expSum = expVector.sum()\n    for (i in 0 until expVector.size) {\n        probs[i] = expVector[i] \u002F expSum\n    }\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cp\u003EThen I apply an \u003Ccode\u003Eargmax\u003C\u002Fcode\u003E to get the most confident class, and we&#39;re done! The output is put into a nice string with a template of: \u003Ccode\u003ECLASS_NAME - CLASS_CONFIDENCE%\u003C\u002Fcode\u003E. I have a \u003Ccode\u003ETextView\u003C\u002Fcode\u003E object that is over the \u003Ccode\u003ETextureView\u003C\u002Fcode\u003E object, and I send a write update to the \u003Ccode\u003ETextView\u003C\u002Fcode\u003E object once I get my prediction. This enables the user to see the predictions themselves.\u003C\u002Fp\u003E\n\u003Ch2 id=\"performance\"\u003EPerformance\u003C\u002Fh2\u003E\n\u003Cp\u003ENeural network performance and application was measured on a Samsung A71 5G UW, which has the following compute specs:\u003C\u002Fp\u003E\n\u003Cul\u003E\n\u003Cli\u003ECPU: Octa-core (1x2.4 GHz Kryo 475 Prime &amp; 1x2.2 GHz Kryo 475 Gold &amp; 6x1.8 GHz Kryo 475 Silver)\u003C\u002Fli\u003E\n\u003Cli\u003EGPU: Qualcomm Adreno 620\u003C\u002Fli\u003E\n\u003Cli\u003EDPS\u002FHTA\u002FHTP: None\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch3 id=\"neural-network-performance\"\u003ENeural Network Performance\u003C\u002Fh3\u003E\n\u003Cp\u003EI measured the inference speed of all the aforementioned neural networks and saw that the quantized models do not improve the speed. This is because, as mentioned before, the Adreno 620 does not support INT8 inferencing so the quantized models are upsampled to FLOAT32. Additionally, FLOAT16 mode is also \u003Cstrong\u003Enot\u003C\u002Fstrong\u003E supported on the Adreno 620.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\".\u002Fpost-res\u002Fsnpe_android_1\u002Fperformance_infer.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Ch3 id=\"end-to-end-application-performance\"\u003EEnd-to-End Application Performance\u003C\u002Fh3\u003E\n\u003Cp\u003EI also measured the FPS of the entire application from camera capture to classifier output for 30 seconds. The average end-to-end time for the application was ~53 FPS. With some sporadic drops in the 30-40 FPS range.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg src=\".\u002Fpost-res\u002Fsnpe_android_1\u002Fperformance.png\" alt=\"\"\u003E\u003C\u002Fp\u003E\n\u003Ctable\u003E\n\u003Cthead\u003E\n\u003Ctr\u003E\n\u003Cth\u003EStatistical Type\u003C\u002Fth\u003E\n\u003Cth\u003EFPS\u003C\u002Fth\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Fthead\u003E\n\u003Ctbody\u003E\u003Ctr\u003E\n\u003Ctd\u003E5th percentile (worst)\u003C\u002Ftd\u003E\n\u003Ctd\u003E43.48\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E99th percentile (best)\u003C\u002Ftd\u003E\n\u003Ctd\u003E66.67\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003EAverage\u003C\u002Ftd\u003E\n\u003Ctd\u003E53.49\u003C\u002Ftd\u003E\n\u003C\u002Ftr\u003E\n\u003C\u002Ftbody\u003E\u003C\u002Ftable\u003E\n\u003Ch3 id=\"conclusions\"\u003EConclusions\u003C\u002Fh3\u003E\n\u003Cp\u003EThroughout my journey with Qualcomm SNPE, I realized that the difficulties with this toolkit are essentially the same as the ones I faced when working through TensorRT for NVIDIA Jetsons, Alibaba&#39;s MNN for the Raspberry Pi 4, or Tensorflow Lite for TinyML. One conceptual thing I&#39;ve noticed is that these toolkits are going through the same feature flattening that major ML frameworks have and are still going through. They are all converging into a similar model conversion pipeline, with a growing set of core supported layer operations, and reliable methods for things like quantization. Many of these toolkits might boast speed-ups  over each other with benchmarks of certain networks, but, as stated before, they fail to capture the fact that network speeds can be erratic.\u003C\u002Fp\u003E\n\u003Cp\u003EIn terms of development, I learned a lot about Android development. I have experience in traditional media  pipelines, such as GStreamer or FFMPEG, so the Camera2 pipeline was pretty straight-forward for me. The callback function presented a very easy way to capture the preview data, but I never figured out how to edit an image and render it on the same \u003Ccode\u003ETextureView\u003C\u002Fcode\u003E. Apart from that, getting Qualcomm SNPE working on Android was also fairly easy since the APK integration worked flawlessly. Finally, accelerating various portions of my application, such as the preprocessing step, was incredibly gratifying because I finally got to use C++. I&#39;ve used OpenCV2 in Python and C++, so this native code on Android made me feel at home. \u003C\u002Fp\u003E\n\u003Cp\u003EThis probably isn&#39;t the end of my exploration of SNPE or mobile app development in general. I think I&#39;ll keep exploring other types of neural networks - such as object detectors.\u003C\u002Fp\u003E\n",created:"Wed, 11 Jan 2023 19:45:28 GMT",excerpt:"For the past couple of weeks, I&#39;ve been working on a project that would mimic the behavior of Apple&#39;s &quot;CenterStage&quot;. As a part of that project, I wanted to explore how neural networks were deployed, accelerated, and utilized on smartphone devices.\n",author:a,readingTime:"16 min read",mediaFilePath:"post-res\u002Fsnpe_android_1\u002Fsnpe_android_1_thumb.png",tags:["machine learning"],art_credit:a}}}("Vijay Rajagopal"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');var s=document.createElement("script");try{new Function("if(0)import('')")();s.src="/client/client.4da8ec3a.js";s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main","/client/client.4da8ec3a.js")}document.head.appendChild(s)</script> 